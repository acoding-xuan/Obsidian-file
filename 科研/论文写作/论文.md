
Sequential recommendation systems fundamentally operate on the premise that by analyzing a user’s interaction history and the order of those interactions, they can predict future engagements. These systems learn and glean insights from interaction patterns to identify items that are likely to captivate user attention next. Initially rooted in Markov chain prediction methods, these systems have undergone significant evolution with the advancements in deep learning, facilitating the incorporation of complex deep neural models like Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) into their architecture. The emergence of the Transformer neural architecture, with its self-attention mechanism, has taken these systems a step further, proving to be efficient with exceptional performance and extensive utility across various applications.


Despite the rapid development of numerous sequential recommendation models built upon diverse architectures, the industry lacks a standardized approach to assess their real-world implications effectively. This shortfall is particularly evident when considering the varying efficacy of models in capturing user preferences and item popularity. While benchmarks exist to evaluate performance, these often do not reflect a model's practical utility in a real-world scenario, which makes it challenging to draw direct comparisons.
This paper seeks to address this gap by conducting a comprehensive comparative analysis of some of the state-of-the-art (SOTA) algorithms on benchmark datasets. The study aims to provide a more nuanced understanding of where these algorithms excel and fall short in terms of their ability to gauge user preferences and the prominence of items.


By conducting a detailed assessment, the paper highlights the inconsistencies in performance and offers insights into the underlying factors contributing to these variations. For instance, it explores how the choice of hyperparameters, training strategies, and model architectures influences the accuracy of recommendation predictions. Furthermore, it examines whether the performance disparities are due to the models’ tendencies to recommend items of higher popularity or if more intricate differentiators are at play.


The analysis goes beyond mere performance metrics, delving into the practical significance of the models' recommendations. It considers the user-centric aspects of recommendation systems, such as diversity, novelty, and fairness, which are critical for maintaining user engagement and satisfaction in the long term. These factors are vital in understanding the real-world implications of sequential recommendation models, as they directly impact the user experience.


The paper's findings and methodologies serve as a guide for future researchers and practitioners in the field, providing a framework for evaluating sequential recommendation models that better reflect their practical utility and real-world impact. The hope is to foster the development of more nuanced and fair benchmarks that can lead to the creation of more effective and user-aligned recommendation systems.

\section{Loss Function}
We adopt the Cross-Entropy (CE) loss function for model training, diverging from the commonly utilized Bayesian Personalized Ranking (BPR) loss in recommendation systems. The decision to employ CE loss stems from its effectiveness in handling multi-class classification problems where the prediction involves the probability distribution over a range of classes.


Cross-Entropy loss, also referred to as log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. The loss increases as the predicted probability diverges from the actual label. The CE loss function is given by the formula:
\begin{equation}
    CE = -\sum_{i=1}^{N} y_i \log(p_i) + (1 - y_i) \log(1 - p_i)
\end{equation}
where $y_i$  is the binary indicator (0 or 1) of the class label for the i-th sample, and  is the predicted probability of the i-th sample being of the class labeled 1. 


On the other hand, BPR loss is specifically designed for item ranking in recommendation systems. It is a pairwise loss function that assumes a user prefers a positively interacted item over a random negatively sampled item. The BPR objective is formalized as:
\begin{equation}
    BPR = -\sum_{(u, i, j) \in D_S} \log(\sigma(\hat{r}_{ui} - \hat{r}_{uj}))
\end{equation}
where $D_s$ represents the training dataset consisting of triples (user u, item i, item j), $\widehat{r}_{ui}$ and $\widehat{r}_{uj}$ are the predicted preferences of user u for items i and j, respectively, and $\sigma$ is the sigmoid function.


The choice of CE loss over BPR for our project is predicated on the rich informational context provided by CE, which is better suited for scenarios where the distinction between multiple levels of interest, rather than just a binary preference, is required. Additionally, the probabilistic nature of CE loss makes it inherently adept at handling unbalanced datasets, a common occurrence in real-world scenarios.


While there are other loss functions, such as Mean Squared Error (MSE) or Hinge loss, employed in various learning tasks, they were deemed unsuitable for this project due to their respective limitations in context. MSE, for instance, is more commonly employed in regression problems rather than classification. In contrast, Hinge loss, used in Support Vector Machines (SVMs), doesn't provide probability estimates, which are essential in the domain of recommendation systems where we aim to predict likelihood in a probabilistic framework.


Hence, based on our model's requirements and the nature of the dataset, CE loss was selected as the most effective and appropriate function for optimizing the recommendation system's performance.


\section{Experimental Settings}
\subsection{Datasets}
Experiments for this project were executed on two datasets customarily leveraged as benchmarks for evaluating sequential recommendation models. The Amazon Beauty dataset comprises a collection of product review data harvested from Amazon.com, providing a rich source of consumer feedback and preferences. Concurrently, the MovieLens-1m dataset represents a subset of a broad movie rating database, commonly employed to refine and assess the performance of movie recommendation systems.


These two datasets have a notable history of utilization in pivotal research, including the primary literature regarding advanced models like BERT4Rec, as well as subsequent studies delving into the reproducibility of outcomes associated with such models. These prior works have set a precedence, validating the datasets' relevance and utility in the realm of sequential recommendation.


Crucially, to achieve a higher standard of reproducibility and impartial comparison, our methodology involved the adoption of a uniform data format. This entailed the conversion of all datasets into atomic file structures within the recbole framework—an approach designed to enhance consistency and clarity across experiments. Such standardization is a key step towards ensuring that subsequent experiments can be faithfully replicated, and comparative analyses can be conducted on an even playing field.


The pragmatic benefits of this approach are manifold, including the minimization of variances that could arise from dataset discrepancies, thereby allowing for a more accurate assessment of the models' merits. The comprehensive statistical data pertaining to these datasets have been meticulously compiled and are conveniently accessible in Table 1 of the manuscript, where readers can ascertain the foundational metrics that undergird our experimental findings.


1. 使用 recbole 。
2. 数据集都是由recbole 进行提供的处理好的数据集



## evaluation
1.准确性指标


2.多样性指标

本文在评估推荐系统性能时，不仅使用了传统的准确性指标如 Recall、Precision 等，还结合了覆盖率和信息损失（ILS）等多样性指标进行综合评估。
覆盖率指标衡量了推荐系统覆盖到的不同物品的比例，即推荐系统能够从整个物品空间中涵盖多少不同的物品。而信息损失（ILS）则关注了推荐列表中物品之间的相似性程度，用以评估推荐系统提供的推荐是否过于同质化。通过对比和分析覆盖率和信息损失等多样性指标的结果，我们能够更全面地了解各个推荐模型的特征。

原始论文

![[Pasted image 20240315230405.png]]
![[Pasted image 20240315230425.png]]

3.模型之间如何进行两两比较

![[Pasted image 20240315232634.png]]

Part1:
对于命中的用户：
1. 对于所有成功命中的user 记录其占所有用户的比例。从而看出一个model的泛化性能是否更强。 对较多人都效果不错还是就对一小部分。 
2. 同时记录 user的平均交互次数，从而确定模型善于捕获活跃用户个性还是不活跃用户。

4. 统计任意两个模型命中user的交叉情况 即 统计出 A model 命中的 B model 没有命中，B model 命中的
   A model 没有命中，以及A B 模型都命中的用户。 交叉和非交叉部分各模型之间的 user交互次数 最大值，平均值，占比， 中位数，以及各部分所占比例等。

对于所有推荐的物品：
	1. 对于一个模型推荐的所有的物品，计算物品的平均交互次数，去看模型的偏好, 是否喜欢推荐流行物品。
对于所有命中的物品：
     2. To see 是流行度高的命中的比较多还是，流行度的比较多。 流行度低的物品命中比较多可能更能够说明一个模型捕获偏好的效果。
     

同时，针对模型推荐的所有物品以及命中的所有物品，我们同样进行了如上所述的统计。
 我们可以通过分析模型推荐的所有物品的平均交互次数，以了解模型的偏好，即是否更倾向于推荐流行物品。
我们还可以通过分析所有命中的物品中流行度高的物品与流行度低的物品各自所占的比例。我们猜测如果流行度低的物品被命中的比例较高，可能能够说明模型捕获用户偏好的效果比较好。

eg.
对于模型所有推荐命中的用户查看 avarage

为什么要做这样的比较，这样的比较的作用

eg. 通过分析了解什么模型适合捕获活跃用户，什么适合非活跃用户


通过所有物品的比较：
我们希望通过分析得到 modelsA 和 modelsB 更擅长的领域。
对于命中物品。
我们希望怎么样。


1. 由于时间和计算资源的限制，我们无法对每两个推荐模型都进行上述比较和分析。因此，我们选择了五组模型进行对比，分别是BERT4Rec vs SASRec、GRU4Rec vs SASRec、GRURec vs NARM、SASRec vs S3Rec。我们选择这五组模型进行对比并非随意，而是有明确目的的：


BERT4Rec vs SASRec
所以在我们的实验中我们也使用了 cross-entropy over softmax for all items

我们想要去看训练和测试阶段的任务是否一致会不会对推荐结果产生较大的影响。


GRU4Rec Vs SASRec.

正如许多序列推荐模型一样，GRU4Rec 和 SASRec 都包含嵌入层和预测层。它们的不同之处在于嵌入层和预测层之间的模型结构。GRU4Rec 使用的是堆叠的多层 GRU 单元，而 SASRec 采用的是堆叠的基于自注意力机制的 Transformer 层。我们希望通过比较这两个模型，了解 GRU 单元和 Transformer 层对于推荐系统预测内容产生的影响，以及它们在捕捉用户行为序列中长期依赖性和兴趣演化方面的效果。这样的比较有助于我们更好地理解不同模型结构对推荐系统性能的影响，从而指导模型选择和优化。


GRU4Rec vs NARM 
正如上面提到的，GRU4Rec使用堆叠的多层门控循环单元（GRU）来建模用户行为序列，较好地捕捉了长期兴趣演化的特点；而NARM同样通过堆叠的多层门控循环单元（GRU）来建模用户行为序列，同时还采用了注意力机制提取重要信息，将两者结合起来，再进行预测。我们希望通过比较这两个模型了解注意力机制对推荐系统预测内容产生的影响。


S3Rec Vs SASRec

S3Rec    Compared whthSASRec, S3Rec devise four auxiliary self-supervised objectives to fully uitilize the association or fusion between context data and sequence data for sequential recommendation.

Compared whthSASRec, S3Rec devise four auxiliary self-supervised objectives to learn the correlations among attribute, item, subsequence, and sequence by utilizing the mutual information maximization (MIM) principle. 



It has been demonstrated in many research[1 2 3] that contextual information  is important to consider for improving the performance of sequential recommender systems


我们希望通过比较发现这四个上游任务对序列推荐系统最后推荐内容及效果的影响。