
We study a new problem in recommendation — aligning the LLMs with the recommendation, where we reveal the limitations of In-context Learning-based approaches and underscore（强调） the significance of instruction tuning.


We conduct extensive experiments, validating the effectiveness and efficiency of the proposed framework, and uncovering its exceptional robustness with seamless navigation across different domains
我们进行了广泛的实验，验证了提出的框架的有效性和效率，并揭示了其在不同领域无缝导航的卓越鲁棒性。


Seamless navigation across different domains
解释：在不同领域中无缝导航
用法：指系统或方法能够在不同的领域或环境中平稳过渡或操作，而不会出现问题或中断。例如："The software allows for seamless navigation across various user interfaces."（该软件允许在各种用户界面之间无缝导航。）

Specifically, we utilize the conditional language modeling objective during the alpaca tuning, as exemplified in the Alpaca repository1

Exemplified in the Alpaca repository
解释：在 Alpaca 仓库中的示例


Scrutinizing
解释：仔细审视、详细检查
Scrutinizing the current studies on SeqRec [14, 16, 34], we can summarize a common pipeline: treating historical interaction sequences of users as the training data, employing the recommender models upon them to
capture the sequential patterns, and then predicting users’ future interactions in the testing data.

This yields the distribution shifts between the training and testing data.
这导致了训练数据和测试数据之间的分布变化。


Distribution shifts undermine the ERM framework, and consequently, the generalization of recommenders deteriorates in serving future data. Deteriorates


Figure 1 demonstrates this issue with empirical evidence(实验的证据). We first divide the YooChoose 1 data into four disjoint shards in chronological order, and calculate the KL-divergence w.r.t. item
distribution between the first and other shards.

Disjoint shards
解释：不相交的分片

Chronological order
解释：按时间顺序
用法：指按照时间的先后顺序排列数据。

Another promising way is Distributionally Robust Optimization (DRO) [1, 19], which hedges against the discrepancy between training and testing distributions. The basic idea is training the model over the distributional family which is determined by a nominal distribution with a robust radius, so as to handle the distributional uncertainty.

Distributionally Robust Optimization (DRO)
解释：分布鲁棒优化
用法：DRO 是一种优化方法，旨在提高模型在面对分布变化或不确定性时的稳健性。它通过在可能的分布集合上优化模型，而不仅仅是基于训练数据分布，从而提高模型的泛化能力。

Hedges against
解释：对抗、防范
用法：指采取措施防止或减轻某种风险或不确定性。在数据科学和优化问题中，这通常涉及开发策略以应对可能的不利情况。例如："The algorithm hedges against potential data shifts to maintain performance."（该算法防范可能的数据变化以保持性能。）

Nominal distribution
解释：名义分布
用法：指在 DRO 中用作基准的概率分布。名义分布通常基于历史数据或某些假设，它定义了分布族的中心位置。在模型优化中，名义分布是重要的参考点。


In this section, we first introduce basic notations and commonlyused ERM frameworks in SeqRec, and then elaborate our proposed DROS in detail.

Elaborate
解释：详细阐述
用法：指对某个概念或方法进行深入的解释或说明。在研究论文中，"elaborate" 常用于引出对方法或结果的详细讨论。例如："We will elaborate on the proposed method in the following sections."（我们将在以下部分详细阐述所提出的方法。）


Our theoretical analysis admits that if the distance between training and testing data is bounded(有界限的), the robustness of DROS can be guaranteed. We also empirically demonstrate（我们还通过实验表明） that our DROS can better adapt to future inference phase in the dynamic SeqRec process.

Generally, minimizing our proposed objective Equation (6) can in principle enhance the dynamic adaptation of SeqRec recommenders, but in practice, the learning process would be unstable due to the maximization term in \(L_{DRO}\). If we can further acquire a closed form of \(L_{DRO}\), this unstable issue can be addressed.

Closed form
解释：闭式解
用法：指一个方程或优化问题的明确解析解，而无需进行迭代计算。获得闭式解可以简化计算并提高模型的稳定性。


Compromise coefficient 
解释：折衷系数
用法：折衷系数用于在不同目标之间进行平衡。在本文中，
α 用于平衡 DRO 和 ERM 目标函数的影响。例如："The compromise coefficient balances the influence of different components in the objective function."（折衷系数平衡了目标函数中不同部分的影响。）


