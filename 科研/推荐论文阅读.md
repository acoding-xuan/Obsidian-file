
# 专业名词

## Offline Reinforcement Learning

给定一个马尔可夫决策过程（MDP），其形式化表示为 (S, A, 𝑃, R, 𝛾)，其中：
- S 是状态集合，𝑠 ∈ R^𝑑（𝑑维实数空间中的状态）；
- A 是动作集合，𝑎 表示动作；
- 𝑃(𝑠'|𝑠, 𝑎) 是从状态𝑠经过动作𝑎转移到新状态𝑠'的转移概率；
- R 是针对特定状态-动作对的奖励函数；
- 𝛾 是折扣率。

从时间戳0到𝑇的轨迹可以写成 (𝑠0, 𝑎0, 𝑟0, ···, 𝑠𝑡, 𝑎𝑡, 𝑟𝑡, ···, 𝑠𝑇, 𝑎𝑇, 𝑟𝑇)，其中(𝑠𝑡, 𝑎𝑡, 𝑟𝑡)是在时间戳𝑡的状态、动作和奖励对。

强化学习（RL）的学习目标是确定一个最优策略，该策略能够在给定特定奖励函数和折扣率的情况下最大化预期累计回报E[Σ𝑇 𝑡=1 𝛾^𝑡 𝑟𝑡]。

1. **状态集合 S**：这是系统可能处于的所有状态的集合。每个状态𝑠是一个𝑑维向量，表示系统在某一时刻的状态。

2. **动作集合 A**：这是系统在不同状态下可以采取的所有可能动作的集合。动作𝑎是系统在给定状态下可以执行的操作。

3. **转移概率 𝑃(𝑠'|𝑠, 𝑎)**：这是一个条件概率，表示系统从状态𝑠执行动作𝑎后转移到新状态𝑠'的概率。这体现了系统的动态行为。

4. **奖励函数 R**：这是一个函数，定义了在特定状态和动作下系统所能获得的奖励。奖励是系统评估动作优劣的依据。

5. **折扣率 𝛾**：这是一个介于0和1之间的值，用于折扣未来的奖励。折扣率越接近1，未来的奖励对当前决策的影响越大；折扣率越小，系统更关注当前的奖励。

6. **轨迹 (𝑠0, 𝑎0, 𝑟0, ···, 𝑠𝑡, 𝑎𝑡, 𝑟𝑡, ···, 𝑠𝑇, 𝑎𝑇, 𝑟𝑇)**：这是系统从初始状态开始，经过一系列状态、动作和对应奖励的序列。它描述了系统从时间戳0到时间戳𝑇的整个路径。

7. **强化学习的目标**：RL的目标是找到一个最优策略，使得在这个策略下，系统能够在给定的奖励函数和折扣率的条件下，最大化预期的累计回报。累计回报是对系统在未来各个时间点上获得的奖励进行加权求和，其中权重由折扣率𝛾确定。最大化预期累计回报意味着策略不仅要考虑当前的即时奖励，还要考虑未来的长期收益。
## Reward to Go (RTG)
在强化学习中，"Reward to Go"（回报或奖励的未来总和）指的是从某个时间点开始到未来所有时间点的累积奖励。这在训练强化学习模型时非常重要，因为它帮助模型了解某个状态和动作在长期内的价值。

在推荐系统中，RTG 可以用于评估某个推荐策略的长期效果。例如，如果一个用户在某个时间点接收到一个推荐项目，RTG 将考虑用户在接下来一段时间内对这个推荐项目的所有反应（如点击、购买等）的总和，而不仅仅是即时反应。这种方式帮助模型更好地优化推荐策略，从而提高用户的长期满意度和系统的整体效益。
## Decision Transformer

不同于使用传统的强化学习算法（例如训练代理策略或近似值函数【20】），决策transformer采用了不同的方法，将强化学习重新表述为具有监督学习目标的序列建模问题。为了使转换器具备识别重要模式的能力，相应的轨迹表示形式设计为具有𝑇个时间戳的序列：

$$ \tau = (R_{1}, s_1, a_1, \ldots, R_{t}, s_t, a_t, \ldots, R_{tT}, s_T) $$
在这种表示形式中， \($R_t = \sum_{k=t}^{T} r_k$\) 定义了从时间𝑡到时间𝑇的累积奖励（RTG），without applying any discount。给定RTG和状态信息，决策转换器能够通过具有层次化自注意力和残差连接的因果屏蔽转换器架构预测下一个动作 。

通过这种方法，决策转换器能够更有效地学习复杂的模式，从而在强化学习任务中提供准确的动作预测。
在每一层中，输入嵌入（input embeddings）{𝑥𝑖} (i=1, ..., m) 被处理以生成相应的输出嵌入 {𝑧𝑖} (i=1, ..., m)。每个标记的位置 𝑖 决定了其被转换为键（key）𝑘𝑖、查询（query）𝑞𝑖和值（value）𝑣𝑖 的方式。对于相同位置的输出，是通过使用查询 𝑞𝑖 和相关的键 𝑘𝑗 之间的归一化点积来调整值 𝑣𝑗 得到的，该点积通过 softmax 激活函数 𝜎𝑠 进一步处理：

$$𝑧𝑖 = \sum_{𝑗=1}^{𝑚} \sigma_s \left( \left\{ \langle 𝑞𝑖 , 𝑘𝑙 \rangle \right\}_{𝑙=1}^{𝑚} \right)_𝑗 \cdot 𝑣𝑗 $$



# 序列推荐
## Sequential Recommendation for Optimizing Both Immediate Feedback and Long-term Retention

we prioritize the optimization of two key performance indicators: click-through rate (CTR) and return frequency.
## Problem Defination
![](../img/Pasted%20image%2020240521114140.png)
![](../img/Pasted%20image%2020240521114204.png)

![](../img/Pasted%20image%2020240521140121.png)

## Adaptive RTG Balancing

![](../img/Pasted%20image%2020240521141755.png)
![](../img/Pasted%20image%2020240521142006.png)

![](../img/Pasted%20image%2020240521142954.png)

## Multi-reward Embedding


## Objective Function with Contrastive Learning Term

![](../img/Pasted%20image%2020240521144910.png)
![](../img/Pasted%20image%2020240521150223.png)
![](../img/Pasted%20image%2020240521150201.png)


## results
![](../img/Pasted%20image%2020240521150625.png)


##  RTG Prompting Analysis
![](../img/Pasted%20image%2020240521153407.png)
LlamaModel
![](../img/Pasted%20image%2020240728192432.png)





![](../img/Pasted%20image%2020240728165023.png)

