# lecture 1
**一，前言：**
**本章节主要对讲解了自然语言处理（NLP）发展与深度学习的简介。**
**二，正文：**

**WordNet（分类词典）的缺点:**

不能准确识别同义词之间的细微差别。

对词语的理解不够精确（单词的词义不能及时更新，WordNet资源也不完整）。

主观性太强（只有固定的离散同义词集）。

耗费大量的人力资源进行标注，来构建 。

难以准确计算字词之间的相似性。

One-hot vectors:

![](file://E:\python_learning\stanford-cs224n-notes-zh-master\docs\Lecture1\media\d3737e074d0d735dae202cbddb31623f.png?lastModify=1697440608)

列如：

[猫，狗，鸡，鸭，汽车旅馆，鸟，骆驼，酒店]，对这个数组进行编码之后；

Motel（汽车旅馆）对应的编码应是：

Motel=[0,0,0,0,1,0,0,0];

同理，Hotel(酒店)对应的编码应是：

Hotel=[0,0,0,0,0,0,0,1];

在上面汽车旅馆和酒店的意思是相似，但是对Motel=[0,0,0,0,1,0,0,0]与Hotel=[0,0,0,0,0,0,0,1]内积为0，也就是完全不相关；所以这种方式并不能表示出这种关系。

![](file://E:\python_learning\stanford-cs224n-notes-zh-master\docs\Lecture1\media\26cef4d801bc54c1939a7e0cc8f734aa.png?lastModify=1697440608)

由于ONE-Hot满足不了相关性，（J.R.Firth）提出了Distributional semantics(分布式语义):单词的意思，应该结合它的上下文来确定的;现代统计NLP成功的理念之一。

e:

用‘banking’许多的上下文来构建它的意思：

![](file://E:\python_learning\stanford-cs224n-notes-zh-master\docs\Lecture1\media\d2b95dae5dedcbc90fad260cbc1da976.png?lastModify=1697440608)

**词向量：**

将选择的每个单词构建了密集的向量，为了更好的预测上下文的意思：

![](file://E:\python_learning\stanford-cs224n-notes-zh-master\docs\Lecture1\media\40677ebb4b091966d2f8e550b4ca6d30.png?lastModify=1697440608)

‘banking’对应的维度是8；每一个单词都有一个向量维度，所有单词在一起将会有一个向量空间。

![](file://E:\python_learning\stanford-cs224n-notes-zh-master\docs\Lecture1\media\90138e84ea3b961732b0d5e70e032f4a.png?lastModify=1697440608)

向量中的每个单词具有不同的基础；

Word2vec:是一个学习单词向量的框架。

![](file://E:\python_learning\stanford-cs224n-notes-zh-master\docs\Lecture1\media\756020be8c51e4f89a721f20dca825bd.png?lastModify=1697440608)

想法;

我们有大量的文本。

固定词汇表中的每一个单词都由一个向量表示

遍历文本中每一个位置,其中有一个中心词C与外部词O。

用单词的向量的相似性来计算O与C的相同（或想反）的概率。

不断的调整单词向量，使概率值最大化。

用迭代来实现这个算法，要确定一个单词的意思，要结合上下文的意思来确定，我们要遍历文本中的每个位置，通过移动单词向量来实现；重复10亿次之后，得到了好的结果；

计算过程：

![](file://E:\python_learning\stanford-cs224n-notes-zh-master\docs\Lecture1\media\4275e279e6d646246334a84434ebf9dd.png?lastModify=1697440608)

目标函数（损失函数或成本函数）：

![](file://E:\python_learning\stanford-cs224n-notes-zh-master\docs\Lecture1\media\939133082e8b9ec6e39ac058a84e8cdd.png?lastModify=1697440608)

对于给定的数据集T，固定了窗口的大小（每个面几个字），给定中心单词Wj;

模型的概率

![](file://E:\python_learning\stanford-cs224n-notes-zh-master\docs\Lecture1\media\1ed8e2c6f59f03f227a0a96783bee07c.png?lastModify=1697440608)

最小化目标函数==最大化概率值；

![](file://E:\python_learning\stanford-cs224n-notes-zh-master\docs\Lecture1\media\86f7fdf808b1a7958f0fa3c15c413ab1.png?lastModify=1697440608)

这个模型中，唯一的一个参数就是我们给定的一个变量；

![](file://E:\python_learning\stanford-cs224n-notes-zh-master\docs\Lecture1\media\f6ee3acc5c552202abf5a7bd033cbbe2.png?lastModify=1697440608)

最小化损失函数；

将每个单词使用俩个向量：当W是中心词是，当W是上下文词时；

然后是中心词C与上下文词O；

e:

定义一个某个单词的上下文模型：

> P（context|Wt）=.....

定义损失函数：

> J=1-P（Wt+j|Wt）(Wt+j与Wt为上下文)

如果预测结果正确，目标函数为0；

在词料库中的不同位置来训练，调整词向量，最小化目标函数：

比如：我喜欢打篮球，也喜欢打羽毛球。

当出现“打篮球”时，预测出现“打羽毛球”的可能性；概率模型为：

> P（打羽毛球|打篮球）（也就是在“打篮球”的条件下“打羽毛球”的概率）

目标函数：

> J=1-p(W|打篮球)

如果w=“打羽毛球”，概率为1，目标函数为0。

![](file://E:\python_learning\stanford-cs224n-notes-zh-master\docs\Lecture1\media\2f999b6467c4c018c0306069f60b3625.png?lastModify=1697440608)

分子是：通过点积来计算O与C的相似性；分母是对整个词汇表进行标准化后给出的概率分布；

这是一个SOFEMAX函数R^N——R^n的例子：

![](file://E:\python_learning\stanford-cs224n-notes-zh-master\docs\Lecture1\media\a1d0273bc4309db805739d88a64fbc92.png?lastModify=1697440608)

在SOFTMAX函数中：

“max”是将一些大的概率表示为最大的概率Xi;

“soft”是将一些小的概率表示为更小的概率Xj;

经常用于深度学习；

**通过优化训练参数训练模型：**

为了更好的训练模型，调整参数使目标函数最小化；

也就是用梯度下降来找到最优点；

![](file://E:\python_learning\stanford-cs224n-notes-zh-master\docs\Lecture1\media\8b5e9d716ea43a7b82695a73ad497674.png?lastModify=1697440608)

**计算所有向量维度：**

在D维向量与V维向量中，有许多单词：

![](file://E:\python_learning\stanford-cs224n-notes-zh-master\docs\Lecture1\media\0f678a32a897d8a7cbdd958b5e73047a.png?lastModify=1697440608)

每个单词都有俩个向量；沿着这个梯度进行优化；
上一讲 [CS224N 笔记 (一)：Word Vector](https://zhuanlan.zhihu.com/p/59016893) 主要介绍了 Word2Vec 模型，它是一种基于 local context window 的 direct prediction 预测模型，对于学习 word vector，还有另一类模型是 count based global matrix factorization，这一讲主要介绍了后一类模型以及 Manning 教授组结合两者优点提出的 GloVe 模型。

# lecture 2
## **SVD 模型**

count based 模型的经典代表是 SVD(Single Value Decomposition）模型。通常我们先扫过一遍所有数据，然后得到单词同时出现的矩阵 co-occurrence matrix，假设矩阵用 $X$X 表示，然后我们对其进行 SVD 得到 $X$X 的分解形式 $USV^T$USV^T 。

如何产生矩阵 $X$X 通常有两种选择。一是 word-document co-occurrence matrix, 其基本假设是在同一篇文章中出现的单词更有可能相互关联。假设单词 $i$i 出现在文章 $j$j 中，则矩阵元素 $X_{ij}$X_{ij} 加一，当我们处理完数据库中的所有文章后，就得到了矩阵 $X$X ，其大小为 $|V|\times M$|V|\times M ，其中 $|V|$|V| 为词汇量，而 $M$M 为文章数。这一构建单词文章 co-occurrence matrix 的方法也是经典的 Latent Semantic Analysis 所采用的。

另一种选择是利用某个定长窗口中单词与单词同时出现的次数来产生 window-based word-word co-occurrence matrix。下面以窗口长度为 1 来举例，假设我们的数据包含以下几个句子：

1.I like deep learning.

2.I like NLP.

3.I enjoy flying。

则我们可以得到如下的 word-word co-occurrence matrix:

![](<assets/1697459518010.png>)

可以看出，随着词汇量的增大，矩阵 $X$X 的尺度会越来大，为了有效的存储，我们可以对其进行 SVD 处理，即将其转化为酉矩阵 X 对角矩阵 X 酉矩阵的形式：

![](<assets/1697459519819.png>)

为了减少尺度同时尽量保存有效信息，可保留对角矩阵的最大的 k 个值，其余置零，并将酉矩阵的相应的行列保留，其余置零：

![](<assets/1697459521755.png>)

这就是经典的 SVD 算法。

## **GloVe 算法**

比较 SVD 这种 count based 模型与 Word2Vec 这种 direct prediction 模型，它们各有优缺点：
**Count based 模型优点是训练快速，并且有效的利用了统计信息，缺点是对于高频词汇较为偏向，并且仅能概括词组的相关性，而且有的时候产生的 word vector 对于解释词的含义如 word analogy 等任务效果不好**；

**Direct Prediction 优点是可以概括比相关性更为复杂的信息，进行 word analogy 等任务时效果较好，缺点是对统计信息利用的不够充分。**

所以 Manning 教授他们想采取一种方法可以结合两者的优势，并将这种算法命名为 GloVe（Global Vectors 的缩写），表示他们可以有效的利用全局的统计信息。

那么如何有效的利用 `word-word， co-occurrence count` 并能学习到词语背后的含义呢？首先为表述问题简洁需要，先定义一些符号：对于矩阵 $X$ ,$X_{ij}$ 代表了单词 $j$ 出现在单词 $i$ 上下文中的次数，则 $X_i=\sum_k X_{ik}$ 即代表所有出现在单词 $i$i 的上下文中的单词次数。我们用 $P_{ij}=P(j|i)=X_{ij}/X_i$来代表单词 $j$ 出现在单词 $i$上下文中的概率。


我们用一个小例子来解释如何利用 co-occurrence probability 来表示词汇含义：

![](<assets/1697459522008.png>)

例如我们想区分热力学上两种不同状态 `ice` 冰与蒸汽 `steam`，它们之间的关系可通过与不同的单词 $x$ 的 co-occurrence probability 的比值来描述，例如对于 solid 固态，虽然 $P(solid|ice)$ 与 $P(solid|steam)$本身很小，不能透露有效的信息，但是它们的比值 $\frac{P(solid|ice)}{P(solid|steam)}$ 却较大，因为 solid 更常用来描述 ice 的状态而不是 steam 的状态，所以在 ice 的上下文中出现几率较大，对于 gas 则恰恰相反，而对于 water 这种描述 ice 与 steam 均可或者 fashion 这种与两者都没什么联系的单词，则比值接近于 1。所以相较于单纯的 co-occurrence probability，实际上 `co-occurrence probability 的相对比值更有意义`。

基于这些观察，视频里直接给出了 GloVe 的损失函数形式：

![](<assets/1697459522217.png>)

个人觉得这里跳跃性略大，并没有解释其背后的思路，所以我们需要参考 GloVe 论文 [https://nlp.stanford.edu/pubs/glove.pdf](https://link.zhihu.com/?target=https%3A//nlp.stanford.edu/pubs/glove.pdf) 来了解一下作者的思路以便理解。有意思的是该文第一作者 Jeffrey Pennington 在成为 Manning 组的 PostDoc 之前是理论物理的博士，他用了物理学家简化假设做 back-of-envelope 计算合理推断的习惯，而并没有从第一性原理出发做严谨的推导，在了解其思路时需要记住这点。

基于对于以上概率比值的观察，我们假设模型的函数有如下形式：

$$F(w_i,w_j,\tilde w_k) = \frac{P_{ik}}{P_{jk}}$$

其中， $\tilde{w}$代表了 `context vector`，如上例中的 `solid，gas，water，fashion` 等。 $w_i,w_j$ 则是我们要比较的两个词汇，如上例中的 `ice，steam`。

$F$ 的可选的形式过多，我们希望有所限定。首先我们希望的是 $F$ 能有效的在单词向量空间内表示概率比值，由于向量空间是线性空间，一个自然的假设是 $F$ 是关于向量 $w_i,w_j$ 的差的形式：

$$F(w_i-w_j,\tilde w_k) = \frac{P_{ik}}{P_{jk}}$$

等式右边为标量形式，左边如何操作能将矢量转化为标量形式呢？一个自然的选择是矢量的点乘形式：

$$F((w_i-w_j)^T\tilde w_k) = \frac{P_{ik}}{P_{jk}}$$

在此，作者又对其进行了对称性分析，即对于 `word-word co-occurrence`，将向量划分为 center word 还是 context word 的选择是不重要的，即我们在交换 $w\leftrightarrow \tilde w$与 $X\leftrightarrow X^T$的时候该式仍然成立。如何保证这种对称性呢？

我们分两步来进行，首先要求满足 $F((w_i-w_j)^T\tilde w_k) = \frac{F(w_i^T\tilde w_k)}{F(w_j^T\tilde w_k)}$，该方程的解为 $F=exp$ 同时与 $F((w_i-w_j)^T\tilde w_k) = \frac{P_{ik}}{P_{jk}}$相比较有 $F(w_i^T\tilde w_k)=P_{ik}=\frac{X_{ik}}{X_i}$

所以， $w_i^T\tilde w_k = log(P_{ik}) = log(X_{ik})-log(X_i)$

注意其中 $log(X_i)$ 破坏了交换 $w\leftrightarrow \tilde w$与 $X\leftrightarrow X^T$ 时的对称性，但是这一项并不依赖于 $k$ ，所以我们可以将其融合进关于 $w_i$ 的 bias 项 $b_i$ ，第二部就是为了平衡对称性，我们再加入关于 $\tilde w_k$的 bias 项 $\tilde b_k$ ，我们就可以得到 $w_i^T\tilde w_k + b_i + \tilde b_k = log(X_{ik})$的形式。

另一方面作者注意到模型的一个缺点是对于所有的 co-occurence 的权重是一样的，即使是那些较少发生的 co-occurrence。作者认为这些可能是噪声，所以他加入了前面的 $f(X_{ij})$ 项来做 weighted least squares regression 模型，即为

$J=\sum_{i,j=1}^Vf(X_{ij})(w_i^T\tilde w_j + b_i + \tilde b_j -logX_{ij})^2$ 的形式。

其中权重项 $f$需满足一下条件：

1.  $f(0)=0$ ，因为要求 $lim_{x\rightarrow 0}f(x)log^2x$ 是有限的。
2.  较少发生的 co-occurrence 所占比重较小。
3.  对于较多发生的 co-occurrence， $f(x)$ 也不能过大。

作者试验效果较好的权重函数形式是

![](<assets/1697459522536.png>)

![](<assets/1697459522868.png>)

可以看出，GloVe 的模型公式的导出有很多假设与 trial and error 的函数设置，想寻求更严谨些的理论的小伙伴可参考这篇文章 [A Latent Variable Model Approach to PMI-based Word Embeddings](https://link.zhihu.com/?target=http%3A//aclweb.org/anthology/Q16-1028)。

## **GloVe 与 Word2Vec 性能比较**

虽然 GloVe 的作者在原论文中说 GloVe 结合了 SVD 与 Word2Vec 的优势，训练速度快并且在各项任务中性能优于 Word2Vec，但是我们应该持有怀疑的态度看待这一结果，可能作者在比较结果时对于 GloVe 模型参数选择较为精细而 Word2Vec 参数较为粗糙导致 GloVe 性能较好，或者换另一个数据集，改换样本数量，两者的性能又会有不同。实际上，在另一篇论文 [Evaluation methods for unsupervised word embeddings](https://link.zhihu.com/?target=http%3A//www.aclweb.org/anthology/D15-1036) 中基于各种 intrinsic 和 extrinsic 任务的性能比较中，Word2Vec 结果要优于或不亚于 GloVe。实际应用中也是 Word2Vec 被采用的更多，对于新的任务，不妨对各种 embedding 方法都做尝试，选择合适自己问题的方法。

# lecture3, 4(就深度学习的一些基础)

# lecture 5
对于**句法结构 (syntactic structure)** 分析，主要有两种方式：`Constituency Parsing` 与 `Dependency Parsing`。

## **Constituency Parsing 基本概念**

Constituency Parsing 主要用 phrase structure grammer 即短语语法来不断的将词语整理成嵌套的组成成分，又被称为 context-free grammers，简写做 CFG。

其主要步骤是先对每个词做词性分析 part of speech, 简称 POS，然后再将其组成短语，再将短语不断递归构成更大的短语。

例如，对于 the cuddly cat by the door, 先做 POS 分析，the 是限定词，用 Det(Determiner) 表示，cuddly 是形容词，用 Adj(Adjective) 代表，cat 和 door 是名词，用 N(Noun) 表示, by 是介词，用 P(Preposition) 表示。

然后 the cuddly cat 构成名词短语 NP(Nou(by)+Nn Phrase)，这里由 Det(the)+Adj(cuddly)+N(cat) 构成，by the door 构成介词短语 PP(Preposition Phrase), 这里由 PP(the door) 构成。

最后，整个短语 the cuddly cat by the door 是 NP，由 NP（the cuddly cat）+ PP(by the door) 构成。

关于 Constituency Parsing, 第 18 讲还会涉及，这一讲集中讨论 Dependency Parsing。

## **Dependency Parsing 基本概念**

`Dependency Structure` 展示了词语之前的依赖关系, 通常用箭头表示其依存关系，有时也会在箭头上标出其具体的语法关系，如是主语还是宾语关系等。

Dependency Structure 有两种表现形式，一种是直接在句子上标出依存关系箭头及语法关系，如：

![](<assets/1697462230516.png>)

另一种是将其做成树状机构（Dependency Tree Graph）

![](<assets/1697462231377.png>)

Dependency Parsing 可以看做是给定输入句子 $S=w_0w_1...w_n$（其中 $w_0$ 常常是 fake ROOT，使得句子中每一个词都依赖于另一个节点）构建对应的 Dependency Tree Graph 的任务。而这个树如何构建呢？

一个有效的方法是 `Transition-based Dependency Parsing`。

## **Transition-based Dependency Parsing**

Transition-based Dependency Parsing 可以看做是 state machine，对于 $S=w_0w_1...w_n$ ，state 由三部分构成 $(\sigma, \beta, A)$ 。

$\sigma$ 是 $S$ 中若干 $w_i$ 构成的 stack。

$\beta$ 是 $S$ 中若干 $w_i$ 构成的 buffer。

$A$ 是 dependency arc 构成的集合，每一条边的形式是 $(w_i,r,w_j)$ ，其中 r 描述了节点的依存关系如动宾关系等。

初始状态时， $\sigma$ 仅包含 ROOT $w_0$ ， $\beta$ 包含了所有的单词 $w_1...w_n$ ，而 $A$ 是空集 $\phi$ 。最终的目标是$\sigma$ 包含 ROOT $w_0$， $\beta$ 清空，而 $A$ 包含了所有的 dependency arc， $A$ 就是我们想要的描述 Dependency 的结果。

state 之间的 transition 有三类：

*   SHIFT：将 buffer 中的第一个词移出并放到 stack 上。
*   LEFT-ARC：将 $(w_j,r,w_i)$加入边的集合 $A$ ，其中 $w_i$ 是 stack 上的次顶层的词， $w_j$ 是 stack 上的最顶层的词。
*   RIGHT-ARC: 将 $(w_i,r,w_j)$ 加入边的集合 $A$ ，其中 $w_i$ 是 stack 上的次顶层的词，$w_j$ 是 stack 上的最顶层的词。

我们不断的进行上述三类操作，直到从初始态达到最终态。在每个状态下如何选择哪种操作呢？当我们考虑到 LEFT-ARC 与 RIGHT-ARC 各有 | R|（|R | 为 r 的类的个数）种 class，我们可以将其看做是 class 数为 2|R|+1 的分类问题，可以用 SVM 等传统机器学习方法解决。

## **Evaluation**

当我们有了 Dependency Parsing 的模型后，我们如何对其准确性进行评估呢？

我们有两个 metric，一个是 LAS（labeled attachment score）即`只有 arc 的箭头方向以及语法关系均正确时才算正确`，以及 UAS（unlabeled attachment score）即`只要 arc 的箭头方向正确即可`。

一个具体的例子如下图所示：

![](<assets/1697462232629.png>)

## **Neural Dependency Parsing**

传统的 Transition-based Dependency Parsing 对 feature engineering 要求较高，我们可以用神经网络来减少 human labor。

对于 Neural Dependency Parser，其输入特征通常包含三种：

*   stack 和 buffer 中的单词及其 dependent word。
*   单词的 Part-of-Speech tag。
*   描述语法关系的 arc label。

![](<assets/1697462233851.png>)

我们将其转换为 embedding vector 并将它们联结起来作为输入层，再经过若干非线性的隐藏层，最后加入 softmax layer 得到每个 class 的概率。

![](<assets/1697462234040.png>)

利用这样简单的前置神经网络，我们就可以减少 feature engineering 并提高准确度，当然，在之后几讲中的 RNN 模型也可以应用到 Dependency Parsing 任务中。

**参考资料**

第五讲讲义 [http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture05-dep-parsing.pdf](https://link.zhihu.com/?target=http%3A//web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture05-dep-parsing.pdf)

补充材料 [http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes04-dependencyparsing.pdf](https://link.zhihu.com/?target=http%3A//web.stanford.edu/class/cs224n/readings/cs224n-2019-notes04-dependencyparsing.pdf)

第五讲视频 [https://youtu.be/nC9_RfjYwqA](https://link.zhihu.com/?target=https%3A//youtu.be/nC9_RfjYwqA)

# lecture 6
CS224N 第六讲主要研究语言模型及循环神经网络在语言模型中的应用。

## **语言模型**

语言模型 (Language Modelling) 研究的是`根据已知序列推测下一个单词的问题`，即假设已知 $x^{(1)},x^{(2)},...,x^{(t)}$，预测下一个词 $x^{(t+1)}$ 的概率 $P(x^{(t+1)}|x^{(t)},...,x^{(1)})$的问题。根据条件概率的链式法则，我们也可以将其看做一系列词出现的概率问题 $P(x^{(t)},...,x^{(1)})=\prod_{t=1}^{T}P(x^{(t)}|x^{(t-1)},...,x^{(1)})$

语言模型应用广泛，比如手机上打字可以智能预测下一个你要打的词是什么，或者谷歌搜索时自动填充问题等都属于语言模型的应用。语言模型是很多涉及到产生文字或预测文字概率的 NLP 问题的组成部分，如语音识别、手写文字识别、自动纠错、机器翻译等等。

## **经典 n-gram 模型**

较为经典的语言模型是 n-gram 模型。n-gram 的定义就是连续的 n 个单词。例如对于 the students opened their __这句话，unigram 就是 "the", "students", "opened", "their", 
bigram 就是 "the students", "students opened", "opened their", 
3-gram 就是 "the students opened", "students opened their"， 
以此类推。该模型的核心思想是 n-gram 的概率应正比于其出现的频率，并且假设 $P(x^{(t+1)})$ 仅仅依赖于它之前的 n-1 个单词，即

$$P(x^{(t+1)}|x^{(t)},...,x^{(1)}) = P(x^{(t+1)}|x^{(t)},...,x^{(t-n+2)}) = \\ \frac{P(x^{(t+1)},x^{(t)},...,x^{(t-n+2)})}{P(x^{(t)},...,x^{(t-n+2)})} \approx \frac{count(x^{(t+1)},x^{(t)},...,x^{(t-n+2)})}{count(x^{(t)},...,x^{(t-n+2)})}$$

其中 count 是通过处理大量文本对相应的 n-gram 出现次数计数得到的。

n-gram 模型主要有两大问题：

1.  稀疏问题 Sparsity Problem。在我们之前的大量文本中，可能分子或分母的组合没有出现过，则其计数为零。并且随着 n 的增大，稀疏性更严重。
2.  我们必须存储所有的 n-gram 对应的计数，随着 n 的增大，模型存储量也会增大。

这些限制了 n 的大小，但如果 n 过小，则我们无法体现稍微远一些的词语对当前词语的影响，这会极大的限制处理语言问题中很多需要依赖相对长程的上文来推测当前单词的任务的能力。

## **Window-based DNN**

一个自然的将神经网络应用到语言模型中的思路是 `window-based DNN`，即将定长窗口中的 word embedding 连在一起，将其经过神经网络做对下一个单词的分类预测，其类的个数为语库中的词汇量，如下图所示

![](<assets/1697531656078.png>)

与 n-gram 模型相比较，它解决了稀疏问题与存储问题，但它仍然存在一些问题：窗口大小固定，扩大窗口会使矩阵 W 变大，且 $x^{(1)},x^{(2)}$与 W 的不同列相乘，没有任何可共享的参数。

## **RNN**

RNN(Recurrent Neural Network) 结构通过不断的应用同一个矩阵 W 可实现参数的有效共享，并且没有固定窗口的限制。其基本结构如下图所示：

![](<assets/1697531656829.png>)

即前一时刻的隐藏状态以及当前的输入通过矩阵 W 加权求和然后在对其非线性变换并利用 softmax 得到对应的输出：

![](<assets/1697531656944.png>)

对于语言模型，RNN 的应用如下所示，即对 word embedding vector 进行如上操作：

![](<assets/1697531657282.png>)

RNN 的训练过程同样依赖于大量的文本，在每个时刻 t 计算模型预测的输出 $y^{(t)}$ 与真实值 $\hat y^{(t)}$ 即 $x^{(t+1)}$ 的 cross-entropy loss，即

![](<assets/1697531657427.png>)

对于文本量为 T 的总的损失即为所有交叉熵损失的平均值：

![](<assets/1697531657841.png>)

语言模型的评估指标是 perplexity，即我们文本库的概率的倒数，也可以表示为损失函数的指数形式，所以 perplexity 越小表示我们的语言模型越好。

![](<assets/1697531658160.png>)

![](<assets/1697531658430.png>)

与之前的模型相比，RNN 模型的优势是：

1. 可以处理任意长度的输入。

2. 理论上 t 时刻可以利用之前很早的历史信息。

3. 对于长的输入序列，模型大小并不增加。

4. 每一时刻都采用相同的权重矩阵，有效的进行了参数共享。

其劣势是：

1. 由于需要顺序计算而不是并行计算，RNN 计算较慢。

2. 实际上由于梯度消失等问题距离较远的早期历史信息难以捕捉。

当然 RNN 不仅限于语言模型，它还可以应用于诸如 Named Entity Recognition, Sentiment Analysis 等问题。

上面我们讨论的是单向 (unidirectional) 的 RNN, 对于某些我们具有整个句子的上下文问题（不适用于语言模型，因为语言模型只有上文），我们可以用双向 (bidirectional) 的 RNN，例如 sentiment analysis 问题, 对于 the movie was terribly exciting 这句话，如果我们只看到 terribly 的上文，我们会认为这是一个负面的词，但如果结合下文 exciting，就会发现 terribly 是形容 exciting 的激烈程度，在这里是一个很正面的词，所以我们既要结合上文又要结合下文来处理这一问题。我们可以用两个分别从前向后以及从后向前读的 RNN，并将它们的隐藏向量联结起来，再进行 sentiment 的 classification，其结构如下：

![](<assets/1697531658892.png>)

总结一下，RNN 可以有效的进行参数共享，并且可以处理任意长度的输入，但是它存在着梯度消失的问题，下一讲会详细讲解该问题以及为解决它产生的 RNN 模型的变种如 LSTM，GRU 等模型。

# lecture 7
这一讲主要研究 RNN 中梯度消失以及梯度爆炸问题，以及为解决梯度消失而设计的 RNN 模型的变种如 LSTM，GRU 等模型。

## **梯度消失**

RNN 理论上是可以捕捉较早的历史信息，但是由于 Vanishing Gradient 的问题会导致远程信息无法有效的被捕捉到。
RNN 的输入、输出及 hidden state 的关系有如下的公式表示：

$$h_t=f(Wh_{t-1}+W_xx_{t)}, \
\hat y_t=softmax(Uh_t)
$$

并且其损失函数为

$$J=\frac{1}{T}\sum_{t=1}^TJ_t ，\\ J_t=\sum_{j=1}^{|V|}y_{t,j}log\hat y_{t,j}$$

所以损失函数相对于 W 的梯度为

$$\frac{\partial J}{\partial W} = \sum_{t=1}^T\frac{\partial J_t}{\partial W} ,\\ \frac{\partial J_t}{\partial W} =\sum_{k=1}^t\frac{\partial J_t}{\partial y_t}\frac{\partial y_t}{\partial h_t}\frac{\partial h_t}{\partial h_k}\frac{\partial h_k}{\partial W}$$

其中 $$\frac{\partial h_t}{\partial h_k}=\prod_{j=k+1}^{t}\frac{\partial h_j}{\partial h_{j-1}}=\prod_{j=k+1}^{t}W^{t-k}\times diag[f'(h_{j-1})]$$

假设矩阵 W 的最大的本征值也小于 1，则 t-k 越大即其相距越远，其梯度会呈指数级衰减，这一问题被称作 vanishing gradient 梯度消失，它导致我们无法分辨 t 时刻与 k 时刻究竟是数据本身毫无关联还是由于梯度消失而导致我们无法捕捉到这一关联。这就导致了我们只能学习到近程的关系而不能学习到远程的关系，会影响很多语言处理问题的准确度。

## **梯度爆炸**

与梯度消失类似，由于存在矩阵 W 的指数项，如果其本征值大于 1，则随着时间越远该梯度指数级增大，这又造成了另一问题 exploding gradient 梯度爆炸，这就会造成当我们步进更新时，步进过大而越过了极值点，一个简单的解决方案是 gradient clipping，即如果梯度大于某一阈值，则在 SGD 新一步更新时按比例缩小梯度值，即我们仍然向梯度下降的方向行进但是其步长缩小，其伪代码如下：

![](<assets/1697532294492.png>)

不进行 clipping 与进行 clipping 的学习过程对比如下

![](<assets/1697532294655.png>)

可见左图中由于没有进行 clipping，步进长度过大，导致损失函数更新到一个不理想的区域，而右图中进行 clipping 后每次步进减小，能更有效的达到极值点区域。

梯度爆炸问题我们可以通过简单的 gradient clipping 来解决，那对于梯度消失问题呢？其基本思路是我们设置一些存储单元来更有效的进行长程信息的存储，LSTM 与 GRU 都是基于此基本思想设计的。

## **LSTM**

LSTM, 全称 Long Short Term Memory。其基本思路是除了 hidden state $h_t$ 之外，引入 cell state $c_t$ 来存储长程信息，LSTM 可以通过控制 gate 来擦除，存储或写入 cell state。

LSTM 的公式如下：

![](<assets/1697532294790.png>)

New Memory cell $\tilde c_t$通过将新的输入 $x_t$ 与代表之前的 context 的 hidden state $h_{t-1}$ 结合生成新的 memory。
Input gate $i_t$\ 决定了新的信息是否有保留的价值。

Forget gate $f_t$ 决定了之前的 cell state 有多少保留的价值。

Final memory cell $c_t$ 通过将 forget gate 与前 memory cell 作元素积得到了前 memory cell 需要传递下去的信息，并与 input gate 和新的 memory cell 的元素积求和。极端情况是 forget gate 值为 1，则之前的 memory cell 的全部信息都会传递下去，使得梯度消失问题不复存在。

output gate $o_t$ 决定了 memory cell 中有多少信息需要保存在 hidden state 中。

## **GRU**

GRU(gated recurrent unit) 可以看做是将 LSTM 中的 forget gate 和 input gate 合并成了一个 update gate，即下式中的 $z_{t}$，同时将 cell state 也合并到 hidden state 中。

![](<assets/1697532295123.png>)

Update Gate 决定了前一时刻的 hidden state 有多少可以保留到下一时刻的 hidden state。reset gate 决定了在产生新的 memory $\tilde h_{t}$ 时前一时刻 hidden state 有多少贡献。

虽然还存在 RNN 的很多其他变种，但是 LSTM 与 RNN 是最广泛应用的。最大的区别就是 GRU 有更少的参数，更便于计算，对于模型效果方面，两者类似。通常我们可以从 LSTM 开始，如果需要提升效率的话再准换成 GRU。

## **延伸**

梯度消失仅仅局限于 RNN 结构吗？实际上对于深度神经网络，尤其层数较多时，由于链式法则以及非线性函数的选择，梯度消失问题也会出现，导致底层的 layer 学习速度很慢，只不过由于 RNN 中由于不断的乘以相同的权重矩阵导致其梯度消失问题更显著。

对于非 RNN 的深度神经网络如前馈神经网络与卷积神经网络，为了解决梯度消失问题，通常可以引入更直接的联结使得梯度更容易传递。

例如在 ResNet 中，通过引入与前层输入一样的联结使得信息更有效的流动，这使得我们可以能够训练更深层的模型。这一联结被称作 Residual Connection 或 Skip Connection。

![](<assets/1697532295566.png>)

更进一步的，在 DenseNet 中，我们可以将几层之间全部建立直接联结，形成 dense connection。

![](<assets/1697532295920.png>)

下一讲中，我们还会看到更有效的捕捉长程信息的机制——Attention 模型, to be continued。
# lecture 8

这一讲研究 NLP 中一个重要的领域——机器翻译 Machine Translation。机器翻译顾名思义，就是将源语言中的文字转换成对应的目标语言中的文字的任务。早期的机器翻译很多是人为 rule-based 的，随后逐渐发展出依赖于统计信息的 Statistical Machine Translation（简称 SMT），之后又发展出利用神经网络使得准确度大幅改善的 Neural Machine Translation（NMT），它依赖于 Sequence-to-Sequence 的模型架构，以及进一步将其改善的 Attention 机制。

## **SMT** Statistical Machine Translation

`SMT 的核心思想是从数据中学习出概率模型`。假设我们想要从法语翻译成英语，我们希望给定输入的法语句子 $x$ ，找到对应最好的英语句子 $y$ , 即找到 $\underset{y}{\mathrm{argmax}}P(y|x)$

利用贝叶斯法则，这相当于求 $\underset{y}{\mathrm{argmax}}P(x|y)P(y)$, 其中 $P(x|y)$ 被称作 `translation model` 即模拟单词或词组该如何翻译，主要是处理局部的词组的翻译， $P(y)$ 被称作` language model` 即如何挑选这些单词或词组组成合理的目标语言中的句子。之前语言模型已总结过如何学习语言模型，而对于 translation model,  我们需要 parallel data，即大量的成对的语法、英语句子。

SMT 系统实际上要比这个复杂很多，而且需要很多 feature engineering，很难维护。而 NMT 的出现极大的改善了这些问题。

## **Neural Machine Translation 与 Seq2Seq**

NMT 依赖于 Sequence-to-Sequence 的模型架构，即`通过一个 RNN 作为 encoder 将输入的源语言转化为某表征空间中的向量，再通过另一个 RNN 作为 decoder 将其转化为目标语言中的句子`。我们可以将 decoder 看做预测目标句子 $y$ 的下一个单词的语言模型，同时其概率依赖于源句子的 encoding，一个将法语翻译成英语的 Seq2Seq 模型如下图所示。

![](<assets/1697533248209.png>)

训练过程中，损失函数与语言模型中类似，即各步中目标单词的 log probability 的相反数的平均值：

![](<assets/1697533248448.png>)

上例中的损失函数如下图所示，并且我们可以看出损失函数的梯度可以一直反向传播到 encoder，模型可以整体优化，所以 Seq2Seq 也被看做是 end2end 模型：

![](<assets/1697533248648.png>)

在做 inference 的时候，我们可以选择 greedy decoding 即每一步均选取概率最大的单词并将其作为下一步的 decoder input。

![](<assets/1697533248831.png>)

但是 greedy decoding 的问题是可能当前的最大概率的单词对于翻译整个句子来讲不一定是最优的选择，但由于每次我们都做 greedy 的选择我们没机会选择另一条整体最优的路径。

为解决这一问题，一个常用的方法是 `beam search decoding`，其基本思想是在 decoder 的每一步，我们不仅仅是取概率最大的单词，而是保存 k 个当前最有可能的翻译假设，其中 k 称作 beam size，通常在 5 到 10 之间。

对于 $y_1,...,y_t$的翻译，它的分数是

![](<assets/1697533249056.png>)

分数越高越好，但求和中的每一项都是负数，这会导致长的翻译分数更低，所以最后在选取整句概率最大的翻译时，要对分数做归一化

![](<assets/1697533249300.png>)

与 SMT 相较，NMT 的优势是我们可以整体的优化模型，而不是需要分开若干个模型各自优化，并且需要 feature engineering 较少，且模型更灵活，准确度也更高。其缺点是更难理解也更纠错，且很难设定一些人为的规则来进行控制。

## **BLEU**

对于机器翻译模型，我们如何来衡量它的好坏呢？一个常用的指标是 BLEU（全称是 **B**i**l**ingual **E**valuation **U**nderstudy)。BLEU 基本思想是看你 machine translation 中 n-gram 在 reference translation（人工翻译作为 reference）中相应出现的几率。

我们用

![](<assets/1697533249581.png>)

来表示 n-gram 的 precision score， $w_n=1/2^n$ 作为权重，另外引入对过短的翻译的 penalty $$\beta = e^{min(0,1-\frac{len_{ref}}{len_{MT}})}$$

BLEU score 可以表示为（其中 k 通常选为 4）

![](<assets/1697533249936.png>)

BLEU 可以较好的反应翻译的准确度，当然它也不是完美的，也有许多关于如何改进机器翻译 evaluation metric 方面的研究。

## **Attention**

观察 Seq2Seq 模型，我们会发现它有一个信息的瓶颈即我们需要将输入的所有信息都 encode 到 encoder 的最后一个 hidden state 上，这通常是不现实的，因此引入 Attention 机制来消除这一瓶颈：在 decoder 的每一步，通过与 encoder 的直接关联来决定当前翻译应关注的源语句的重点部分。
详细步骤可参考之前的总结文章 [Attention 机制详解（一）——Seq2Seq 中的 Attention](https://zhuanlan.zhihu.com/p/47063917)。





# 作业
## a4

![image-20240320193333390](C:\Users\86155\AppData\Roaming\Typora\typora-user-images\image-20240320193333390.png)![image-20240320193614284](C:\Users\86155\AppData\Roaming\Typora\typora-user-images\image-20240320193614284.png)![image-20240320193720819](C:\Users\86155\AppData\Roaming\Typora\typora-user-images\image-20240320193720819.png)![image-20240320194905402](C:\Users\86155\AppData\Roaming\Typora\typora-user-images\image-20240320194905402.png)
计算decoder的next_step![image-20240320204526157](C:\Users\86155\AppData\Roaming\Typora\typora-user-images\image-20240320204526157.png)
计算解码器每一个隐藏状态 $h_t$ 对应的  $\alpha_t$![image-20240320195102768](C:\Users\86155\AppData\Roaming\Typora\typora-user-images\image-20240320195102768.png)
