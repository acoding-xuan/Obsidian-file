# sentence masks

句子掩码（sentence masks）通常用于将文本分割成句子级别的段落或对文本中的句子进行个别处理。以下是一个示例，说明什么是句子掩码以及如何使用它来处理文本：

假设有以下文本段落：

"自然语言处理（NLP）是一项重要的研究领域，它涉及到文本分析、文本生成和情感分析等任务。NLP的应用非常广泛，包括机器翻译、聊天机器人和文本摘要。"

这个段落包含两个句子。您可以创建一个句子掩码来表示每个句子的起始和结束位置，如下所示：

- 原始文本：自然语言处理（NLP）是一项重要的研究领域，它涉及到文本分析、文本生成和情感分析等任务。NLP的应用非常广泛，包括机器翻译、聊天机器人和文本摘要。
- 句子掩码：[1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]

在这里，句子掩码中的1表示每个句子的起始位置，0表示句子中的其他位置。这个句子掩码可以用于以下任务：

1. **句子分割**：通过句子掩码，您可以将文本分割成句子级别的段落。对于上述示例，您可以使用句子掩码来分割原始文本，将其分为两个句子。
    
2. **句子级别的情感分析**：如果您想对每个句子进行情感分析，句子掩码将帮助您确定每个句子的起始和结束位置，以便将情感分析应用于每个句子。
    
3. **句子级别的注意力**：在某些任务中，如机器翻译，您可能需要模型关注源语言和目标语言中不同句子。句子掩码可以用于调整注意力机制，以确保模型在不同句子之间正确分配注意力。
    

这就是句子掩码的基本概念和用途。它们有助于处理文本中的句子级别信息，使您能够在自然语言处理任务中更精细地控制和分析文本的结构。
# n-gram
N-gram 是自然语言处理中的一种文本表示方法，它是由连续的 n 个单词（或字符）组成的序列。N-gram 在文本处理和语言模型中广泛应用。N 代表 n-gram 中包含的单词或字符的数量，通常是一个正整数。

N-grams 可以用于多种自然语言处理任务，包括文本处理、语言建模、机器翻译、信息检索、文本分类和语音识别等。以下是一些不同 n-gram 的示例：

1. **Unigram (1-gram)**：Unigram 是包含单个单词的 n-gram。例如，在句子 "I love natural language processing" 中，"I"、"love"、"natural"、"language"、"processing" 都是 Unigrams。
    
2. **Bigram (2-gram)**：Bigram 包含两个连续的单词。在相同的句子中，"I love"、"love natural"、"natural language"、"language processing" 都是 Bigrams。
    
3. **Trigram (3-gram)**：Trigram 包含三个连续的单词。在相同的句子中，"I love natural"、"love natural language"、"natural language processing" 都是 Trigrams。
    
4. **4-gram、5-gram 等**：同样，还有 4-gram、5-gram 等，包含更多连续的单词。
    

N-grams 可以用于文本建模，其中模型根据前面的单词来预测下一个单词，这在语言建模中非常有用。N-grams 也可以用于文本分类，其中模型可以识别文本中的特定短语或模式。在信息检索中，N-grams 可以用于查找文档中与查询匹配的短语。

# BLUE
BLEU（Bilingual Evaluation Understudy）是一种自动化评估机器翻译系统质量的指标，用于衡量机器翻译生成的文本与参考翻译之间的相似度。BLEU 往往用于自然语言处理任务中，特别是在机器翻译领域，以确定机器翻译系统的性能。

BLEU 评估机器生成的翻译文本与一个或多个人工参考翻译之间的相似度。它的得分通常在0到1之间，表示机器翻译的质量。BLEU 得分越接近1，表示机器翻译的性能越好。

https://blog.csdn.net/qq_30232405/article/details/104219396



# beam_search
Beam search（束搜索）是一种在自然语言处理和序列生成任务中广泛用于生成高质量输出序列的搜索算法。它通常用于机器翻译、文本生成、语音识别、语音合成等任务中，以确定最可能的输出序列。

Beam search 的核心思想是在生成序列时，同时保持多个备选输出序列，每个备选序列称为一个 "束"（beam）。在每个时间步骤，beam search 会根据当前的部分序列和模型的预测，生成多个备选的下一个词或标记，然后选择其中最有可能的一部分作为下一步的输入。这种方式可以减小搜索空间，提高生成序列的质量。

以下是 beam search 的基本工作步骤：

1. **初始化**：从起始状态开始，生成一个起始标记（如 `<start>`）并创建一个初始束（beam），将其添加到备选序列列表中。
    
2. **生成备选序列**：在每个时间步骤，对于每个束，根据当前的部分序列和模型的条件概率分布，生成多个备选的下一个词或标记。这通常涉及到使用模型进行预测，例如，使用序列到序列模型或语言模型。
    
3. **选择备选序列**：对于每个备选的下一个词，计算其条件概率，并选择具有最高概率的部分序列。然后，保留前几个高概率的备选序列，称为 "top-k" 或 "beam size"。这有助于控制搜索的广度。
    
4. **终止条件**：当生成的部分序列中包含终止标记（如 `<end>`）或达到一定的最大长度时，将该部分序列视为完成的输出。这些部分序列将被从备选序列列表中移除。
    
5. **重复生成**：重复上述步骤，直到所有备选序列都达到终止条件，或者达到了生成序列的最大长度。
    
6. **选择最终输出**：从所有已完成的备选序列中选择具有最高质量（通常是累积条件概率最高）的序列作为最终的输出序列。
    

Beam search 允许生成多个备选序列，从而更好地探索模型的输出空间。通常，选择适当的 beam 大小是一个重要的超参数，较大的 beam 大小可以提高搜索的广度，但也会增加计算成本。 Beam search 是一种常用的序列生成算法，用于生成高质量的自然语言文本和序列。
#  Hierarchical Softmax
[word2vec原理(二) 基于Hierarchical Softmax的模型 - 刘建平Pinard - 博客园 (cnblogs.com)](https://www.cnblogs.com/pinard/p/7243513.html)

　　　　我们先回顾下传统的神经网络词向量语言模型，里面一般有三层，输入层（词向量），隐藏层和输出层（softmax 层）。里面最大的问题在于从隐藏层到输出的 softmax 层的计算量很大，因为要计算所有词的 softmax 概率，再去找概率最大的值。这个模型如下图所示。其中 $V是词汇表的大小，

![](https://images2017.cnblogs.com/blog/1042406/201707/1042406-20170727105326843-18935623.png)

　　　　word2vec 对这个模型做了改进，首先，对于从输入层到隐藏层的映射，没有采取神经网络的线性变换加激活函数的方法，而是采用简单的对所有输入词向量求和并取平均的方法。比如输入的是三个 4 维词向量：$(1,2,3,4), (9,6,11,8),(5,10,7,12)$, 那么我们 word2vec 映射后的词向量就是 $(5,6,7,8)$。由于这里是从多个词向量变成了一个词向量。

　　　　第二个改进就是从隐藏层到输出的 softmax 层这里的计算量个改进。为了避免要计算所有词的 softmax 概率，word2vec 采样了霍夫曼树来代替从隐藏层到输出 softmax 层的映射。我们在上一节已经介绍了霍夫曼树的原理。如何映射呢？这里就是理解 word2vec 的关键所在了。

　　　　由于我们把之前所有都要计算的从输出 softmax 层的概率计算变成了一颗二叉霍夫曼树，那么我们的 softmax 概率计算只需要沿着树形结构进行就可以了。如下图所示，我们可以沿着霍夫曼树从根节点一直走到我们的叶子节点的词 $w_2$。

 ![](https://images2017.cnblogs.com/blog/1042406/201707/1042406-20170727105752968-819608237.png)

　　　　和之前的神经网络语言模型相比，我们的霍夫曼树的所有内部节点就类似之前神经网络隐藏层的神经元, 其中，根节点的词向量对应我们的投影后的词向量，而所有叶子节点就类似于之前神经网络 softmax 输出层的神经元，叶子节点的个数就是词汇表的大小。在霍夫曼树中，隐藏层到输出层的 softmax 映射不是一下子完成的，而是沿着霍夫曼树一步步完成的，因此这种 softmax 取名为 "Hierarchical Softmax"。

　　　　如何 “沿着霍夫曼树一步步完成” 呢？在 word2vec 中，我们采用了二元逻辑回归的方法，即规定沿着左子树走，那么就是负类(霍夫曼树编码 1)，沿着右子树走，那么就是正类(霍夫曼树编码 0)。判别正类和负类的方法是使用 sigmoid 函数，即：

$$P(+) = \sigma(x_w^T\theta) = \frac{1}{1+e^{-x_w^T\theta}}$$

　　　　其中 $x_w$是当前内部节点的词向量，而$\theta$则是我们需要从训练样本求出的逻辑回归的模型参数。

　　　　使用霍夫曼树有什么好处呢？首先，由于是二叉树，之前计算量为 $V$, 现在变成了 $log_2V$。第二，由于使用霍夫曼树是高频的词靠近树根，这样高频词需要更少的时间会被找到，这符合我们的贪心优化思想。

　　　　容易理解，被划分为左子树而成为负类的概率为 P(-) =  1-P(+)。在某一个内部节点，要判断是沿左子树还是右子树走的标准就是看 $P(-),P(+)$谁的概率值大。而控制 $P(-),P(+)$谁的概率值大的因素一个是当前节点的词向量，另一个是当前节点的模型参数$\theta$。

　　　　对于上图中的 $w_2$，如果它是一个训练样本的输出，那么我们期望对于里面的隐藏节点 $n(w_2,1)$的 $P(-)$概率大，$n(w_2,2)$的 $P(-)$概率大，$n(w_2,3)$的 $P(+)$概率大。

　　　　回到基于 Hierarchical Softmax 的 word2vec 本身，我们的目标就是找到合适的所有节点的词向量和所有内部节点$\theta$, 使训练样本达到最大似然。那么如何达到最大似然呢？
