# 专有名词和一些方法
## 蒸馏法
的蒸馏法（Distillation Method）是一种技术，通常用于提高深度学习推荐系统的性能。这个方法灵感来自于蒸馏（distillation）的概念，即从一个模型中提取知识并将其传递给另一个模型，以帮助后者更好地学习。在推荐系统中，通常有两个主要角色：

1. 教师模型（Teacher Model）：这是一个性能较高的推荐系统模型，它在大规模数据上进行训练，并且具有良好的推荐性能。
   
2. 学生模型（Student Model）：这是一个需要训练的推荐系统模型，通常是轻量级模型，目标是使其达到与教师模型相似的性能，但具有更小的计算和存储要求。
蒸馏法的基本思想是从教师模型中传输知识给学生模型，以便学生模型可以学到教师模型的"智慧"。这通常包括以下步骤：

1. **教师模型的训练**：首先，教师模型会在大规模数据上进行训练，以便获得高性能的推荐能力。
   
2. **教师模型的输出蒸馏**：学生模型会使用教师模型的输出作为辅助信息。通常，这些输出是概率分布，表示项目或内容的相关性。学生模型会努力模仿教师模型的输出，以获得相似的预测。
   
3. **学生模型的训练**：学生模型会在与教师模型相同或相似的数据上进行训练，但它的损失函数通常包括对教师模型输出的蒸馏目标，以及推荐任务的损失函数。
	    

通过蒸馏法，学生模型可以受益于教师模型的知识，提高自身性能，同时保持相对较小的模型规模，更适合在资源受限的环境中部署。这种方法在深度学习推荐系统领域已经取得了显著的成功，帮助改善了推荐性能和模型的效率。
## Session-based recommendations
会话式推荐是指一种推荐系统，它侧重于基于用户当前会话或一系列交互来提供个性化推荐。在这个背景下，"会话"通常代表用户在应用程序或网站中的短期互动，例如在单个会话中的一系列点击、搜索或查看。这些推荐旨在满足用户在当前会话中的即时兴趣，而不仅仅依赖于他们的长期偏好。\
## dropout
Dropout 是一种正则化技术，用于防止深度神经网络过拟合，提高模型的泛化能力。它的作用是在每个训练批次中随机将一部分神经元单元（通常是隐藏层的神经元）的输出设置为零，从而减少神经元之间的相互依赖性。这样可以促使模型不过分依赖某些特定的神经元，从而提高模型的鲁棒性。

Dropout 的计算方法如下：

1. **训练阶段**：
    - 随机选择一部分神经元单元，通常通过一个概率分布（通常是伯努利分布）来决定哪些神经元被保留，哪些被丢弃。这个概率通常称为"丢弃概率"，通常设为0.5。
      
    - 对于被选择的神经元单元，将其输出设置为零，即将它们关闭。这相当于将它们的激活值清零。
      
    - 对于每个训练批次，都要重复上述操作，以确保每个批次都使用了不同的神经元子集。
      
    - 在正向传播和反向传播中都使用了丢弃的神经元，但在反向传播时，将丢弃的神经元的梯度乘以零，以确保它们不会对权重的更新产生影响。
2. **推理（测试）阶段**：
        - 在模型推理或测试时，不再使用 Dropout。这意味着所有神经元都保持开启状态。
    Dropout 的主要作用包括：

- 减少过拟合：通过随机关闭神经元，Dropout 可以减少神经元之间的协同作用，从而减小了模型对特定训练样本的过度依赖，减少过拟合风险。
  
- 提高泛化性能：通过增加模型的鲁棒性，Dropout 帮助模型更好地适应不同数据分布，提高泛化性能。
  
- 防止神经元之间的协同适应：Dropout 鼓励神经元单元独立工作，防止它们过分适应训练数据。

需要注意的是，Dropout 的丢弃概率是一个超参数，通常需要根据具体问题和数据集来调整。一般来说，较小的丢弃概率可以提供更多的正则化效果，但如果太小，模型可能会欠拟合。

## e-commerce scenarios

"e-commerce scenariso" 指的是电子商务场景或情境，即在线购物领域中的各种应用和情境。这些情境涵盖了电子商务平台和在线零售商的各种业务操作和用户互动，包括以下内容(等等)：

1. **在线购物**: 用户在电子商务平台上搜索和购买商品或服务的情境。这可能包括在网上商店浏览产品、将商品添加到购物车、进行支付等一系列操作。
   
2. **支付和结算**: 这个场景涵盖了在线支付、交易结算和支付安全。这包括使用信用卡、数字钱包、支付网关等方式完成交易。
   
3. **商品推荐**: 电子商务平台通常使用推荐系统来为用户推荐个性化的商品。这些推荐可能基于用户的浏览历史、购买历史、兴趣和行为。
   
4. **用户评价和评论**: 用户可以在产品页面上撰写评价和评论，分享他们的购物经验和产品反馈。这些评论对其他购物者的购物决策可能产生影响。
   

这些场景反映了电子商务领域的多样性和用户与在线购物平台互动的各种方式。电子商务公司通常致力于提供优质的用户体验，促进销售，并确保顾客满意度。同时，这些场景也涉及到许多技术和业务挑战，包括推荐系统、支付处理、库存管理、客户服务等。

## bi-linear Matching Scheme
"Bilinear Matching Scheme" 是一种匹配方案，通常应用于自然语言处理（NLP）和信息检索领域，用于衡量两个文本序列（如查询和文档）之间的相似性或相关性。这种方案依赖于双线性模型，用于编码文本序列的信息并进行匹配。

在双线性匹配方案中，文本序列通常通过嵌入（例如词嵌入）表示成连续的向量。然后，双线性模型将这些向量进行组合以捕捉文本序列之间的交互。这通常涉及到以下步骤：

1. **文本嵌入**：将文本序列中的词或子词转化为向量表示，通常使用词嵌入（Word Embeddings）等技术。每个词对应于一个向量，将序列中的词向量连接或求和以获得整个序列的嵌入表示。

2. **双线性交互**：在此阶段，文本序列的嵌入表示被输入到双线性模型中。该模型执行矩阵乘法操作，以对两个序列的嵌入进行交叉。这个过程可被看作是在原始嵌入空间中进行的双线性交互，以捕捉不同序列之间的相互关系。

3. **匹配得分**：双线性交互生成的结果可以用作匹配得分，表示两个文本序列的相关性或相似度。通常，这些得分用于排序或分类任务，如文档检索、问答系统、语义匹配等。

双线性匹配方案的优点在于它可以捕捉到文本序列之间的复杂交互关系，从而更好地表达它们之间的语义相关性。这种方法在信息检索和自然语言处理任务中被广泛使用，特别是在需要模型捕捉文本语义匹配的情景下。

需要注意的是，具体的双线性匹配模型可能有不同的实现方式和变种，以满足不同任务的需求。这些模型可能包括一些额外的设计元素，如注意力机制、规范化等，以提高性能。

## implicit feedback

"Implicit feedback" 是指用户在不明确表示其喜好或反感的情况下，通过其行为或交互来暗示其兴趣或反感的信息。这与"explicit feedback" 相对，后者是用户明确提供的反馈，通常是评级、评论或喜好标签。

在推荐系统和信息检索领域，了解用户的兴趣至关重要。Implicit feedback 是一种从用户行为中隐含地获取这些信息的方法。一些示例包括：

1. **点击数据**：用户在网页上点击的内容，例如点击文章、产品或链接，可以被视为用户对这些内容的兴趣。
   
2. **浏览历史**：查看页面或滚动浏览的历史，尽管没有点击，也可以提供用户对内容的兴趣信息。
   
3. **停留时间**：用户在网页上停留的时间长短，可以暗示他们对页面内容的兴趣。长时间的停留通常表示更高的兴趣。
   
4. **购买历史**：用户过去的购买记录也可以被视为隐含的反馈，因为它们揭示了用户的偏好。
   
5. **搜索历史**：用户的搜索查询历史和搜索结果点击可以提供用户对特定主题或关键词的兴趣。
   
6. **应用使用情况**：在移动应用中，用户在应用内的行为，例如应用的打开频率、使用时间和功能的选择，都可以提供暗示的反馈。
   

Implicit feedback 对于推荐系统和个性化内容推荐非常重要，因为它通常更容易获得，无需用户明确地提供反馈。然而，处理隐性反馈数据也具有挑战，因为它需要建立模型来推断用户的兴趣，而这些推断可能不总是准确的。因此，许多推荐系统使用隐性反馈数据与明示反馈数据结合，以更好地理解用户的需求和提供更准确的建议。

## recall 和 recall@K
1. recall 的计算。 其实就是预测结果的正确率
	"Recall" 是信息检索和评估分类模型性能的一项重要指标，它度量了系统在所有实际正例中成功检索到多少个正例。Recall 计算方法如下：
	
	Recall = (True Positives) / (True Positives + False Negatives)
	
	其中：
	- "True Positives"（真正例）表示模型正确地识别为正例的样本数量。
	- "False Negatives"（假负例）表示实际为正例但模型错误地识别为负例的样本数量。
	eg:
	现在，我们有一个数据集，包含了 100 名患者的医疗检查结果，其中实际有 20 名患者患有恶性肿瘤。我们的模型对这些患者进行了诊断，结果如下：
	- 模型正确识别了 15 名患者患有恶性肿瘤（True Positives）。
	- 模型错误地将 5 名患者诊断为没有恶性肿瘤，但实际上他们患有恶性肿瘤（False Negatives）。
	现在，我们可以使用 Recall 的计算方法来评估模型的性能：
	Recall = (True Positives) / (True Positives + False Negatives)
	Recall = 15 / (15 + 5) = 15 / 20 = 0.75

2. recall@k
	recall@k 实际上是指查全率。 即topk个物品中 命中的物品数。
	即：Recall@k = (True Positives) / (Positives)
	假设我们有一个测试用户 "UserA"，该用户已经观看了以下三部电影，这些电影是他实际感兴趣的：
	1. "Movie1"
	2. "Movie2"
	3. "Movie3"
	现在，我们的推荐系统为用户 "UserA" 生成了以下 5 个电影的推荐列表，按照推荐优先级的顺序：
	1. "Movie4"
	2. "Movie1"
	3. "Movie5"
	4. "Movie2"
	5. "Movie6"
	我们将计算 Recall@5，即在前 5 个推荐电影中有多少是用户实际感兴趣的。
	1. 首先，我们查看用户的实际感兴趣项目列表，其中包括 "Movie1"、"Movie2" 和 "Movie3"。这些是用户的 "Positives"。
	2. 接下来，我们看用户 "UserA" 在前 5 个推荐中实际感兴趣的项目。从列表中可以看出，用户实际感兴趣的电影 "Movie1" 和 "Movie2" 分别出现在推荐的第 2 和第 4 位。这些是 "True Positives"。
	3. 现在，我们可以使用 Recall@5 的计算公式：
	    Recall@5 = (True Positives) / (Positives)
	    Recall@5 = 2 / 3 ≈ 0.67


# 论文
## Improved Recurrent Neural Networks for Session-based REC

本文主要在原始的 gru rec 的基础上进行了几点优化

1. data augmentation 主要通过以下两个方面
* 通过将所有的前缀数据都当成训练的数据，增加了训练的数据量。
* 对通过embedding layer 后的 vector 进行 dropout ，从而达到随机删除序列，防止过拟合。
![[Pasted image 20231015204430.png]]

2. Adapting to temporal changes
核心思想：通过设置一个阈值，使模型更注重与最近的数据。并且使用了pre-train 的方法防止以前的数据被两浪费。
pre-train: 先通过所有的数据进行训练。 再在之前训练好的模型的基础上 对满足不超过长度阈值的序列再次进行训练。

## Neural Attentive Session-based Recommendation
![[Pasted image 20231015214821.png]]
核心思想：
part1 Encoder : 通过Global 和 local encoder 分别对序列提取序列信息和 main propose 信息。并将两者进行结合。

part2 Decoder: 为了减少全连接层的参数数目。使用了 bi-linear Matching Scheme.
就是将 item进行embedding 后， 再将 embedding 后的 向量和 c 进行 相似性计算。从而减少了参数。
从  we reduce the number of parameters from |N| ∗ |H| to |D| ∗ |H|,
D是item 嵌入后 的向量维度。 H 是 item 的数目  N 是 $c_t$ 的维度。


## Filter-enhanced MLP is All You Need for Sequential Recommendation

核心思想：为了防止过拟合，提出了一种带有可学习性Filter的序列化推荐模型。

![image-20240116133340797](C:\Users\86155\AppData\Roaming\Typora\typora-user-images\image-20240116133340797.png)



question:  计算cross_entropy 的方法和原来的不太确定；



数据的形式也不太相同。





处理的方法不一致，这里为什么用99 个负样本。？？？

# 推荐系统代码

## gru4_rec

![image-20230729192725417](C:\Users\86155\AppData\Roaming\Typora\typora-user-images\image-20230729192725417.png)

```python
flops 计算参数的个数
```

注意 embedding层的作用


![image-20230730001711442](C:\Users\86155\AppData\Roaming\Typora\typora-user-images\image-20230730001711442.png)
### unpersonalized MC model

非个性化的MC模型通过分析历史数据，计算不同商品之间的转换概率，基于它们的共现模式进行推断。它假设从一个商品过渡到另一个商品的概率仅取决于该转换在数据中的出现频率。
## the difference between Metrics Function and Loss Function
#### 一、简介
在深度学习或者机器学习领域中，评分函数（scoring function）主要有三个目的：
1. 性能评估（Performance evaluation）：简单说就是我们的模型做得怎么样？也就是需要一个指标来比较不同的模型的效果；
2. 模型优化：也就是通过什么方法可以提升模型的效果，优化模型的性能；
这两个东西既互相联系，也有一定的区别。也于我们说的这两个函数（指标函数与损失函数）之间有很大的关系。而我们上面所述的两个函数也就是评分函数下两个类别。

#### 二、性能评估（Performance evaluation）

一个性能指标告诉我们，我们的模型做得如何。性能评估的目标是让一个人（你、我、不管是谁）读到这个分数并掌握一些关于我们模型的信息。

衡量标准应该设计得对人有意义，并有效地传达信息。

这里的指标函数就是用来描述模型性能的东西。例如，均方根误差（Mean squared error，MSE）就是一个非常有名的指标函数（metrics），通过这个指标，我们可以清楚看到我们预测的结果与实际值之间的偏差。因此，指标函数（metrics）就是用来度量模型性能的函数。

#### 三、模型优化（Model optimization）

模型优化是我们另一个关注的重点内容。也是评分函数的另一个应用。这里需要用到的就是损失函数（loss function）了。

当你通过数据拟合一个模型时，你基本上是在微调一些参数，这些参数决定了模型的预测能力，以便它尽可能地接近你的数据。

损失函数就是用来描述你模型预测性能变化的东西。当我们的模型在某一个时刻有了一个预测能力的时候，损失函数会告诉模型参数应该朝哪个方向变化。因此，需要注意的是，损失函数需要能够指导我们的优化算法对模型进行优化。以最常用的梯度下降（gradient descent）为例，在这种优化器下，损失函数必须是可微分的（differentiable）。显然，并不是所有的metrics都是可微的。因此，虽然metrics能够度量模型的性能，但是如果它不能帮助我们指导模型优化方向，那就不是损失函数。当然，我们目前常见的大多数指标函数都可以当损失函数使用。

经过这种分析，我们还可以看到，大部分时候我们可以根据我们的需要自定义一个指标函数来度量模型的性能，但是却不能任意用一个损失函数来作为模型优化的参数。因为度量函数只需要符合我们的目标，可以评估模型好坏即可，但是损失函数的设计会影响的模型优化的效果，例如损失函数是否平滑等对于优化器来说都十分重要。因此，损失函数的设计必须易于优化且与优化的目标一致。

## 注意力机制

注意力机制（Attention Mechanism）是神经网络模型中的一个重要组成部分，尤其在自然语言处理领域中发挥着关键作用。它允许模型在执行任务时动态地集中注意力于输入数据的不同部分。

在序列到序列模型中，比如机器翻译或文本摘要，注意力机制让模型在生成每个输出元素时能够有选择性地关注输入序列的不同部分。模型不再仅仅依赖于对整个输入序列的固定长度表示，而是能够在生成过程的每个步骤中自适应地关注相关的输入部分。

注意力机制的工作原理是根据当前的解码器状态（或输出）与相应的编码器状态（或输入）之间的相似度，计算每个输入元素的注意力权重。这些注意力权重确定了每个解码步骤中每个输入元素的重要性或相关性。然后，模型将加权的输入元素表示，即上下文向量，结合起来生成输出。

注意力机制的主要好处是能够捕捉长程依赖关系，同时在生成输出时考虑局部和全局信息。这有助于提高模型处理长序列的能力，并更有效地处理翻译或摘要等任务。

总体而言，注意力机制在各种自然语言处理任务中起到了至关重要的作用，提升了神经网络模型的性能和灵活性。它已经成为基于Transformer架构等最先进模型中不可或缺的组件，使它们在机器翻译、问答和文本摘要等任务上取得了令人瞩目的成果。

## **Full parametrization**

Full parametrization（全参数化）是指将一个系统或模型的所有参数以明确的方式进行表示和描述的过程。通过全参数化，可以完整地捕捉系统的特征和行为，并方便后续的分析、模拟和优化。

## basket data

推荐系统中的basket data指的是购物篮数据，也称为交易数据或订单数据。它记录了用户在一次购物中所购买的物品或商品。这些数据对于推荐系统的发展和训练非常重要，因为它可以揭示用户的偏好和购买行为，从而提供个性化的推荐。



非个性化的MC（Markov Chain）模型是一种推荐模型，不考虑用户的个人偏好或特点。它是一种基于商品在操作序列中的转换统计分析的简单概率模型。

非个性化的MC模型通过分析历史数据，计算不同商品之间的转换概率，基于它们的共现模式进行推断。它假设从一个商品过渡到另一个商品的概率仅取决于该转换在数据中的出现频率。

例如，如果大量购买商品A的用户也购买了商品B，该模型会给予从A到B的转换更高的概率。

然而，该模型并不考虑个别用户的特定偏好或兴趣。它将所有用户视为具有类似偏好，并仅基于共现频率推荐商品。因此，非个性化的MC模型生成的推荐可能不太符合用户的个体需求。

非个性化的MC模型相对简单且计算效率高，适用于个性化不是关键要求或者用户数据有限的情况。然而，为了更准确和个性化的推荐，通常会采用考虑用户偏好的模型，例如协同过滤或基于内容的方法。

## narm

###  关键函数解析

```python
    def forward(self, item_seq, item_seq_len):
        item_seq_emb = self.item_embedding(item_seq)
        item_seq_emb_dropout = self.emb_dropout(item_seq_emb)
        gru_out, _ = self.gru(item_seq_emb_dropout)

        # fetch the last hidden state of last timestamp   取出global最后位置的输出向量
        c_global = ht = self.gather_indexes(gru_out, item_seq_len - 1) # 最后的ht 刚好就是 c_global的output
        # avoid the influence of padding 将输出
        mask = item_seq.gt(0).unsqueeze(2).expand_as(gru_out) # ba
        q1 = self.a_1(gru_out)
        q2 = self.a_2(ht)
        q2_expand = q2.unsqueeze(1).expand_as(q1)
        # calculate weighted factors α
        alpha = self.v_t(mask * torch.sigmoid(q1 + q2_expand))
        c_local = torch.sum(alpha.expand_as(gru_out) * gru_out, 1) # 按照
        c_t = torch.cat([c_local, c_global], 1)
        c_t = self.ct_dropout(c_t)
        seq_output = self.b(c_t)
        return seq_output
```

在人工智能领域，“mask”（掩码）通常指的是一种用于隐藏或标记特定信息的技术或操作。具体来说，"mask"可以有以下几种代表意思：

1. 数据掩码：在数据处理中，"mask"可以用于标记或隐藏某些数据。例如，在自然语言处理任务中，可以使用掩码来标记句子中的特定单词或实体，以便进行后续的处理或分析。
2. 掩码操作：在神经网络中，"mask"可以用于对模型的输入或输出进行操作。例如，在序列模型（如循环神经网络或Transformer）中，可以使用掩码来屏蔽某些输入或输出位置，以便在处理序列数据时忽略或处理特定的位置。
3. 掩码学习：在深度学习中，"mask"可以用于学习模型的参数或权重。通过将某些参数的值设置为0或1，可以实现对模型的部分参数进行掩码，从而控制模型的学习过程或减少模型的复杂度。

总的来说，"mask"在人工智能领域可以表示数据的标记或隐藏、对模型输入输出的操作，以及模型参数的掩码学习等含义。具体的意义取决于上下文和具体的应用场景。



## 时间推荐（Temporal Recommendation）

时间推荐（Temporal Recommendation）指的是根据用户的偏好和数据的时间特征，为用户提供个性化的推荐。它考虑到用户偏好和物品可用性的时间依赖性，以提供更相关和及时的推荐。

传统的推荐系统通常假设用户的偏好是静态的，不会随时间改变。然而，在现实中，用户的偏好是动态的，受到各种因素的影响，如趋势、季节性和个人情况。时间推荐模型旨在捕捉这些时间动态，并提供与用户当前偏好更加一致的推荐。

时间推荐有几种方法：

1. 基于时间感知的协同过滤（Time-aware Collaborative Filtering）：协同过滤是一种流行的推荐技术，利用用户与物品的交互来进行推荐。基于时间感知的协同过滤通过加入时间信息，如用户与物品交互的时间戳，扩展了这种方法。它认为最近的交互比较旧的交互更具信息量，并相应地给予它们不同的权重。
2. 序列推荐（Sequential Recommendation）：序列推荐模型专注于捕捉用户行为的序列模式。它们考虑用户与物品的交互顺序，并利用这些信息进行推荐。循环神经网络（RNN）及其变种，如长短期记忆（LSTM）网络，通常用于建模推荐系统中的序列模式。
3. 基于趋势的推荐（Trend-based Recommendation）：基于趋势的推荐模型旨在识别和利用用户偏好的时间趋势。它们分析历史数据，以检测物品流行度随时间的变化模式。通过了解这些趋势，模型可以推荐当前流行或与用户在特定时间段内的历史偏好相符的物品。
4. 上下文感知推荐（Context-aware Recommendation）：上下文感知推荐系统考虑用户与物品交互周围的上下文信息，包括时间上下文。上下文可以包括时间、星期几、地点和天气等因素。通过融入时间上下文，系统可以提供根据特定时间条件个性化的推荐。

时间推荐技术在物品的相关性随时间变化的领域特别有用，如新闻文章、电影、时尚和活动推荐。

## Self-Attentive Sequential Recommendation

![image-20230901141813453](C:\Users\86155\AppData\Roaming\Typora\typora-user-images\image-20230901141813453.png)

自注意力序列推荐（Self-attentive Sequential Recommendation）是一种利用自注意力机制进行序列推荐的方法。它结合了序列模型和自注意力机制，以捕捉用户行为序列中的重要关系和上下文信息。
在传统的序列推荐中，通常使用循环神经网络（RNN）或长短期记忆（LSTM）等模型来建模用户行为序列。然而，这些模型在长序列中可能会面临信息丢失或难以捕捉长距离依赖关系的问题。
自注意力机制通过计算序列中不同位置的注意力权重，能够自动学习序列中的关联程度，并将重要的上下文信息传递给推荐模型。它不受序列长度的限制，能够捕捉长距离的依赖关系，并且可以对序列中的不同部分分配不同的权重。
自注意力序列推荐模型通常包括以下步骤：
1. 嵌入层（Embedding Layer）：将用户历史行为序列和物品信息转换为低维度的向量表示，以便于模型学习。
2. 自注意力层（Self-Attention Layer）：通过计算序列中不同位置的注意力权重，捕捉序列中的关联程度。这可以通过计算每个位置与其他位置的相似度得到，然后应用softmax函数获得注意力权重。
3. 上下文编码层（Context Encoding Layer）：根据自注意力层计算得到的注意力权重，将重要的上下文信息编码为固定长度的向量表示。这可以通过对序列中的向量进行加权求和来实现。
4. 预测层（Prediction Layer）：使用编码后的上下文向量表示进行推荐预测。可以使用全连接层或其他方法将上下文向量映射到预测目标空间，如物品推荐或评分预测。
自注意力序列推荐模型能够更好地捕捉用户行为序列中的长期依赖关系和上下文信息，从而提高推荐的准确性和个性化程度。它在许多序列推荐任务中取得了良好的性能，并在推荐系统领域得到了广泛应用。

![image-20230901143302115](C:\Users\86155\AppData\Roaming\Typora\typora-user-images\image-20230901143302115.png)
## residual connections
残差连接（Residual connections）是深度神经网络中的一种技术，旨在解决训练深层网络时出现的梯度消失或梯度爆炸的问题。它通过引入跳跃连接来连接网络中的不同层，使得网络可以更有效地传递梯度和学习更复杂的表示。
在传统的神经网络中，每个层的输入都是前一层的输出，通过层与层之间的连接进行传递。然而，在深层网络中，当通过多个层进行前向传播时，梯度可能会逐渐变小，导致梯度消失的问题。这会使得深层网络难以训练，限制了网络的深度和性能。
残差连接通过在网络中添加跳跃连接来解决这个问题。它允许网络直接跳过一些层，将前一层的输出添加到后续层的输入中。这样，在进行前向传播时，网络可以学习残差，即当前层相对于前一层的变化。这种方式使得网络可以更容易地学习到较小的变化，而不会受到梯度消失的影响。
残差连接的优点包括：
1. 缓解梯度消失问题：通过直接传递梯度，允许网络更有效地训练深层网络，使得网络可以更深，并且更容易收敛。
2. 更深的网络表示能力：通过残差连接，网络可以学习更复杂的表示，从而提升网络的性能和表达能力。
3. 网络设计的灵活性：残差连接使得网络设计更加灵活，可以根据任务需求和网络结构进行适当的添加或移除。
残差连接被广泛应用于深度学习领域，特别是在诸如残差网络（ResNet）和变换器（Transformer）等网络结构中，取得了显著的性能提升。
## Layer Normalization
层归一化（Layer Normalization）是一种用于深度神经网络的归一化技术，旨在解决神经网络训练中的内部协变量偏移问题。它对网络中的每个样本进行归一化，使得网络在不同样本上的输出具有相似的分布。

传统的批归一化（Batch Normalization）技术是在每个批次的数据上进行归一化，通过计算批次内样本的均值和方差来归一化输入。然而，批归一化在处理序列数据或小批次训练时可能会引入额外的噪声和不稳定性。

层归一化则是在网络的每个层上进行归一化，而不是在批次上。它计算每个样本在特征维度上的均值和方差，并将其用于归一化。具体而言，对于网络中的每个样本，层归一化通过以下公式对输入进行归一化：

Copy

```
y = (x - μ) / σ
```

其中，y是归一化后的输出，x是原始输入，μ是均值，σ是标准差。这样，每个样本都可以在特征维度上具有零均值和单位方差。

层归一化的主要优点包括：
1. 较少的依赖性：相比于批归一化，层归一化对于批次大小的依赖性更小，因此在处理序列数据或小批次训练时更为稳定。
2. 更强的泛化能力：由于层归一化在每个样本上进行归一化，它能够更好地适应不同样本的分布，从而提高网络的泛化能力。
3. 不受训练集大小限制：与批归一化相比，层归一化不受训练集大小的限制，因此可以在小型数据集上进行有效的训练。

层归一化已广泛应用于深度神经网络中，特别是在自然语言处理（NLP）任务和图像生成任务中，取得了显著的改进。它可以作为网络中的一层操作，嵌入到不同类型的网络结构中，如卷积神经网络（CNN）和循环神经网络（RNN）等。

## Dropout

Dropout是一种用于深度神经网络的正则化技术，旨在减少过拟合（overfitting）现象，提高模型的泛化能力。它通过在网络的训练过程中随机地丢弃（dropout）一部分神经元的输出，强制网络在不同的子集上进行学习，从而减少神经元之间的共适应性。

在Dropout中，每个神经元在训练过程中以概率p被保留，以概率1-p被丢弃。对于每个训练样本，被丢弃的神经元将不会对前向传播或反向传播中的信息传递产生影响。这样，网络在训练过程中会变得更加鲁棒，因为它不依赖于特定的神经元的存在。

Dropout的主要优点包括：

1. 减少过拟合：Dropout通过随机丢弃神经元的输出，减少了神经元之间的共适应性，从而减少了过拟合现象，提高了模型的泛化能力。
2. 防止神经元依赖：Dropout迫使网络在不同的子集上学习，使得每个神经元都需要学会独立地对输入进行有意义的响应，而不是依赖其他特定的神经元。
3. 提高模型鲁棒性：通过随机丢弃神经元，Dropout使网络对于输入中的噪声和干扰变得更加鲁棒，从而提高了模型的鲁棒性。

需要注意的是，在测试阶段，Dropout不会应用于网络，而是将所有神经元的输出按照训练阶段的概率进行缩放。这是因为在测试阶段，我们希望使用所有的神经元来进行预测，以获得更准确的结果。

Dropout已广泛应用于深度神经网络中，并且在提高模型性能和减少过拟合方面取得了显著的效果。它可以与不同类型的神经网络结构一起使用，包括卷积神经网络（CNN）、循环神经网络（RNN）和全连接神经网络等。

## Bidirectional Attention Mask

Bidirectional Attention Mask（双向注意力掩码）是在自然语言处理（NLP）中使用的一种技术，用于在模型中同时考虑上下文信息和当前输入的关键信息。

在NLP任务中，如机器翻译、问答系统等，理解上下文是非常重要的。传统的注意力机制（Attention）通常只考虑上下文中与当前位置相关的信息，而忽略了当前位置对上下文的影响。双向注意力掩码通过在注意力计算中引入额外的掩码机制，同时考虑上下文信息和当前输入，以更全面地捕捉输入之间的关系。

双向注意力掩码的关键思想是在计算注意力权重时，引入两个掩码矩阵：前向掩码和后向掩码。前向掩码将当前位置之后的输入掩盖，以防止当前位置受到后续输入的影响。后向掩码则将当前位置之前的输入掩盖，以防止当前位置受到之前输入的影响。通过同时考虑两个方向的掩码，双向注意力掩码可以实现对整个上下文的全面关注。

**使用双向注意力掩码的典型模型是Transformer模型，它是一种基于自注意力机制的神经网络模型，广泛应用于各种NLP任务中。在Transformer中，通过引入前向和后向的掩码矩阵，模型能够同时利用上下文的前后信息，从而更好地理解输入序列之间的关系。**

双向注意力掩码的优点包括：

1. 全面考虑上下文：双向注意力掩码能够同时考虑当前位置的前后上下文信息，从而更好地捕捉输入之间的联系和依赖关系。
2. 提高模型性能：通过引入双向掩码，模型可以更全面地感知输入序列的信息，从而提高模型在各种NLP任务中的性能。
然而，双向注意力掩码也可能增加模型的计算和存储成本，因为需要计算和存储额外的掩码矩阵。
总结而言，双向注意力掩码是在NLP中使用的一种技术，通过同时考虑当前位置的前后上下文信息，提供了更全面的输入表示。它在各种NLP任务中可以提高模型性能，特别是对于需要全面理解上下文的任务。
## Transformer


## Bert

## Questions

#### 关于负采样

negative sampling is necessary, such as setting `--train_neg_sample_args="{'distribution': 'uniform', 'sample_num': 1}"`  可以指定大小。

- `train_neg_sample_args (dict)` : This parameter have 4 keys: `distribution`, `sample_num`, `dynamic`, and `candidate_num`.
  - `distribution (str)` : decides the distribution of negative items in sampling pools. Now we support two kinds of distribution: `['uniform', 'popularity']`. `uniform` means uniformly select negative items while `popularity` means select negative items based on their popularity (Counter(item) in .inter file). The default value is `uniform`.
  - `sample_num (int)` : decides the number of negative samples we intend to take. The default value is `1`.  
  - `dynamic (bool)` : decides whether we adopt dynamic negative sampling. The default value is `False`.
  - `candidate_num (int)` : decides the number of candidate negative items when dynamic negative sampling. The default value is `0`.



## Recbole

### gru4rec

- `embedding_size (int)` : The embedding size of items. Defaults to `64`.

- `hidden_size (int)` : The number of features in the hidden state. Defaults to `128`.
- `num_layers (int)` : The number of layers in GRU. Defaults to `1`.
- `dropout_prob (float)`: The dropout rate. Defaults to `0.3`.

* `loss_type (str)` : The type of loss function. default `'CE'`, 



### FPMC

- `embedding_size (int)` : The embedding size of users and items. Defaults to `64`.
- `loss_type (str)` : The type of loss function. It is set to `'BPR'`,



### NARM

- `embedding_size (int)` : The embedding size of items. Defaults to `64`.
- `hidden_size (int)` : The number of features in the hidden state. Defaults to `128`.
- `n_layers (int)` : The number of layers in GRU. Defaults to `1`.
- `dropout_probs (list of float)` : The dropout rate, there are two values, the former is for embedding layer and the latter is for concatenation of the vector obtained by the local encoder and the vector obtained by the global encoder. Defaults to `[0.25,0.5]`.
- loss_type (str) default 'CE'

### SASRec

- `hidden_size (int)` : The number of features in the hidden state. **It is also the initial embedding size of item**. 

  Defaults to `64`.

- `inner_size (int)` : The inner hidden size in feed-forward layer. Defaults to `256`.

- `n_layers (int)` : The number of transformer layers in transformer encoder. Defaults to `2`.

- `n_heads (int)` : The number of attention heads for multi-head attention layer. Defaults to `2`.

- `hidden_dropout_prob (float)` : The probability of an element to be zeroed. Defaults to `0.5`.

- `attn_dropout_prob (float)` : The probability of an attention score to be zeroed. Defaults to `0.5`.

- `hidden_act (str)` : The activation function in feed-forward layer. Defaults to `'gelu'`. Range in `['gelu', 'relu', 'swish', 'tanh', 'sigmoid']`.

- `layer_norm_eps (float)` : A value added to the denominator for numerical stability. Defaults to `1e-12`.

- `initializer_range (float)` : The standard deviation for normal initialization. Defaults to 0.02``.

### Bert4Rec

- `hidden_size (int)` : The number of features in the hidden state. It is also the initial embedding size of items. Defaults to `64`.
- `inner_size (int)` : The inner hidden size in feed-forward layer. Defaults to `256`.
- `n_layers (int)` : The number of transformer layers in transformer encoder. Defaults to `2`.
- `n_heads (int)` : The number of attention heads for multi-head attention layer. Defaults to `2`.
- `hidden_dropout_prob (float)` : The probability of an element to be zeroed. Defaults to `0.5`.
- `attn_dropout_prob (float)` : The probability of an attention score to be zeroed. Defaults to `0.5`.
- `hidden_act (str)` : The activation function in feed-forward layer. Defaults to `'gelu'`. Range in `['gelu', 'relu', 'swish', 'tanh', 'sigmoid']`.
- `layer_norm_eps (float)` : A value added to the denominator for numerical stability. Defaults to `1e-12`.
- `initializer_range (float)` : The standard deviation for normal initialization. Defaults to `0.02`.
- `mask_ratio (float)` : The probability for a item replaced by MASK token. Defaults to `0.2`.
- loss_type (str) Defaults 'CE'





## 数据集

Our data module designs an elegant data flow that transforms raw data into the model input。

RecBole designs an input data format called Atomic Files. `以原子形式存储数据`



- `metrics (list or str)`: Evaluation metrics. Defaults to `['Recall', 'MRR', 'NDCG', 'Hit', 'Precision']`. Range in the following table:

  | Type          | Metrics                                                      |
  | ------------- | ------------------------------------------------------------ |
  | Ranking-based | Recall, MRR, NDCG, Hit, MAP, Precision, GAUC, ItemCoverage, AveragePopularity, GiniIndex, ShannonEntropy, TailPercentage |
  | value-based   | AUC, MAE, RMSE, LogLoss                                      |

  Note that value-based metrics and ranking-based metrics can not be used together.

- `topk (list or int or None)`: The value of k for topk evaluation metrics. Defaults to `10`.
- `valid_metric (str)`: The evaluation metric for early stopping. It must be one of used `metrics`. Defaults to `'MRR@10'`.
- `eval_batch_size (int)`: The evaluation batch size. Defaults to `4096`. **evaluate** 过程使用的数据集大小
- `metric_decimal_place(int)`: The decimal place of metric scores. Defaults to `4`.



### 关于evaluation

- `eval_args (dict)`: This parameter have 4 keys: `group_by`, `order`, `split`, and `mode`, which respectively control the data grouping strategy, data ordering strategy, data splitting strategy and evaluation mode for model evaluation.
  - `group_by (str)`: decides how we group the data in .inter. Now we support two kinds of grouping strategies: `['user', 'none']`. If the value of `group_by` is `user`, the data will be grouped by the column of `USER_ID_FIELD` and split in user dimension. If the value is `none`, the data won’t be grouped. **The default value is `user`.**  
  - `order (str)`: decides how we sort the data in .inter. Now we support two kinds of ordering strategies: `['RO', 'TO']`, which denotes the random ordering and temporal ordering. For `RO`, we will shuffle the data and then split them in this order. For `TO`, we will sort the data by the column of TIME_FIELD in ascending order and the split them in this order. **The default value is `RO`.**
  - `split (dict)`: decides how we split the data in .inter. Now we support two kinds of splitting strategies: `['RS','LS']`, which denotes the ratio-based data splitting and leave-one-out data splitting. If the key of `split` is `RS`, you need to set the splitting ratio like `[0.8,0.1,0.1]`, `[7,2,1]` or `[8,0,2]`, which denotes the ratio of training set, validation set and testing set respectively. If the key of `split` is `LS`, now we support three kinds of `LS` mode: `['valid_and_test', 'valid_only', 'test_only']` and you should choose one mode as the value of `LS`. The default value of `split` is `{'RS': [0.8,0.1,0.1]}`.
  - `mode (str)`: decides the data range which we evaluate the model on. Now we support four kinds of evaluation mode: `['full','unixxx','popxxx','labeled']`. `full` , `unixxx` and `popxxx` are designed for the evaluation on implicit feedback (data without label). For implicit feedback, we regard the items with observed interactions as positive items and those without observed interactions as negative items. `full` means evaluating the model on the set of all items. `unixxx`, for example `uni100`, means uniformly sample 100 negative items for each positive item in testing set, and evaluate the model on these positive items with their sampled negative items. `popxxx`, for example `pop100`, means sample 100 negative items for each positive item in testing set based on item popularity (`Counter(item)` in .inter file), and evaluate the model on these positive items with their sampled negative items. Here the xxx must be an integer. For explicit feedback (data with label), you should set the mode as `labeled` and we will evaluate the model based on your label. The default value is `full`.
