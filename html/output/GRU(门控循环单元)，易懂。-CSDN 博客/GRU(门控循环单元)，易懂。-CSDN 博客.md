---
url: https://blog.csdn.net/Michale_L/article/details/122778270?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522171325167116777224480322%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=171325167116777224480322&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-2-122778270-null-null.142^v100^pc_search_result_base1&utm_term=GRU&spm=1018.2226.3001.4187
title: GRU(门控循环单元)，易懂。-CSDN 博客
date: 2024-04-16 15:20:22
tag: 
summary: 
---
一、什么是 [GRU](https://so.csdn.net/so/search?q=GRU&spm=1001.2101.3001.7020)？

GRU（Gate Recurrent Unit）是[循环神经网络](https://so.csdn.net/so/search?q=%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&spm=1001.2101.3001.7020)（RNN）的一种，可以解决 RNN 中不能长期记忆和反向传播中的梯度等问题，与 LSTM 的作用类似，不过比 LSTM 简单，容易进行训练。

二、GRU 详解

GRU 模型中有两个门，重置门和更新门，具体作用后面展开说。

先来看一张 GRU 的图，看不懂没关系，后面慢慢展开说。

![](<assets/1713252022476.png>)

符号说明：

![](<assets/1713252044585.png>)

: 当前时刻输入信息 

![](<assets/1713252043635.png>)

: 上一时刻的隐藏状态。隐藏状态充当了神经网络记忆，它包含之前节点所见过的数据的信息

![](<assets/1713252043824.png>)

: 传递到下一时刻的隐藏状态

![](<assets/1713252043961.png>)

: 候选隐藏状态

![](<assets/1713252044099.png>)

: 重置门

![](<assets/1713252044249.png>)

: 更新门

![](<assets/1713252044387.png>)

:sigmoid 函数，通过这个函数可以将数据变为 0-1 范围的数值。

tanh: tanh 函数，通过这个函数可以将数据变为 [-1,1] 范围的数值

先不看内部具体的复杂关系，将上图简化为下图：

![](<assets/1713252044538.png>)

 结合

![](<assets/1713252046397.png>)

 和 

![](<assets/1713252044790.png>)

，GRU 会得到当前隐藏节点的输出

![](<assets/1713252045359.png>)

和传递给下一个节点的隐藏状态

![](<assets/1713252045610.png>)

, 这个

![](<assets/1713252045753.png>)

 

的推导是 GRU 的关键所在，我们看一下 GRU 所用到的公式：

![](<assets/1713252045961.png>)

这四个公式互有关联，并不是单独去使用，下面我们详细展开。

1. 重置门

 **重置门决定了如何将新的输入信息与前面的记忆相结合，这句话猛的一看也不好理解，我们再继续拆解。**

![](<assets/1713252046005.png>)

 将这个图片转化为公式就是重置门的公式：

![](<assets/1713252046048.png>)

这里

![](<assets/1713252046108.png>)

并不是一个值，而是一个权重矩阵。

用这个权重矩阵对

![](<assets/1713252047921.png>)

和

![](<assets/1713252046550.png>)

拼接而成的矩阵进行线性变换 (两个矩阵相乘）。然后将两个矩阵相乘得到的值投入 sigmoide 函数，会得到

![](<assets/1713252046689.png>)

的值，比如：0.6 。这个值会用到候选隐藏状态的公式中，即下面这个公式：

![](<assets/1713252047086.png>)

 为了方便理解，我们将这个公式展开：

![](<assets/1713252047128.png>)

下面便是重点:

![](<assets/1713252047299.png>)

的值越小，它与

![](<assets/1713252047438.png>)

哈达玛积出来的矩阵数值越小，再与权重矩阵相乘得到的值越小，

![](<assets/1713252047712.png>)

也就是这个值越小，

说明上一时刻需要遗忘的越多，丢弃的越多。

![](<assets/1713252047743.png>)

的值越大，

![](<assets/1713252047887.png>)

 值越大，说明上一时刻需要记住的越多，**新的输入信息（也就是当前的输入信息**

![](https://latex.csdn.net/eq?x_%7Bt%7D)

**）与前面的记忆相结合的越多。**

当

![](<assets/1713252048070.png>)

的值接近 0 时，

![](<assets/1713252048383.png>)

值也接近为 0，说明上一时刻的内容需要全部丢弃，只保留当前时刻的输入，所以可以用来丢弃与预测无关的历史信息。

当

![](<assets/1713252048426.png>)

的值接近 1 时，

![](<assets/1713252048760.png>)

值也接近为 1，表示保留上一时刻的隐藏状态。 

这就是重置门的作用，有助于捕捉时间序列里短期的依赖关系。

2. 更新门

 更新门用于控制前一时刻的状态信息被带入到当前状态中的程度，也就是更新门帮助模型决定到底要将多少过去的信息传递到未来，简单来说就是用于更新记忆。结合下面两个公式比较好理解：

![](<assets/1713252048791.png>)

更新门公式：

![](<assets/1713252048832.png>)

更新记忆表达式：

![](<assets/1713252048878.png>)

![](<assets/1713252048935.png>)

越接近 1，代表”记忆 “下来的数据越多；而越接近 0 则代表” 遗忘“的越多。

![](<assets/1713252049086.png>)

：表示对上一时刻隐藏状态进行选择性 “遗忘”。忘记

![](<assets/1713252049124.png>)

中一些不重要的信息，把不相关的丢弃。

![](<assets/1713252050500.png>)

：表示对候选隐藏状态的进一步选择性” 记忆 “。会忘记 

![](<assets/1713252050534.png>)

中的一些不重要的信息。也就是对

![](<assets/1713252052182.png>)

中的某些信息进一步选择。

综上，

![](<assets/1713252052338.png>)

![](<assets/1713252052385.png>)

忘记传递下来的 

![](<assets/1713252052575.png>)

中的某些信息，并加入当前节点输入的某些信息。这就是最终的记忆。

门控循环单元 GRU 不会随时间而清除以前的信息，它会保留相关的信息并传递到下一个单元。

参考资料：

[人人都能看懂的 GRU - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/32481747 "人人都能看懂的GRU - 知乎 (zhihu.com)")

 [GRU 学习总结_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1oh411e72G?spm_id_from=333.999.0.0 "GRU学习总结_哔哩哔哩_bilibili")