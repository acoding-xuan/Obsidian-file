---
url: https://zhuanlan.zhihu.com/p/32965521
title: fastText 原理及实践
date: 2023-11-02 17:41:52
tag: 
summary: 
---
【文章作者】 王江

【作者简介】 达观数据自然语言处理工程师，负责达观 NLP 底层开发、私有化应用系统开发等工作。主要参与大型系统的开发，对机器学习、NLP 等领域有浓厚兴趣。

fastText 是 Facebook 于 2016 年开源的一个词向量计算和文本分类工具，在学术上并没有太大创新。但是它的优点也非常明显，在文本分类任务中，fastText（浅层网络）往往能取得和深度网络相媲美的精度，却在训练时间上比深度网络快许多数量级。在标准的多核 CPU 上， 能够训练 10 亿词级别语料库的词向量在 10 分钟之内，能够分类有着 30 万多类别的 50 多万句子在 1 分钟之内。

本文首先会介绍一些预备知识，比如 softmax、ngram 等，然后简单介绍 word2vec 原理，之后来讲解 fastText 的原理，并着手使用 keras 搭建一个简单的 fastText 分类器，最后，我们会介绍 fastText 在达观数据的应用。

## 1. 预备知识

## （1）Softmax 回归

Softmax 回归（Softmax Regression）又被称作多项逻辑回归（multinomial logistic regression），它是逻辑回归在处理多类别任务上的推广。

在逻辑回归中， 我们有 m 个被标注的样本： [Math Processing Error]$\left\{ (x^{(1)},y^{(1)}),..., (x^{(m)},y^{(m)}) \right\}$\left\{(x^{(1)},y^{(1)}),..., (x^{(m)},y^{(m)}) \right\} ，其中 [Math Processing Error]$x^{(i)}∈R^{n}$x^{(i)}∈R^{n} 。因为类标是二元的，所以我们有 [Math Processing Error]$y^{(i)}∈\left\{ 0,1\right\}$y^{(i)}∈\left\{ 0,1\right\} 。我们的**假设**（hypothesis）有如下形式：$h_θ (x)=\frac{1}{1+e^{-θ^T x }}$h_θ (x)=\frac{1}{1+e^{-θ^T x }}

代价函数（cost function）如下：

![](<assets/1698918112835.png>)

在 Softmax 回归中，类标是大于 2 的，因此在我们的训练集$\left\{ \left( x^{(1)}, y^{(1)}\right),...,\left( x^{(m)}, y^{(m)}\right) \right\}$\left\{\left( x^{(1)}, y^{(1)}\right),...,\left( x^{(m)}, y^{(m)}\right) \right\} 中， $y^{(i)}\in\left\{ 1,2,...,K \right\}$y^{(i)}\in\left\{ 1,2,...,K \right\} 。给定一个测试输入_x_，我们的假设应该输出一个 K 维的向量，向量内每个元素的值表示_x_属于当前类别的概率。具体地，**假设** $h_{\theta}(x)$h_{\theta}(x) 形式如下：

![](<assets/1698918114852.png>)

代价函数如下：

![](<assets/1698918115121.png>)

其中 1{·} 是指示函数，即 1{true}=1,1{false}=0

既然我们说 Softmax 回归是逻辑回归的推广，那我们是否能够在代价函数上推导出它们的一致性呢？当然可以，于是：

![](<assets/1698918117100.png>)

可以看到，逻辑回归是 softmax 回归在 K=2 时的特例。

## （2）分层 Softmax

你可能也发现了，标准的 Softmax 回归中，要计算 y=j 时的 Softmax 概率： $P(y=j)$P(y=j) ，我们需要对所有的 K 个概率做归一化，这在 | y | 很大时非常耗时。于是，分层 Softmax 诞生了，它的基本思想是使用树的层级结构替代扁平化的标准 Softmax，使得在计算 $P(y=j)$P(y=j) 时，只需计算一条路径上的所有节点的概率值，无需在意其它的节点。

下图是一个分层 Softmax 示例：

![](<assets/1698918117337.png>)

树的结构是根据类标的频数构造的霍夫曼树。K 个不同的类标组成所有的叶子节点，K-1 个内部节点作为内部参数，从根节点到某个叶子节点经过的节点和边形成一条路径，路径长度被表示为 $L(y_{j})$L(y_{j}) 。于是， $P(y_{j})$P(y_{j}) 就可以被写成：

![](<assets/1698918119316.png>)

其中：

$\sigma(·)$\sigma(·) 表示 sigmoid 函数；

$LC(n)$LC(n) 表示 n 节点的左孩子；

⟦⟧$⟦x⟧$⟦x⟧ 是一个特殊的函数，被定义为：

![](<assets/1698918119663.png>)

$\theta_{n(y_{j},l)}$\theta_{n(y_{j},l)} 是中间节点 $n(y_{j},l)$n(y_{j},l) 的参数；

X 是 Softmax 层的输入

上图中，高亮的节点和边是从根节点到 $y_{2}$y_{2} 的路径，路径长度 $L(y_{2})=4,P(y_{2})$L(y_{2})=4,P(y_{2}) 可以被表示为：

![](<assets/1698918119831.png>)

于是，从根节点走到叶子节点 $y_{2}$y_{2} ，实际上是在做了 3 次二分类的逻辑回归。

通过分层的 Softmax，计算复杂度一下从 | K | 降低到 log|K|。

## （3）n-gram 特征

在文本特征提取中，常常能看到 n-gram 的身影。它是一种基于语言模型的算法，基本思想是将文本内容按照字节顺序进行大小为 N 的滑动窗口操作，最终形成长度为 N 的字节片段序列。看下面的例子：

我来到达观数据参观

相应的 bigram 特征为：我来 来到 到达 达观 观数 数据 据参 参观

相应的 trigram 特征为：我来到 来到达 到达观 达观数 观数据 数据参 据参观

注意一点：n-gram 中的 gram 根据粒度不同，有不同的含义。它可以是字粒度，也可以是词粒度的。上面所举的例子属于**字粒度**的 n-gram，**词粒度**的 n-gram 看下面例子：

我 来到 达观数据 参观

相应的 bigram 特征为：我 / 来到 来到 / 达观数据 达观数据 / 参观

相应的 trigram 特征为：我 / 来到 / 达观数据 来到 / 达观数据 / 参观

n-gram 产生的特征只是作为文本特征的候选集，你后面可能会采用信息熵、卡方统计、IDF 等文本特征选择方式筛选出比较重要特征。

## 2. word2vec

你可能要问，这篇文章不是介绍 fastText 的么，怎么开始介绍起了 word2vec？最主要的原因是 word2vec 的 CBOW 模型架构和 fastText 模型非常相似。于是，你看到 facebook 开源的 fastText 工具不仅实现了 fastText 文本分类工具，还实现了快速词向量训练工具。word2vec 主要有两种模型：skip-gram 模型和 CBOW 模型，这里只介绍 CBOW 模型，有关 skip-gram 模型的内容请参考达观另一篇技术文章：

漫谈 Word2vec 之 skip-gram 模型 [http://mp.weixin.qq.com/s/reT4lAjwo4fHV4ctR9zbxQ](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s/reT4lAjwo4fHV4ctR9zbxQ)

## （1）模型架构

CBOW 模型的基本思路是：用上下文预测目标词汇。架构图如下所示：

![](<assets/1698918121812.png>)

输入层由目标词汇 **y** 的上下文单词 $\left\{ x_{1},...,x_{c} \right\}$\left\{x_{1},...,x_{c} \right\} 组成， $x_{i}$x_{i} 是被 onehot 编码过的 V 维向量，其中 V 是词汇量；隐含层是 N 维向量 **h**；输出层是被 onehot 编码过的目标词 y。输入向量通过 $V*N$V*N 维的权重矩阵 W 连接到隐含层；隐含层通过 $N*V$N*V 维的权重矩阵 $W'$W' 连接到输出层。因为词库 V 往往非常大，使用标准的 softmax 计算相当耗时，于是 CBOW 的输出层采用的正是上文提到过的分层 Softmax。

## （2）前向传播

输入是如何计算而获得输出呢？先假设我们已经获得了权重矩阵 $W$W 和 $W^{'}$W^{'} （具体的推导见第 3 节），隐含层 **h** 的输出的计算公式：

![](<assets/1698918122083.png>)

即：隐含层的输出是 C 个上下文单词向量的加权平均，权重为_W_。

接着我们计算输出层的每个节点： $u_{j}={v'}_{w_{j}}^{T}·h$u_{j}={v'}_{w_{j}}^{T}·h

这里 ${v'}_{w_{j}}${v'}_{w_{j}} 是矩阵 $W’$W’ 的第 j 列，最后，将 $u_{j}$u_{j} 作为 softmax 函数的输入，得到 $y_{j}$y_{j} ：

![](<assets/1698918122352.png>)

## （3）反向传播学习权重矩阵

在学习权重矩阵和过程中，我们首先随机产生初始值，然后 feed 训练样本到我们的模型，并观测我们期望输出和真实输出的误差。接着，我们计算误差关于权重矩阵的梯度，并在梯度的方向纠正它们。

首先**定义损失函数**，objective 是最大化给定输入上下文，target 单词的条件概率。因此，损失函数为：

![](<assets/1698918122583.png>)

这里， $j^{*}$j^{*} 表示目标单词在词库 V 中的索引。

如何**更新权重** $W'$W' **?**

我们先对 E 关于 $W_{ij}^{'}$W_{ij}^{'} 求导：

![](<assets/1698918122859.png>)

⟦⟧$⟦j=j^* ⟧$⟦j=j^* ⟧ 函数表示：

![](<assets/1698918123101.png>)

于是， $W_{ij}^{'}$W_{ij}^{'} 的更新公式：

![](<assets/1698918123353.png>)

如何**更新权重**_W_？

我们首先计算 E 关于隐含层节点的导数：

![](<assets/1698918123610.png>)

然后，E 关于权重的导数为：

![](<assets/1698918123785.png>)

于是， $w_{ki}$w_{ki} 的更新公式：

![](<assets/1698918123962.png>)

## 3. fastText 分类

终于到我们的 fastText 出场了。这里有一点需要特别注意，一般情况下，使用 fastText 进行文本分类的同时也会产生词的 embedding，即 embedding 是 fastText 分类的产物。除非你决定使用预训练的 embedding 来训练 fastText 分类模型，这另当别论。

## （1）字符级别的 n-gram

word2vec 把语料库中的每个单词当成原子的，它会为每个单词生成一个向量。这忽略了单词内部的形态特征，比如：“apple” 和 “apples”，“达观数据” 和“达观”，这两个例子中，两个单词都有较多公共字符，即它们的内部形态类似，但是在传统的 word2vec 中，这种单词内部形态信息因为它们被转换成不同的 id 丢失了。

为了克服这个问题，fastText 使用了字符级别的 n-grams 来表示一个单词。对于单词 “apple”，假设 n 的取值为 3，则它的 trigram 有

“<ap”, “app”, “ppl”, “ple”, “le>”

其中，<表示前缀，>表示后缀。于是，我们可以用这些 trigram 来表示 “apple” 这个单词，进一步，我们可以用这 5 个 trigram 的向量叠加来表示 “apple” 的词向量。

这带来两点**好处**：

1. 对于低频词生成的词向量效果会更好。因为它们的 n-gram 可以和其它词共享。

2. 对于训练词库之外的单词，仍然可以构建它们的词向量。我们可以叠加它们的字符级 n-gram 向量。

## （2）模型架构

之前提到过，fastText 模型架构和 word2vec 的 CBOW 模型架构非常相似。下面是 fastText 模型架构图：

![](<assets/1698918124215.png>)

注意：此架构图没有展示词向量的训练过程。可以看到，和 CBOW 一样，fastText 模型也只有三层：输入层、隐含层、输出层（Hierarchical Softmax），输入都是多个经向量表示的单词，输出都是一个特定的 target，隐含层都是对多个词向量的叠加平均。不同的是，CBOW 的输入是目标单词的上下文，fastText 的输入是多个单词及其 n-gram 特征，这些特征用来表示单个文档；CBOW 的输入单词被 onehot 编码过，fastText 的输入特征是被 embedding 过；CBOW 的输出是目标词汇，fastText 的输出是文档对应的类标。

值得注意的是，fastText 在输入时，将单词的字符级别的 n-gram 向量作为额外的特征；在输出时，fastText 采用了分层 Softmax，大大降低了模型训练时间。这两个知识点在前文中已经讲过，这里不再赘述。

fastText 相关公式的推导和 CBOW 非常类似，这里也不展开了。

## （3）核心思想

现在抛开那些不是很讨人喜欢的公式推导，来想一想 fastText 文本分类的核心思想是什么？

仔细观察模型的后半部分，即从隐含层输出到输出层输出，会发现它就是一个 softmax 线性多类别分类器，分类器的输入是一个用来表征当前文档的向量；模型的前半部分，即从输入层输入到隐含层输出部分，主要在做一件事情：生成用来表征文档的向量。那么它是如何做的呢？叠加构成这篇文档的所有词及 n-gram 的词向量，然后取平均。叠加词向量背后的思想就是传统的词袋法，即将文档看成一个由词构成的集合。

于是 fastText 的核心思想就是：将整篇文档的词及 n-gram 向量叠加平均得到文档向量，然后使用文档向量做 softmax 多分类。这中间涉及到两个技巧：字符级 n-gram 特征的引入以及分层 Softmax 分类。

## （4）关于分类效果

还有个问题，就是为何 fastText 的分类效果常常不输于传统的非线性分类器？

假设我们有两段文本：

我 来到 达观数据

俺 去了 达而观信息科技

这两段文本意思几乎一模一样，如果要分类，肯定要分到同一个类中去。但在传统的分类器中，用来表征这两段文本的向量可能差距非常大。传统的文本分类中，你需要计算出每个词的权重，比如 tfidf 值， “我” 和 “俺” 算出的 tfidf 值相差可能会比较大，其它词类似，于是，VSM（向量空间模型）中用来表征这两段文本的文本向量差别可能比较大。但是 fastText 就不一样了，它是用单词的 embedding 叠加获得的文档向量，词向量的重要特点就是向量的距离可以用来衡量单词间的语义相似程度，于是，在 fastText 模型中，这两段文本的向量应该是非常相似的，于是，它们很大概率会被分到同一个类中。

**使用词 embedding 而非词本身作为特征**，这是 fastText 效果好的一个原因；另一个原因就是**字符级 n-gram 特征的引入对分类效果会有一些提升** 。

## 4. 手写一个 fastText

keras 是一个抽象层次很高的神经网络 API，由 python 编写，底层可以基于 Tensorflow、Theano 或者 CNTK。它的优点在于：用户友好、模块性好、易扩展等。所以下面我会用 keras 简单搭一个 fastText 的 demo 版，生产可用的 fastText 请移步 [https://github.com/facebookresearch/fastText](https://link.zhihu.com/?target=https%3A//github.com/facebookresearch/fastText)。如果你弄懂了上面所讲的它的原理，下面的 demo 对你来讲应该是非常明了的。

**为了简化我们的任务：**

1. 训练词向量时，我们使用正常的 word2vec 方法，而真实的 fastText 还附加了字符级别的 n-gram 作为特征输入；

2. 我们的输出层使用简单的 softmax 分类，而真实的 fastText 使用的是 Hierarchical Softmax。

**首先定义几个常量：**

VOCAB_SIZE = 2000

EMBEDDING_DIM =100

MAX_WORDS = 500

CLASS_NUM = 5

**VOCAB_SIZE** 表示词汇表大小，这里简单设置为 2000；

**EMBEDDING_DIM** 表示经过 embedding 层输出，每个词被分布式表示的向量的维度，这里设置为 100。比如对于 “达观” 这个词，会被一个长度为 100 的类似于 [ 0.97860014, 5.93589592, 0.22342691, -3.83102846, -0.23053935, …] 的实值向量来表示；

**MAX_WORDS** 表示一篇文档最多使用的词个数，因为文档可能长短不一（即词数不同），为了能 feed 到一个固定维度的神经网络，我们需要设置一个最大词数，对于词数少于这个阈值的文档，我们需要用 “未知词” 去填充。比如可以设置词汇表中索引为 0 的词为“未知词”，用 0 去填充少于阈值的部分；

**CLASS_NUM** 表示类别数，多分类问题，这里简单设置为 5。

**模型搭建遵循以下步骤**：

1. **添加输入层（embedding 层）**。Embedding 层的输入是一批文档，每个文档由一个词汇索引序列构成。例如：[10, 30, 80, 1000] 可能表示 “我 昨天 来到 达观数据” 这个短文本，其中 “我”、“昨天”、“来到”、“达观数据” 在词汇表中的索引分别是 10、30、80、1000；Embedding 层将每个单词映射成 EMBEDDING_DIM 维的向量。于是：input_shape=(BATCH_SIZE, MAX_WORDS), output_shape=(BATCH_SIZE,  
MAX_WORDS, EMBEDDING_DIM)；

2. **添加隐含层（投影层）**。投影层对一个文档中所有单词的向量进行叠加平均。keras 提供的 GlobalAveragePooling1D 类可以帮我们实现这个功能。这层的 input_shape 是 Embedding 层的 output_shape，这层的 output_shape=(BATCH_SIZE, EMBEDDING_DIM)；

3. **添加输出层（softmax 层）**。真实的 fastText 这层是 Hierarchical Softmax，因为 keras 原生并没有支持 Hierarchical Softmax，所以这里用 Softmax 代替。这层指定了 CLASS_NUM，对于一篇文档，输出层会产生 CLASS_NUM 个概率值，分别表示此文档属于当前类的可能性。这层的 output_shape=(BATCH_SIZE, CLASS_NUM)

4. **指定损失函数、优化器类型、评价指标，编译模型**。损失函数我们设置为 categorical_crossentropy，它就是我们上面所说的 softmax 回归的损失函数；优化器我们设置为 SGD，表示随机梯度下降优化器；评价指标选择 accuracy，表示精度。

**用训练数据 feed 模型时**，你需要：

1. **将文档分好词，构建词汇表**。词汇表中每个词用一个整数（索引）来代替，并预留 “未知词” 索引，假设为 0；

2. **对类标进行 onehot 化**。假设我们文本数据总共有 3 个类别，对应的类标分别是 1、2、3，那么这三个类标对应的 onehot 向量分别是 [1, 0,  
0]、[0, 1, 0]、[0, 0, 1]；

3. **对一批文本，将每个文本转化为词索引序列，每个类标转化为 onehot 向量**。就像之前的例子，“我 昨天 来到 达观数据” 可能被转化为 [10, 30,  
80, 1000]；它属于类别 1，它的类标就是 [1, 0, 0]。由于我们设置了 MAX_WORDS=500，这个短文本向量后面就需要补 496 个 0，即 [10, 30, 80, 1000, 0, 0, 0, …, 0]。因此，batch_xs 的 维度为 ( BATCH_SIZE,MAX_WORDS)，batch_ys 的维度为（BATCH_SIZE, CLASS_NUM）。

下面是构建模型的代码，数据处理、feed 数据到模型的代码比较繁琐，这里不展示。

![](<assets/1698918124474.png>)

## 5. fastText 在达观数据的应用

fastText 作为诞生不久的词向量训练、文本分类工具，在达观得到了比较深入的应用。主要被用在以下两个系统：

1. 同近义词挖掘。Facebook 开源的 fastText 工具也实现了词向量的训练，达观基于各种垂直领域的语料，使用其挖掘出一批同近义词；

2. 文本分类系统。在类标数、数据量都比较大时，达观会选择 fastText 来做文本分类，以实现快速训练预测、节省内存的目的。

## **编者注：**

如对文本挖掘领域的技术实践感兴趣，可前往下载达观研究院编写而成的[《达观数据技术实践特刊》](https://link.zhihu.com/?target=http%3A//www.datagrand.com/%3Fopen_magazine_dialog%3Dtrue)，该书集合了当下最热门的人工智能领域自然语言处理、个性化推荐、垂直搜索引擎三大方向的技术实践总结，融合了达观技术团队在服务华为、中兴、招行、平安、京东云等不同行业上百家企业后的技术感悟，是国内第一本系统介绍 NLP、深度学习等 AI 技术实践应用的电子刊，欢迎各位技术爱好者[前往下载](https://link.zhihu.com/?target=http%3A//www.datagrand.com/%3Fopen_magazine_dialog%3Dtrue)。

【本文版权归达观数据（[http://www.datagrand.com](https://link.zhihu.com/?target=http%3A//www.datagrand.com/)）所有，如需转载请注明出处。】