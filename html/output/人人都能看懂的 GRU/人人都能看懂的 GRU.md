## 1. 什么是 GRU

GRU（Gate Recurrent Unit）是循环神经网络（Recurrent Neural Network, RNN）的一种。和 LSTM（Long-Short Term Memory）一样，也是为了解决长期记忆和反向传播中的梯度等问题而提出来的。

GRU 和 LSTM 在很多情况下实际表现上相差无几，那么为什么我们要使用新人 GRU（2014 年提出）而不是相对经受了更多考验的 LSTM（1997 提出）呢。

下图 1-1 引用论文中的一段话来说明 GRU 的优势所在。
![](<assets/1713251948968.png>)
**简单来说就是贫穷限制了我们的计算能力...**

相比 LSTM，使用 GRU 能够达到相当的效果，并且相比之下更容易进行训练，能够很大程度上提高训练效率，因此很多时候会更倾向于使用 GRU。

OK，那么为什么说 GRU 更容易进行训练呢，下面开始介绍一下 GRU 的内部结构。

## 2. GRU 浅析

## 2.1 GRU 的输入输出结构

GRU 的输入输出结构与普通的 RNN 是一样的。

![](<assets/1713251949079.png>)
## 2.2 GRU 的内部结构

首先，我们先通过上一个传输下来的状态 $h^{t-1}$ 和当前节点的输入 $x^t$ 来获取两个门控状态。如下图 2-2 所示，其中 $r$  控制重置的门控（reset gate）， $z$ 为控制更新的门控（update gate）。

Tips： $\sigma$ 为_[sigmoid](https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Sigmoid_function)_ 函数，通过这个函数可以将数据变换为 0-1 范围内的数值，从而来充当门控信号。

![](<assets/1713251949143.png>)

**与 LSTM 分明的层次结构不同，下面将对 GRU 进行一气呵成的介绍~~~ 请大家屏住呼吸，不要眨眼。**

得到门控信号之后，首先使用重置门控来得到 **“重置”** 之后的数据 ${h^{t-1}}' = h^{t-1} \odot r$ $${h^{t-1}}' = h^{t-1} \odot r$$ ，再将 ${h^{t-1}}'$ 与输入 $x^t$ 进行拼接，再通过一个 [tanh](https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/tanh) 激活函数来将数据放缩到 **-1~1** 的范围内。即得到如下图 2-3 所示的 $h'$ 。

![](<assets/1713251949209.png>)

这里的 $h'$ 主要是包含了当前输入的 $x^t$  数据。有针对性地对 $h'$ 添加到当前的隐藏状态，相当于” 记忆了当前时刻的状态 “。类似于 LSTM 的选择记忆阶段（参照我的上一篇文章）。

![](<assets/1713251949260.png>)

图 2-4 中的 $\odot$ 是 Hadamard Product，也就是操作矩阵中对应的元素相乘，因此要求两个相乘矩阵是同型的。 $\oplus$则代表进行矩阵加法操作。

最后介绍 GRU 最关键的一个步骤，我们可以称之为” 更新记忆 “**阶段。
在这个阶段，我们同时进行了遗忘了记忆两个步骤。我们使用了先前得到的更新门控 $z$ （update gate）。
更新表达式： $$h^t = (1-z) \odot h^{t-1} + z\odot h'$$
首先再次强调一下，门控信号（这里的 $z$）的范围为 0~1。门控信号越接近 1，代表”记忆 “下来的数据越多；而越接近 0 则代表” 遗忘“的越多。

GRU 很聪明的一点就在于，**我们使用了同一个门控 $z$z 就同时可以进行遗忘和选择记忆（LSTM 则要使用多个门控）**。

*   $(1-z) \odot h^{t-1}$(1-z) \odot h^{t-1} ：表示对原本隐藏状态的选择性 “遗忘”。这里的 $1-z$1-z 可以想象成遗忘门（forget gate），忘记 $h^{t-1}$h^{t-1} 维度中一些不重要的信息。
*   $z \odot h'$z \odot h' ： 表示对包含当前节点信息的 $h'$h' 进行选择性” 记忆 “。与上面类似，这里的 $(1-z)$(1-z) 同理会忘记 $h '$h ' 维度中的一些不重要的信息。或者，这里我们更应当看做是对 $h'$ h' 维度中的某些信息进行选择。
*   $h^t =(1- z) \odot h^{t-1} + z\odot h'$h^t =(1- z) \odot h^{t-1} + z\odot h' ：结合上述，这一步的操作就是忘记传递下来的 $h^{t-1}$ h^{t-1} 中的某些维度信息，并加入当前节点输入的某些维度信息。

可以看到，这里的遗忘 $z$z 和选择 $(1-z)$(1-z) 是联动的。也就是说，对于传递进来的维度信息，我们会进行选择性遗忘，则遗忘了多少权重 （$z$z ），我们就会使用包含当前输入的 $h'$h' 中所对应的权重进行弥补 $(1-z)$(1-z) 。以保持一种” 恒定 “状态。

## 3. LSTM 与 GRU 的关系

GRU 是在 2014 年提出来的，而 LSTM 是 1997 年。他们的提出都是为了解决相似的问题，那么 GRU 难免会参考 LSTM 的内部结构。那么他们之间的关系大概是怎么样的呢？这里简单介绍一下。

大家看到 $r$r (reset gate) 实际上与他的名字有点不符。我们仅仅使用它来获得了 $h’$h’ 。

那么这里的 $h'$h' 实际上可以看成对应于 LSTM 中的 hidden state；上一个节点传下来的 $h^{t-1}$h^{t-1} 则对应于 LSTM 中的 cell state。1-z 对应的则是 LSTM 中的 $z^f$z^f forget gate，那么 z 我们似乎就可以看成是选择门 $z^i$z^i 了。大家可以结合我的两篇文章来进行观察，这是非常有趣的。

## 4. 总结

GRU 输入输出的结构与普通的 RNN 相似，其中的内部思想与 LSTM 相似。

与 LSTM 相比，GRU 内部少了一个” 门控 “，参数比 LSTM 少，但是却也能够达到与 LSTM 相当的功能。考虑到硬件的**计算能力**和**时间成本**，因而很多时候我们也就会选择更加” 实用 “的 GRU 啦。

