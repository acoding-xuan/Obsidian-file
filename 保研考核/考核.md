认真阅读一下论文：Decoding Matters: Addressing Amplification Bias and Homogeneity Issue for LLM-based Recommendation & 相关文献，搞懂论文中解决的问题&方案。然后，复现论文中的实验。
# 论文中提到的问题
## Amplification Bias
![](../img/Pasted%20image%2020240730191702.png)
![](../img/Pasted%20image%2020240730191726.png)
放大偏差：由于item在语言空间中的分布不均匀，某些item可能在特定条件下包含生成概率接近1的标记（称为幽灵标记）。现有的解码方法倾向于提高这些item的得分。通常，这些方法使用 **长度归一化** 来抵消生成过程中的长度偏差  :   较长序列仅仅因为每个标记的概率乘法而比短序列具有更低的概率。然而，当出现幽灵标记时，它们的生成概率（接近1）不会显著降低最终得分，但长度归一化仍然适用，导致得分放大。

## Homogeneity Issue
是指在基于大型语言模型（LLM）的推荐系统中，模型在给用户生成推荐列表时，倾向于推荐结构和内容高度相似的物品。这种现象导致推荐结果缺乏多样性，限制了用户接触到不同类型或特征物品的机会。
同质化问题的几个可能的原因：
1. **相似性评分**：
   - 在使用传统的解码方法（如束搜索）时，模型往往会给文本上相似的序列分配相似的分数。这导致在生成推荐时，模型可能重复推荐同一类别或系列的物品。
2. **幽灵标记（Ghost Tokens）**：
   - 某些物品可能包含一些生成概率接近1的标记（幽灵标记），这些标记在序列生成中几乎可以确定出现。这导致这些物品的推荐分数被放大，进一步加剧了同质化问题。
3. **用户历史交互**：
   - 模型可能会基于用户的历史交互数据，重复推荐用户之前已经接触过的物品或类别，而不是探索更广泛的选择。

# 解决问题的方法
##  Amplification Bias
为了解决放大偏差，根据其来源，直观的方法是在计算方程(3)中归一化时排除幽灵标记。本质上，只对正常标记应用长度归一化。然而，我们的分析显示，去除这些标记后，item标记序列的长度分布非常均匀，从而使得对剩余标记的长度归一化变得不必要。因此，我们**选择直接消除长度归一化**，以中和幽灵标记的影响并消除解码中的放大偏差。
![](../img/Pasted%20image%2020240731151604.png)
ghost tokens的比例对总长度有显著影响，起决定性作用。原始令牌长度存在相当大的差异，这表明长度在生成过程中显著影响item的得分。去除ghost tokens后，方差减少，item的长度变得相对一致。因此，直接消除长度归一化因子是可行的.
代码实现方法：
直接调节 length_penalty
## Homogeneity Issue
解决问题的关键是，提高那些被基于LLM的推荐模型低估但有意义的token的分数，同时避免过度忽视基于LLM的推荐模型的高分数标记，以保持推荐性能。本文提出 **利用一个额外的无文本模型来辅助** 。尽管这个模型的推荐能力较差，但它仍然可以为基于LLM的推荐模型提供有意义的推荐建议，这些建议不易受到文本相似性问题的影响。
![](../img/Pasted%20image%2020240801213439.png)
![](../img/Pasted%20image%2020240801213758.png)
即在生成token每一步中，我们并不完全依赖于，在recLLM中编码的知识。相反，我们利用来自一个脱离语言上下文的无文本模型的日志记录，来指导recLLM的生成。通过在每个阶段注入无文本的模型推断，我们减轻了由于模型对基于语言的属性的过度依赖所导致的同质性和冗余性。

实现思路：
自定义类实现 transformers 中的 LogitsProcessor 接口：
```c++
class ValidItemLogitsProcessor(LogitsProcessor):
	def __init__(self, tokenizer, a):
		#self.valid_item_ids = valid_item_ids
		self.tokenizer = tokenizer
		self.a = a
	def __call__(self, input_ids, scores):
		batch_size, seq_length = input_ids.shape
		# 用一个无文本分类模型 将 input_ids 输入求出预测概率 归一化产生 scores1
		return self.a * scores + (1 - self.a) * scores1
```
# 论文复现
## 数据集处理
2024-07-30 19:55:56.045 | INFO     | __main__:gao:172 - interaction_list: 853747
2024-07-30 19:56:20.036 | INFO     | __main__:gao:196 - Train Books: 682997
2024-07-30 19:56:20.042 | INFO     | __main__:gao:197 - Valid Books: 85375
2024-07-30 19:56:20.048 | INFO     | __main__:gao:198 - Test Books: 85375
2024-07-30 19:56:20.050 | INFO     | __main__:gao:199 - Done!
![](../img/Pasted%20image%2020240729221458.png)
![](../img/Pasted%20image%2020240803182001.png)
模型训练
LLM方法使用AdamW优化器，学习率范围为[1e-3, 1e-4, 5e-5]。训练过程中使用余弦学习调度器，提前停止的耐心值为一个epoch​​。 在所有与 temperature coefficient 相关的实验中，我们在[1.0, 1.5, 2.0] 的范围内进行了调整。
## 调参
Qwen-1.5 0.5B
lr: 1e-3
![](../img/Pasted%20image%2020240731183405.png)
lr: 1e-4
![](../img/Pasted%20image%2020240801174629.png)
lr: 5e-5
![](../img/Pasted%20image%2020240731183428.png)
## BIGRec
![](../img/Pasted%20image%2020240728213854.png)
![](../img/Pasted%20image%2020240728213846.png)
![](../img/Pasted%20image%2020240803180807.png)

## +D3
 “+D3” denotes applying our decoding method to TIGER/BIGRec.
![](../img/Pasted%20image%2020240803180858.png)
思路： 将在生成时去除归一化参数即可
```python
# add方法用来将一个beam（对应的容器）添加到整个列表中：
def add(self, hyp: torch.LongTensor, sum_logprobs: float, beam_indices: Optional[torch.LongTensor] = None):
	"""
	Add a new hypothesis to the list.
	"""
	score = sum_logprobs / (hyp.shape[-1] ** self.length_penalty)
	if len(self) < self.num_beams or score > self.worst_score:
		self.beams.append((score, hyp, beam_indices))
		if len(self) > self.num_beams:
			sorted_next_scores = sorted([(s, idx) for idx, (s, _, _) in enumerate(self.beams)])
			del self.beams[sorted_next_scores[0][1]]
			self.worst_score = sorted_next_scores[1][0]
		else:
			self.worst_score = min(score, self.worst_score)
```

```python
generation_config = GenerationConfig(
	temperature=temperature,
	top_p=top_p,
	top_k=top_k,
	num_beams=num_beams,
	num_return_sequences=num_beams,
	eos_token_id=model.config.eos_token_id,
	pad_token_id=model.config.pad_token_id,
	length_penalty=0  # 去除长度归一化
	**kwargs,
)
```
# 论文中的缺陷

## 改进幽灵令牌的识别与处理机制：

这种方法是否会引入新的偏差，因为有些情况下这些所谓的幽灵token可能就是用户想要的结果，如果直接去除归一化可能会导致，这些需要含有幽灵token结果可能会被剪枝。我觉得，可以尝试适当调节控制长度归一化参数，不一定要直接去除归一化。
## 缺乏对无文本分类模型使用的讨论
本文没有提及无文本分类模型的选择，以及讨论不同的无文本分类模型，对推荐效果以及推理时间的影响
思路：能不能在finetuning 时，引入一些非语义信息。


