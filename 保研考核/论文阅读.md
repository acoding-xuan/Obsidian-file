# Decoding Matters: Addressing Amplification Bias and Homogeneity Issue for LLM-based Recommendation

https://chatgpt.com/c/e19d92b4-fe84-4064-84cb-b330fa7c78ae
## 摘要
这篇论文主要探讨了在基于大型语言模型（LLM）的推荐系统中，解码过程中需要注意的问题，尤其是放大偏差（amplification bias）和同质化问题（homogeneity issue）。

（1）放大偏差：标准长度归一化会提高包含生成概率接近1的标记的项目得分（称为幽灵标记）；
（2）同质化问题——为用户生成多个相似或重复的项目。

为了解决这些挑战，我们引入了一种新的解码方法，名为 **去偏多样化解码（D3）**。D3禁用幽灵标记的长度归一化以减轻放大偏差，它结合了一个无文本助手模型，以鼓励LLM不经常生成的标记，以对抗推荐同质化。在真实世界数据集上的广泛实验证明了该方法在提高准确性和多样性方面的有效性。

## intro
### 放大偏差
![](../img/Pasted%20image%2020240730191702.png)
![](../img/Pasted%20image%2020240730191726.png)
放大偏差：由于 item 在语言空间中的分布不均，某些 item 在特定条件下可能会生成概率接近 1 的标签（称为幽灵标记）。现有的解码方法往往倾向于提高这些 item 的  得分。通常，这些方法使用 **长度归一化** 来抵消生成过程中的长度偏差(即较长的序列因为每个标记的概率乘法而比短序列具有更低的概率)。然而，当出现幽灵标记时，其生成概率接近 1，不会显著降低最终得分，而长度归一化仍然适用，从而导致得分被放大。

### 同质化问题
同质化问题：使用原始码方法解时，LLM在为用户提供多个推荐时，经常产生结构和内容相似的item，因为文本相似的序列通常获得相似的得分。

例如，在推荐产品时，模型可能会推荐来自同一系列或类别的几个item（例如“PlayStation 3”和“PlayStation 4”）。此外，由于LLM继承了匹配和复制机制，模型经常根据过去的用户交互重复item特征。

为了在解决长度偏差的同时减轻放大偏差，D3考虑选择性地对标记应用长度归一化，同时排除幽灵标记。在实施过程中，我们发现排除幽灵标记的会导致 item标记序列的长度分布相对均匀，使得对剩余标记的长度归一化变得不必要。 **因此，D3在解码过程中去除了所有长度归一化，使其实现更加方便。**

为了解决同质化问题，D3在每个解码步骤中结合了 text-free assistant model 的分数以指导token生成。这有助于避免生成相似和重复的items，因为 text-free assistant model 不受重复文本的影响，并且可以根据其推荐能力提供有意义的非重复/非相似标记建议。


### 总贡献
我们强调了LLM在推荐场景中生成的解码策略的重要性，并确定了两个关键挑战：对包含幽灵标记的item存在放大偏差和生成相似和重复项目时的同质化问题。
我们指出了幽灵标记和重复现象，并提出通过去除长度惩罚和利用无文本助手模型来减轻这个问题。
广泛的实验表明，我们的方法可以同时提高推荐的准确性和多样性。

## prelinary

### 大语言模型中的解码操作
为了解决这个问题，一种常见的方法是调整温度参数。这种方法修改了每一步生成中标记的概率分布的锐度，减少了由于特定标记被选择的概率过高而导致的过于相似的结果的可能性。除此之外，Diverse Beam Search（DBS）引入了束组的概念，通过划分生成过程来管理不同组之间的相似性，从而增加了生成结果的多样性。目前，由于解码空间的差异，使用LLM进行推荐提出了独特的挑战。在这一领域缺乏对解码阶段的全面讨论。在本文中，我们进行了首次探索性分析和优化解码过程和推荐。

**2 相关工作**  
在这一部分，我们将简要介绍本文相关领域的发展和现状，主要包括LLM在推荐系统中的应用和语言模型的解码策略。
**2.1 推荐系统中的大型语言模型**  
LLM展示的卓越能力激发了推荐系统社区的兴趣，引发了将LLM整合到这些系统中的研究热潮。这项研究大致可以分为两个流派。第一流派涉及直接使用LLM作为编码器来表示item，然后将这些item输入到传统的推荐模型中。
然而，普遍的LLM主要是仅解码器架构，优化用于下一个标记预测，这不利于信息编码。这一限制可能限制了LLM在推荐任务中的全部潜力。第二流派的研究集中在利用LLM的生成能力直接产生推荐。尽管这种方法，观察到LLM在预训练期间对推荐特定数据的接触有限，需要微调才能在该领域有效应用。尽管这类方法在增加，它们通常优先通过训练增强提高LLM的推荐性能，忽视了一个关键方面：对模型推荐输出的详细检查以及对解码阶段的考虑。

**2.2 语言模型中的解码**  
在开放式文本生成领域，多样性问题引起了广泛关注，主要是由于语言模型在生成过程中依赖于最大化概率（Welleck等人，2020；Holtzman等人，2020）。为了解决这个问题，一种常见的方法是调整温度参数。这种方法修改了每一步生成中标记的概率分布的锐度，减少了由于特定标记被选择的概率过高而导致的过于相似的结果的可能性。除此之外，Diverse Beam Search（DBS）引入了束组的概念，通过划分生成过程来管理不同组之间的相似性，从而增加了生成结果的多样性。目前，由于解码空间的差异，使用LLM进行推荐提出了独特的挑战。在这一领域缺乏对解码阶段的全面讨论。在本文中，我们进行了首次探索性分析和优化解码过程和推荐。



**3 预备知识**  
在这一部分，我们将介绍当前基于LLM的推荐方法（recLLM）的解码过程的背景知识。考虑到解码过程，模型采用指令输入和一系列用户历史交互来生成代表推荐项目的合适标记序列。我们的输入，长度为n，表示为x = x1 ... xn，其中每个xi是模型词汇表V中的一个标记。在解码期间，一个项目通过生成长度为m的标记序列来表示，表示为y = y1 ... ym。在解码时，标记是逐步生成的，一次一个，根据前面的上下文，并期望选择概率最高的输出：
![](../img/Pasted%20image%2020240729213058.png)
其中\( $p(y_i|x, y_{<i})$ \)表示下一个标记的概率分布。然而，考虑到遍历所有可能的情况来计算这个概率是不切实际的，贪心搜索可以应用于在每一步选择概率最高的标记xi。



**束搜索**  
由于贪心解码方法只能生成一个项目，并且经常找不到最优的项目序列（Holtzman等人，2020），目前使用LLM进行推荐的研究人员通常采用束搜索方法来同时生成多个项目（Zheng等人，2024；Tan等人，2024）。具体来说，束搜索中在步骤t的假设h的分数计算公式为：
$S(h_{\leq t}) = S(h_{\leq t-1}) + \log(p(h_t|x, h_{\leq t-1}))$

其中\( $S(h_{\leq t})$ \)是在步骤t用于选择假设的分数，\( $p(h_t|x, h_{\leq t-1})$ \)是在给定先前假设\( $h_{\leq t-1}$ \)和输入x的情况下，标记\( h_t \)的条件概率。束搜索在每一步跟踪分数最高的B个假设，其中B是束宽度。然后，它通过考虑最可能的k个标记来扩展这些假设。根据上述公式对新假设进行评分，并保留下一步的B个最高假设。这个过程一直持续到预测序列结束标记或达到最大序列长度。此外，在自然语言生成中，为了避免重复和过长的输出，在生成完成后通常添加长度归一化项。
![](../img/Pasted%20image%2020240729213502.png)
其中hL为h的长度，α是控制长度惩罚的超参数。一旦光束搜索完成，将选择顶部的B假设作为最终输出[y1，y2，...，yB]



## methods 
### 去除放大偏差
为了解决放大偏差，根据其来源，直观的方法是在计算方程(3)中归一化时排除幽灵标记。本质上，只对正常标记应用长度归一化。然而，我们的分析显示，去除这些标记的结果是项目标记序列的长度分布非常均匀，使得对剩余标记的长度归一化变得不必要。因此，我们**选择直接消除长度归一化**，以中和幽灵标记的影响并消除解码中的放大偏差。

### 解决同质化问题
鉴于同质化分析，我们得出结论，在束搜索期间，由于某些标记具有异常高分数的主导地位，许多潜在的推荐结果被过早地剪枝。因此，这不仅损害了推荐质量，也损害了推荐多样性。为了解决这个问题，必须改进模型的评分机制，以确保在生成阶段有更多的选择范围。因此，我们的目标是调整每一步的每个候选标记的分数，以帮助基于LLM的推荐模型更好地生成要推荐的项目。为了实现这一目标，关键是提高那些被基于LLM的推荐模型低估但有意义的token的分数，同时避免过度忽视基于LLM的推荐模型的高分数标记，以保持推荐性能。

仅依靠基于LLM的推荐模型的预测来完成这一任务是具有挑战性的。因此，我们提出 **利用一个额外的无文本模型来辅助** 。尽管这个模型的推荐能力较差，但它仍然可以为基于LLM的推荐模型提供有意义的推荐建议，这些建议不易受到文本相似性问题的影响。

具体来说，我们利用无文本模型在每个解码步骤为标记生成额外的分数，使用这些分数来完善recLLM的标记分数。

设I表示所有item的集合，Ii表示第i个item。我们将无文本模型推荐Ii的概率表示为pTF(Ii)。

在解码步骤中，给定已经产生的假设h<t，我们需要基于它生成一个新token ht。
为此，只有一部分item有资格生成，即与h<t匹配的项目，表示为Ih<t。基于这个集合，我们定义了无文本模型对潜在标记ht的标记分数STF(h≤t)如下：

![](../img/Pasted%20image%2020240728212910.png)
其中Ih<t,ht表示在Ih<t中ht紧随h<t的item，LTF(ht|h<t-1)表示无文本助手模型的标记概率，类似于方程(2)。然后我们用来指导recLLM逐步生成的整体分数函数是：
![](../img/Pasted%20image%2020240728213221.png)
其中α是一个超参数，用于控制无文本模型的辅助级别。 

这个公式表明，在生成的每一步中，我们并不完全依赖于，在recLLM中编码的知识。相反，我们利用来自一个脱离语言上下文的无文本模型的日志记录，来指导recLLM的生成。通过在每个阶段注入无文本的模型推断，我们减轻了由于模型对基于语言的属性的过度依赖所导致的同质性和冗余性。

## 实验
+Temp” denotes adjusting the temperature coefficient for BIGRec’s decoding. 
“+D3” denotes applying our decoding method to TIGER/BIGRec.

![](../img/Pasted%20image%2020240728213854.png)
![](../img/Pasted%20image%2020240728213846.png)
BIGRec是一种直接生成item的指令调整方法。我们对这种方法的解码方法进行了修改，确保生成的结果始终在数据集的项目列表中。

https://qwenlm.github.io/zh/blog/qwen1.5/
对于基于LLM的方法，为了效率，我们应用 Qwen1.5-1.8B 作为 backbone LLM。我们使用AdamW优化器，并在[1e-3, 1e-4, 5e-5] 的范围内调整学习率。在训练期间，我们应用了3个周期的余弦学习率调度器，并将早期停止耐心设置为一个周期。
在我们与温度系数相关的实验中，我们在[1.0, 1.5, 2.0] 的范围内进行了调整。
所有实验都在配备 32GB  VRAM 的 Ascend 910B上进行。

## 数据集处理
我们在来自Amazon评论数据的六个真实世界数据集上进行实验，包括乐器、CD、游戏、玩具、运动和书籍。所有数据集都包含从1996年5月到2018年10月的用户评论数据。为了预处理数据，我们遵循BIGRec论文中概述的策略，根据时间信息截断数据集，考虑到训练LLM的成本很高。此外，我们过滤掉不受欢迎的用户和少于五次交互的item，并将最大项目序列长度设置为10，以满足基线要求。

预处理步骤的详细信息和处理后数据集的统计信息可以在附录A中找到。
![](../img/Pasted%20image%2020240729221458.png)

如表3所示，我们概述了我们数据集的情况。

在预处理阶段，我们考虑了训练大型语言模型（LLMs）的资源影响以及随机抽样数据集的稀疏性问题。
因此，从2017年10月开始，我们处理了数据集，确保每个用户/物品在处理后至少有5次核心交互。
如果处理后的物品数量低于阈值（我们的情况设置为10,000），我们会将时间范围向后延长一个月并重复该程序，最终形成六个不同的数据集。（注意：即使使用完整的乐器数据，也只有9,239个物品可用）。


#  BigRec
## abs
我们的目标是研究LLM的综合排名能力，并提出一个名为BIGRec（推荐双步定位范式）的两步定位框架。
它首先通过微调LLM以生成有意义的item标记，将LLM定位到推荐空间，然后识别与生成的标记相对应的适当的实际item。
通过对两个数据集进行广泛的实验，我们证实了BIGRec的优越性能、处理少样本场景的能力和跨多个领域的多功能性。
## intro
![](../img/Pasted%20image%2020240728220955.png)
“语言空间”→“推荐空间”→“实际项目空间”
如图1所示。语言空间指的是LLM可以生成的所有可能序列的集合；
推荐空间是语言空间的一个子集，包括满足用户偏好的各种项目的描述，包括实际和假想的项目.

在第一步中，我们通过微调LLM以生成item描述的有意义标记，将LLM定位到推荐空间。

第二步 涉及识别与生成的标记最匹配的最合适的实际物品，利用它们从LLM获得的潜在表示。这一步还提供了灵活性，以便将所需的各种统计信息整合到推荐中，例如根据项目的流行度对表示的距离进行加权。

我们在 all-rank setting 中研究了LLM4Rec，并引入了一个双步定位范式，该范式以有效和高效的方式利用了LLM的理解和生成能力，并支持无缝整合统计信息。
我们验证了BIGRec的有效性，它显示出了在少样本和跨领域推荐方面的非凡能力；并揭示了扩大训练数据的影响。
我们将流行度和协同信息整合到BIGRec中，并揭示了这些统计信息在LLM4Rec中的好处，展示了未来的潜在方向。
### all-rank setting
在这种设置下，系统会对所有的候选项进行评分和排序，确保每个候选项都得到了充分的考虑。这种方法有助于提供更全面和准确的推荐结果。
## methods
### 将语言空间定位到推荐空间

根据[3]中提出的方法，我们使用LLaMA [40]对alpaca自指令数据 [39] 进行指令调整阶段。
之后，我们进行推荐特定的指令调整，以限制LLM的输出从语言空间到推荐空间。
![](../img/Pasted%20image%2020240729092536.png)
如表1所示，我们以生成方式微调LLM：给定用户与物品的过去交互，我们要求LLM生成一个新物品作为对用户的推荐。
通过这种方式的微调，我们将LLM的输出限制在指定的推荐空间中。然而，由于LLM的创造性，很难确保LLM的输出将对应于现实世界中存在的实际物品。因此，将LLM的输出定位到实际物品空间是必要的。
### 将推荐空间定位到实际物品空间
本次任务中没有这一步的需求。

在这一个小节中，我们详细说明了如何将推荐空间锚定到实际物品空间。
1. 首先，我们根据LLM的表示将LLM的输出与现实世界的物品对齐，以实现BIGRec的基本版本。然后，我们引入统计信息（例如，流行度和协同信息）以准确定位推荐的实际物品。
2. 具体来说，我们提取生成的标记的潜在表示和实际物品的嵌入。然后，我们通过计算它们嵌入之间的L2距离对这些实际物品进行排名。L2距离如下获得：
![](../img/Pasted%20image%2020240728224324.png)
其中 embi​ 表示第i个物品的嵌入，oracle表示LLM生成的输出的嵌入。
3. 引入统计信息。然后我们介绍了如何将**流行度信息和协同信息**整合到定位步骤中。
为了引入流行度，我们遵循PDA [57]中的思想，并通过流行度重新加权方程(1)中的L2距离。具体来说，我们首先根据以下公式计算每个物品的流行度因子：
![](../img/Pasted%20image%2020240728224550.png)
其中，N表示训练数据中用户-物品交互的集合，N𝑗表示N中物品𝑗观察到的交互数，I表示所有项，𝐶𝑖表示第𝑖项的流行度，𝑃𝑖为𝐶𝑖的归一化值。
然后我们通过流行度调整方程(1)中的L2距离：
![](../img/Pasted%20image%2020240729093603.png)
其中𝐷𝑖表示第𝑖项的嵌入与llm生成的输出的嵌入之间的L2距离，ˆ𝐷𝑖是归一化的𝐷𝑖，𝐷e𝑖使用流行度对ˆ𝐷𝑖重新加权。为了重新加权ˆ𝐷𝑖，我们利用逆流行度，并引入一个超参数𝛾来调节受欢迎度的影响。通过把流行度放在分母上，一个受欢迎的项目将被分配给一个更小的L2距离和一个更高的等级。这与PDA中的原始实现略有不同，后者直接重新加权预测分数，而不是L2距离。

与流行程度不同的是，使用统计方法来量化协同信息是有难度的。考虑到传统的推荐模型依赖于协同过滤的推荐，我们将这些协同过滤模型的预测得分视为协同信息。类似于流行的注入，我们可以在等式中的变量𝑃𝑖(3)通过预测评分，将协同信息注入到接地过程中。详情请参见第4.4节。
## exp
### 数据集处理
为了模拟现实世界的顺序推荐场景，我们根据交互的时间戳将每个数据集分成10个时期。然后，我们使用8:1:1的比例将每个数据集的时期划分为训练集、验证集和测试集5。这种方法确保了在测试期间预测的交互发生在训练期间观察到的所有交互之后，防止了训练模型期间测试阶段信息的泄露[21]。这更接近现实世界的情况[21, 58]。

### 实现细节
为了预处理数据，我们首先将用户交互历史的填充长度小于11的填充到固定长度11。然后，我们使用长度为11的滑动窗口提取序列，其中每个序列中的最后一个物品用作预测目标，以及来自过去交互序列的先前物品。对于所有传统模型，我们使用二元交叉熵损失进行优化，并均匀采样负样本。我们使用Adam [24]作为优化器，调整学习率为1e − 3，批量大小为1024，并在范围[1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7]内调整权重衰减。关于传统模型架构的超参数，我们将它们的嵌入大小设置为64，dropout比例设置为0.1。根据原始论文的建议，我们只为GRU4Rec使用一个GRU层；对于SASRec，我们将自注意力头和块的数量设置为1。我们基于SASRec实现了DROS，因为它在大多数情况下表现最佳。
![](../img/Pasted%20image%2020240729094645.png)
对于基于LLM的方法，我们遵循Alpaca设置 [39]，直接将学习率设置为1e− 4，并使用AdamW [31]优化器。
在生成过程中，由于在图2(b)中显示的大量使用GPU的计算和推理成本，我们遵循先前工作[5, 8, 41]的设置，并采用 **束大小为4** 来生成有意义的输出。对于BM25的超参数(BM25是一个常用的文本检索模型)，我们遵循GPT4Rec [27]论文中提供的调整范围。

对于模型选择，我们采用早停策略，基线方法的耐心为20个周期，LLM方法为5个周期(这意味着如果在连续5个训练周期内性能没有提升，就会停止训练。)。此外，我们报告了三个随机种子的平均结果。对于方程(3)中的 $gama$ 的超参数，由于传统模型和基于LLM的推荐模型在整合信息时采用了不同的计算方法，我们进行了详细的搜索，使得两种方法的信息能够被有效地整合。










# 连接服务器

```c++
本地生成PEM key
ssh-keygen -m PEM -t rsa -C "ustc-"
然后将 id_rsa.pub 传入服务器 
cd ～/.ssh 文件内


将公钥输出到authorized_keys
cat id_rsa.pub >> authorized_keys

设置ssh私钥文件
IdentityFile "C:\Users\bing\.ssh\keys\ustc"
```




# vscode debug

https://github.com/yuanzhoulvpi2017/vscode_debug_transformers
