# Part I see the following blogs
[斯坦福CS224N深度学习自然语言处理2019冬学习笔记目录 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/59011576)

# Part II self_record
## Lecture 9 Transformer
###  the lack of rnns
#### Linear interaction distance
![[Pasted image 20240323170803.png]]


*  It's hard to learn long-distance dependencies
*  Linear order of words maybe is not the right way to understand sentence
#### Lack of parallelizability
![[Pasted image 20240323171354.png]]
* Forward and backward passes have O(sequence length) unparallelizable operations

### The lacks and benefits of two kind of position  representation
#### Position representation vectors through sinusoids
![[Pasted image 20240323172548.png]]
#### Position representation vectors learned from scratch
![[Pasted image 20240323172747.png]]

### Barriers and solutions for Self-Attention as a building block(three main barriers)
![[Pasted image 20240323191824.png]]

### Implementation details of transformer
#### The computing process of the Transformer(user three extra matirces $K, Q, V$)
![[Pasted image 20240323202521.png]]
#### The implementation of  Multihead attention
![[Pasted image 20240323203141.png]]

#### Scaled Dot Product
![[Pasted image 20240323203647.png]]

#### Residual Connection
![[Pasted image 20240323204049.png]]
#### Layer Normalization(Done for every vector)
![[Pasted image 20240323204436.png]]
#### cross-attention(In decoder, the $K, Q, V$ from different parts)
![[Pasted image 20240323211253.png]]
* We use the output from encoder to generate $K, V$ and use ther vector from decoder to generate the query $Q$ 

### The Drawbacks Transformers
![[Pasted image 20240323212735.png]]

## Lecture 10 Pretraining model

#### Full Finetuning vs. Parameter-Efficient Finetuning
![[Pasted image 20240323222008.png]]

#### Parameter-Efficient Finetuning: Prefix-Tuning, Prompt tuning

1. **Prefix-Tuning**:
    - **Definition**: Prefix-tuning involves modifying the input to the model by adding a prefix specific to the task or domain before feeding it to the model.
    - **Usage**: This technique is often employed when the task requires a specific format or structure for the input. By providing a task-specific prefix, the model can better understand the context and generate more relevant outputs.
    - **Example**: In a language generation task where the model needs to complete a sentence with a specific sentiment, a prefix such as "Generate a sentence with positive sentiment:" can be added before the input text.
2. **Prompt Tuning**:    
    - **Definition**: Prompt tuning involves fine-tuning a pre-trained language model by providing it with task-specific prompts and examples.
    - **Usage**: Instead of directly fine-tuning the model on the target task data, prompt tuning leverages prompts, which are short, task-specific instructions or questions, to guide the model's generation process.
    - **Example**: In a text classification task where the goal is to classify news articles into categories such as politics, sports, or entertainment, prompts such as "Is this article about politics, sports, or entertainment?" can be provided along with corresponding examples.

### Pretraining for three types of architectures
![[Pasted image 20240324143607.png]]

#### encoder
##### bert
* two objectives
![[Pasted image 20240324143731.png]]![[Pasted image 20240324143904.png]]
#####   Limitations of pretrained encoders
![[Pasted image 20240324144310.png]]


#### encoder-decoders
##### T5
![[Pasted image 20240324150108.png]]


#### decoders
![[Pasted image 20240324150410.png]]

##### Generative Pretrained Transformer (GPT)
![[Pasted image 20240324151110.png]]

* The process of finetuning GPT is using some special tokens to specify this task.

##### Increasingly convincing generations (GPT2)
![[Pasted image 20240324152154.png]]
##### GPT3
![[Pasted image 20240324152610.png]]
## Lecture 11 Natural Language Generation
![[Pasted image 20240324154817.png]]
### autoregressive text generation models 
![[Pasted image 20240324162852.png]]
### Two ways to improve performance of nlg 
#### Decoding from NLG models 
![[Pasted image 20240324163246.png]]
##### problems
![[Pasted image 20240324163513.png]]
**Is finding the most likely string reasonable for open ended generation?** No

##### topk sampling and top-p sampling
![[Pasted image 20240324161151.png]]
![[Pasted image 20240324165904.png]]

##### Scaling randomness: Temperature
![[Pasted image 20240324170006.png]]
##### Improving Decoding: Re-ranking
![[Pasted image 20240324170543.png]]
#### Training NLG models

##### Exposure Bias and Solutions
* The discrepancy between training stage and test stage.
![[Pasted image 20240324171300.png]]
![[Pasted image 20240324171947.png]]
![[Pasted image 20240324172010.png]]
#### Reward Estimation Metrics
![[Pasted image 20240324172159.png]]

![[Pasted image 20240324172221.png]]
### Evaluating NLG Systems
![[Pasted image 20240324172744.png]]

#### Content overlap metrics
![[Pasted image 20240324172841.png]]

![[Pasted image 20240324173008.png]]
#### Model-based metrics
![[Pasted image 20240324185650.png]]
#### Human evaluations
![[Pasted image 20240324190413.png]]

## Lecture 12  Prompting, Instruction Finetuning, and RLHF
### Zero-Shot (ZS)  Few-Shot (FS) In-Context Learning
![[Pasted image 20240330205729.png]]

![[Pasted image 20240330210539.png]]
#### limits of prompting  and Chainof-thought prompting
![[Pasted image 20240330211307.png]]![[Pasted image 20240330213510.png]]![[Pasted image 20240330213740.png]]
#### benefits and limits of ZS FS
![[Pasted image 20240330214338.png]]

### Instruction finetuning
Collect examples of (instruction, output) pairs across many tasks and finetune an LM
#### benefits and limitations

![[Pasted image 20240330215702.png]]
![[Pasted image 20240330215727.png]]

### Reinforcement Learning from Human Feedback (RLHF)
#### The princple

![[Pasted image 20240330230032.png]]
![[Pasted image 20240330230732.png]]
 ### benefits and limitations
 ![[Pasted image 20240330231533.png]]
![[Pasted image 20240330231626.png]]
## lecture 13 Question Answering
The goal of question answering is to build systems that automatically answer questions posed by humans in a natural language

### Neural models for reading comprehension
![[Pasted image 20240331210130.png]]


#### BiDAF: the Bidirectional Attention Flow model
![[Pasted image 20240331211831.png]]
#### BERT for reading comprehension
![[Pasted image 20240331212023.png]]

#### Comparisons between BiDAF and BERT models
• BERT model has many many more parameters (110M or 330M) BiDAF has ~2.5M parameters.
• BiDAF is built on top of several bidirectional LSTMs while BERT is built on top of Transformers (no recurrence architecture and easier to parallelize).
• BERT is pre-trained while BiDAF is only built on top of GloVe (and all the remaining parameters need to be learned from the supervision datasets).
#### Retriever-reader framework

![[Pasted image 20240331221345.png]]

## lecture13 ConvNets for NLP and Tree Recursive Neural Networks


## Multimodal Deep Learning
![[Pasted image 20240401113543.png]]
### Early models

#### Cross-modal “Visual-Semantic Embeddings”
![[Pasted image 20240401114149.png]]
#### Multimodal distributional semantics
![[Pasted image 20240401114239.png]]
#### Beyond words: Sentence level alignment

![[Pasted image 20240401114422.png]]
#### Image to text: Captioning
![[Pasted image 20240401114521.png]]

#### Text to image: Conditional image synthesis
![[Pasted image 20240401114614.png]]
### Features and fusion

#### Region features
![[Pasted image 20240401120347.png]]

#### Vision Transformers
![[Pasted image 20240401120423.png]]

#### Multimodal fusion
![[Pasted image 20240401120450.png]]

### Contrastive models

#### CLIP (Radford et al. 2021)
![[Pasted image 20240401120638.png]]

#### Multimodal foundation models
There are still many of the Multimodals methoned in the ppt. Because I didn't understand them all so I did't list them below.
#### Visual BERTs: VisualBERT
![[Pasted image 20240401121548.png]]
#### ViLT (Kim et al. 2021)
![[Pasted image 20240401122122.png]]
### Evaluation

#### COCO - Common Objects in Context
![[Pasted image 20240401124709.png]]

#### VQA - Visual Question Answering (Antol et al., 2015)
![[Pasted image 20240401124937.png]]


