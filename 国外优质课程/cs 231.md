
# part3 图像分类
这是一篇介绍性教程，面向非计算机视觉领域的同学。教程将向同学们介绍图像分类问题和数据驱动方法。下面是**内容列表**：

*   图像分类、数据驱动方法和流程  
    
*   Nearest Neighbor 分类器

*   k-Nearest Neighbor 
    
*   验证集、交叉验证集和超参数调参  
    
*   Nearest Neighbor 的优劣  
    
*   小结  
    
*   小结：应用 kNN 实践  
    
*   拓展阅读
## 图像分类

**目标**：这一节我们将介绍图像分类问题。
`所谓图像分类问题，就是已有固定的分类标签集合，然后对于输入的图像，从分类标签集合中找出一个分类标签，最后把分类标签分配给该输入图像`。虽然看起来挺简单的，但这可是计算机视觉领域的核心问题之一，并且有着各种各样的实际应用。在后面的课程中，我们可以看到计算机视觉领域中很多看似不同的问题（比如物体检测和分割），都可以被归结为图像分类问题。

**例子**：以下图为例，图像分类模型读取该图片，并生成该图片属于集合 {cat, dog, hat, mug} 中各个标签的概率。需要注意的是，对于计算机来说，图像是一个由数字组成的巨大的 3 维数组。在这个例子中，猫的图像大小是宽 248 像素，高 400 像素，有 3 个颜色通道，分别是红、绿和蓝（简称 RGB）。如此，该图像就包含了 248X400X3=297600 个数字，每个数字都是在范围 0-255 之间的整型，其中 0 表示全黑，255 表示全白。我们的任务就是把这些上百万的数字变成一个简单的标签，比如 “猫”。

—————————————————————————————————————————

![](<assets/1698235002265.png>)

—————————————————————————————————————————

**困难和挑战**：对于人来说，识别出一个像 “猫” 一样视觉概念是简单至极的，然而从计算机视觉算法的角度来看就值得深思了。我们在下面列举了计算机视觉算法在图像识别方面遇到的一些困难，要记住图像是以 3 维数组来表示的，数组中的元素是亮度值。

*   **视角变化（Viewpoint variation**）**：同一个物体，摄像机可以从多个角度来展现。
*   **大小变化（Scale variation**）**：物体可视的大小通常是会变化的（不仅是在图片中，在真实世界中大小也是变化的）。
*   **形变（Deformation**）**：很多东西的形状并非一成不变，会有很大变化。
*   **遮挡（Occlusion**）**：目标物体可能被挡住。有时候只有物体的一小部分（可以小到几个像素）是可见的。
*   **光照条件（Illumination conditions**）**：在像素层面上，光照的影响非常大。
*   **背景干扰（Background clutter**）**：物体可能混入背景之中，使之难以被辨认。
*   **类内差异（Intra-class variation**）**：一类物体的个体之间的外形差异很大，比如椅子。这一类物体有许多不同的对象，每个都有自己的外形。

面对以上所有变化及其组合，好的图像分类模型能够在维持分类结论稳定的同时，保持对类间差异足够敏感。  

![](<assets/1698235002432.png>)

**数据驱动方法**：如何写一个图像分类的算法呢？这和写个排序算法可是大不一样。怎么写一个从图像中认出猫的算法？搞不清楚。因此，与其在代码中直接写明各类物体到底看起来是什么样的，倒不如说我们采取的方法和教小孩儿看图识物类似：给计算机很多数据，然后实现学习算法，让计算机学习到每个类的外形。这种方法，就是_数据驱动方法_。既然该方法的第一步就是收集已经做好分类标注的图片来作为训练集，那么下面就看看数据库到底长什么样：  

—————————————————————————————————————————

![](<assets/1698235002647.png>)
**图像分类流程**。在课程视频中已经学习过，**图像分类**就是输入一个元素为像素值的数组，然后给它分配一个分类标签。完整流程如下：
*   **输入**：输入是包含 N 个图像的集合，每个图像的标签是 K 种分类标签中的一种。这个集合称为_训练集。_
*   **学习**：这一步的任务是使用训练集来学习每个类到底长什么样。一般该步骤叫做_训练分类器_或者_学习一个模型_。
*   **评价**：让分类器来预测它未曾见过的图像的分类标签，并以此来评价分类器的质量。我们会把分类器预测的标签和图像真正的分类标签对比。毫无疑问，分类器预测的分类标签和图像真正的分类标签如果一致，那就是好事，这样的情况越多越好。

## Nearest Neighbor 分类器

作为课程介绍的第一个方法，我们来实现一个 **Nearest Neighbor 分类器**。虽然这个分类器和卷积神经网络没有任何关系，实际中也极少使用，但通过实现它，可以让读者对于解决图像分类问题的方法有个基本的认识。

**图像分类数据集：CIFAR-10。一个非常流行的图像分类数据集是 [CIFAR-10](https://link.zhihu.com/?target=http%3A//www.cs.toronto.edu/~kriz/cifar.html)。这个数据集包含了 60000 张 32X32 的小图像。每张图像都有 10 种分类标签中的一种。这 60000 张图像被分为包含 50000 张图像的训练集和包含 10000 张图像的测试集。在下图中你可以看见 10 个类的 10 张随机图片。

![](<assets/1698235002849.png>)

**左边** [CIFAR-10](https://link.zhihu.com/?target=http%3A//www.cs.toronto.edu/~kriz/cifar.html) **右边**

假设现在我们有 CIFAR-10 的 50000 张图片（每种分类 5000 张）作为训练集，我们希望将余下的 10000 作为测试集并给他们打上标签。`Nearest Neighbor 算法将会拿着测试图片和训练集中每一张图片去比较，然后将它认为最相似的那个训练集图片的标签赋给这张测试图片。`上面右边的图片就展示了这样的结果。请注意上面 10 个分类中，只有 3 个是准确的。比如第 8 行中，马头被分类为一个红色的跑车，原因在于红色跑车的黑色背景非常强烈，所以这匹马就被错误分类为跑车了。

那么具体如何比较两张图片呢？在本例中，就是比较 32x32x3 的像素块。最简单的方法就是逐个像素比较，最后将差异值全部加起来。换句话说，就是将两张图片先转化为两个向量 $I_1$ 和 $I_2$，然后计算他们的 **L1 距离：**

$\displaystyle d_1(I_1,I_2)=\sum_p|I^p_1-I^p_2|$

这里的求和是针对所有的像素。下面是整个比较流程的图例：  
![](<assets/1698235003009.png>)

—————————————————————————————————————————

下面，让我们看看如何用代码来实现这个分类器。首先，我们将 CIFAR-10 的数据加载到内存中，并分成 4 个数组：训练数据和标签，测试数据和标签。在下面的代码中，**Xtr**（大小是 50000x32x32x3）存有训练集中所有的图像，**Ytr** 是对应的长度为 50000 的 1 维数组，存有图像对应的分类标签（从 0 到 9）：

```python
Xtr, Ytr, Xte, Yte = load_CIFAR10('data/cifar10/') # a magic function we provide
# flatten out all images to be one-dimensional
Xtr_rows = Xtr.reshape(Xtr.shape[0], 32 * 32 * 3) # Xtr_rows becomes 50000 x 3072
Xte_rows = Xte.reshape(Xte.shape[0], 32 * 32 * 3) # Xte_rows becomes 10000 x 3072
```

现在我们得到所有的图像数据，并且把他们拉长成为行向量了。接下来展示如何训练并评价一个分类器：  

```python
nn = NearestNeighbor() # create a Nearest Neighbor classifier class
nn.train(Xtr_rows, Ytr) # train the classifier on the training images and labels
Yte_predict = nn.predict(Xte_rows) # predict labels on the test images
# and now print the classification accuracy, which is the average number
# of examples that are correctly predicted (i.e. label matches)
print 'accuracy: %f' % ( np.mean(Yte_predict == Yte) )
```

作为评价标准，我们常常使用**准确率**，它描述了我们预测正确的得分。请注意以后我们实现的所有分类器都需要有这个 API：**train(X, y)** 函数。该函数使用训练集的数据和标签来进行训练。从其内部来看，类应该实现一些关于标签和标签如何被预测的模型。这里还有个 **predict(X)** 函数，它的作用是预测输入的新数据的分类标签。现在还没介绍分类器的实现，下面就是使用 L1 距离的 Nearest Neighbor 分类器的实现套路：  

```python
import numpy as np

class NearestNeighbor(object):
  def __init__(self):
    pass

  def train(self, X, y):
    """ X is N x D where each row is an example. Y is 1-dimension of size N """
    # the nearest neighbor classifier simply remembers all the training data
    self.Xtr = X
    self.ytr = y

  def predict(self, X):
    """ X is N x D where each row is an example we wish to predict label for """
    num_test = X.shape[0]
    # lets make sure that the output type matches the input type
    Ypred = np.zeros(num_test, dtype = self.ytr.dtype)

    # loop over all test rows
    for i in xrange(num_test):
      # find the nearest training image to the i'th test image
      # using the L1 distance (sum of absolute value differences)
      distances = np.sum(np.abs(self.Xtr - X[i,:]), axis = 1)
      min_index = np.argmin(distances) # get the index with smallest distance
      Ypred[i] = self.ytr[min_index] # predict the label of the nearest example
    return Ypred
```

如果你用这段代码跑 CIFAR-10，你会发现准确率能达到 **38.6%**。这比随机猜测的 10% 要好，但是比人类识别的水平（[据研究推测是 94%](https://link.zhihu.com/?target=http%3A//karpathy.github.io/2011/04/27/manually-classifying-cifar10/)）和卷积神经网络能达到的 95% 还是差多了。点击查看基于 CIFAR-10 数据的 [Kaggle 算法竞赛排行榜](https://link.zhihu.com/?target=http%3A//www.kaggle.com/c/cifar-10/leaderboard)。

**距离选择**：计算向量间的距离有很多种方法，另一个常用的方法是 **L2 距离**，从几何学的角度，可以理解为它在计算两个向量间的欧式距离。L2 距离的公式如下：  

$$\displaystyle d_2(I_1,I_2)=\sqrt{ \sum_p(I^p_1-I^p_2)^2}$$

换句话说，我们依旧是在计算像素间的差值，只是先求其平方，然后把这些平方全部加起来，最后对这个和开方。在 Numpy 中，我们只需要替换上面代码中的 1 行代码就行：  

```python
distances = np.sqrt(np.sum(np.square(self.Xtr - X[i,:]), axis = 1))
```

注意在这里使用了 **np.sqrt**，但是在实际中可能不用。因为求平方根函数是一个_单调函数_，它对不同距离的绝对值求平方根虽然改变了数值大小，但依然保持了不同距离大小的顺序。所以用不用它，都能够对像素差异的大小进行正确比较。如果你在 CIFAR-10 上面跑这个模型，正确率是 **35.4%**，比刚才低了一点。

**L1 和 L2 比较**。比较这两个度量方式是挺有意思的。在面对两个向量之间的差异时，L2 比 L1 更加不能容忍这些差异。也就是说，相对于 1 个巨大的差异，L2 距离更倾向于接受多个中等程度的差异。L1 和 L2 都是在 [p-norm](https://link.zhihu.com/?target=http%3A//planetmath.org/vectorpnorm) 常用的特殊形式。

## k-Nearest Neighbor 分类器

你可能注意到了，为什么只用最相似的 1 张图片的标签来作为测试图像的标签呢？这不是很奇怪吗！是的，使用 **k-Nearest Neighbor 分类器**就能做得更好。它的思想很简单：`与其只找最相近的那 1 个图片的标签，我们找最相似的 k 个图片的标签，然后让他们针对测试图片进行投票，最后把票数最高的标签作为对测试图片的预测`。所以当 k=1 的时候，k-Nearest Neighbor 分类器就是 Nearest Neighbor 分类器。从直观感受上就可以看到，更高的 k 值可以让分类的效果更平滑，使得分类器对于异常值更有抵抗力。  

—————————————————————————————————————————

![](<assets/1698235003260.png>)

**决策边界****泛化（****generalization****）

在实际中，大多使用 k-NN 分类器。但是 k 值如何确定呢？接下来就讨论这个问题。  
## 用于超参数调优的验证集

k-NN 分类器需要设定 k 值，那么选择哪个 k 值最合适的呢？我们可以选择不同的距离函数，比如 L1 范数和 L2 范数等，那么选哪个好？还有不少选择我们甚至连考虑都没有考虑到（比如：点积）。所有这些选择，被称为**超参数（hyperparameter）**。在基于数据进行学习的机器学习算法设计中，超参数是很常见的。一般说来，这些超参数具体怎么设置或取值并不是显而易见的。

你可能会建议尝试不同的值，看哪个值表现最好就选哪个。好主意！我们就是这么做的，但这样做的时候要非常细心。特别注意：**决不能使用测试集来进行调优**。当你在设计机器学习算法的时候，应该把测试集看做非常珍贵的资源，不到最后一步，绝不使用它。如果你使用测试集来调优，而且算法看起来效果不错，那么真正的危险在于：算法实际部署后，性能可能会远低于预期。这种情况，称之为算法对测试集**过拟合**。从另一个角度来说，如果使用测试集来调优，实际上就是把测试集当做训练集，由测试集训练出来的算法再跑测试集，自然性能看起来会很好。这其实是过于乐观了，实际部署起来效果就会差很多。所以，最终测试的时候再使用测试集，可以很好地近似度量你所设计的分类器的泛化性能（在接下来的课程中会有很多关于泛化性能的讨论）。

测试数据集只使用一次，即在训练完成后评价最终的模型时使用。

好在我们有不用测试集调优的方法。其思路是：从训练集中取出一部分数据用来调优，我们称之为**验证集（****validation set****）**。以 CIFAR-10 为例，我们可以用 49000 个图像作为训练集，用 1000 个图像作为验证集。验证集其实就是作为假的测试集来调优。下面就是代码：

```python
# assume we have Xtr_rows, Ytr, Xte_rows, Yte as before
# recall Xtr_rows is 50,000 x 3072 matrix
Xval_rows = Xtr_rows[:1000, :] # take first 1000 for validation
Yval = Ytr[:1000]
Xtr_rows = Xtr_rows[1000:, :] # keep last 49,000 for train
Ytr = Ytr[1000:]

# find hyperparameters that work best on the validation set
validation_accuracies = []
for k in [1, 3, 5, 10, 20, 50, 100]:

  # use a particular value of k and evaluation on validation data
  nn = NearestNeighbor()
  nn.train(Xtr_rows, Ytr)
  # here we assume a modified NearestNeighbor class that can take a k as input
  Yval_predict = nn.predict(Xval_rows, k = k)
  acc = np.mean(Yval_predict == Yval)
  print 'accuracy: %f' % (acc,)

  # keep track of what works on the validation set
  validation_accuracies.append((k, acc))
```

程序结束后，我们会作图分析出哪个 k 值表现最好，然后用这个 k 值来跑真正的测试集，并作出对算法的评价。  

把训练集分成训练集和验证集。使用验证集来对所有超参数调优。最后只在测试集上跑一次并报告结果。

**交叉验证**。有时候，训练集数量较小（因此验证集的数量更小），人们会使用一种被称为**交叉验证**的方法，这种方法更加复杂些。还是用刚才的例子，如果是交叉验证集，我们就不是取 1000 个图像，而是将训练集平均分成 5 份，其中 4 份用来训练，1 份用来验证。然后我们循环着取其中 4 份来训练，其中 1 份来验证，最后取所有 5 次验证结果的平均值作为算法验证结果。

—————————————————————————————————————————

![](<assets/1698235009628.png>)

**实际应用**。在实际情况下，人们不是很喜欢用交叉验证，主要是因为它会耗费较多的计算资源。一般直接把训练集按照 50%-90% 的比例分成训练集和验证集。但这也是根据具体情况来定的：如果超参数数量多，你可能就想用更大的验证集，而验证集的数量不够，那么最好还是用叉交验证吧。至于分成几份比较好，一般都是分成 3、5 和 10 份。  

![](<assets/1698235009867.png>)

## Nearest Neighbor 分类器的优劣

现在对 Nearest Neighbor 分类器的优缺点进行思考。首先，Nearest Neighbor 分类器易于理解，实现简单。其次，算法的训练不需要花时间，因为其训练过程只是将训练集数据存储起来。然而测试要花费大量时间计算，因为每个测试图像需要和所有存储的训练图像进行比较，这显然是一个缺点。在实际应用中，我们关注测试效率远远高于训练效率。其实，我们后续要学习的卷积神经网络在这个权衡上走到了另一个极端：虽然训练花费很多时间，但是一旦训练完成，对新的测试数据进行分类非常快。这样的模式就符合实际使用需求。

Nearest Neighbor 分类器的计算复杂度研究是一个活跃的研究领域，若干 **Approximate Nearest Neighbor** (ANN) 算法和库的使用可以提升 Nearest Neighbor 分类器在数据上的计算速度（比如：[FLANN](https://link.zhihu.com/?target=http%3A//www.cs.ubc.ca/research/flann/)）。这些算法可以在准确率和时空复杂度之间进行权衡，并通常依赖一个预处理 / 索引过程，这个过程中一般包含 kd 树的创建和 k-means 算法的运用。

Nearest Neighbor 分类器在某些特定情况（比如数据维度较低）下，可能是不错的选择。但是在实际的图像分类工作中，很少使用。因为图像都是高维度数据（他们通常包含很多像素），而高维度向量之间的距离通常是反直觉的。下面的图片展示了基于像素的相似和基于感官的相似是有很大不同的：

—————————————————————————————————————————

![](<assets/1698235010026.png>)

—————————————————————————————————————————

这里还有个视觉化证据，可以证明使用像素差异来比较图像是不够的。z 这是一个叫做 [t-SNE](https://link.zhihu.com/?target=http%3A//lvdmaaten.github.io/tsne/) 的可视化技术，它将 CIFAR-10 中的图片按照二维方式排布，这样能很好展示图片之间的像素差异值。在这张图片中，排列相邻的图片 L2 距离就小。  

—————————————————————————————————————————

![](<assets/1698235010230.png>)

——————————————————————————————————————————

具体说来，这些图片的排布更像是一种颜色分布函数，或者说是基于背景的，而不是图片的语义主体。比如，狗的图片可能和青蛙的图片非常接近，这是因为两张图片都是白色背景。从理想效果上来说，我们肯定是希望同类的图片能够聚集在一起，而不被背景或其他不相关因素干扰。为了达到这个目的，我们不能止步于原始像素比较，得继续前进。

## 小结

简要说来：

*   介绍了**图像分类**问题。在该问题中，给出一个由被标注了分类标签的图像组成的集合，要求算法能预测没有标签的图像的分类标签，并根据算法预测准确率进行评价。
*   介绍了一个简单的图像分类器：**最近邻分类器 (Nearest Neighbor classifier)**。分类器中存在不同的超参数 (比如 k 值或距离类型的选取)，要想选取好的超参数不是一件轻而易举的事。
*   选取超参数的正确方法是：将原始训练集分为训练集和**验证集**，我们在验证集上尝试不同的超参数，最后保留表现最好那个。
*   如果训练数据量不够，使用**交叉验证**方法，它能帮助我们在选取最优超参数的时候减少噪音。
*   一旦找到最优的超参数，就让算法以该参数在测试集跑且只跑一次，并根据测试结果评价算法。
*   最近邻分类器能够在 CIFAR-10 上得到将近 40% 的准确率。该算法简单易实现，但需要存储所有训练数据，并且在测试的时候过于耗费计算能力。
*   最后，我们知道了仅仅使用 L1 和 L2 范数来进行像素比较是不够的，图像更多的是按照背景和颜色被分类，而不是语义主体分身。

在接下来的课程中，我们将专注于解决这些问题和挑战，并最终能够得到超过 90% 准确率的解决方案。该方案能够在完成学习就丢掉训练集，并在一毫秒之内就完成一张图片的分类。

## 小结：实际应用 k-NN

如果你希望将 k-NN 分类器用到实处（最好别用到图像上，若是仅仅作为练手还可以接受），那么可以按照以下流程：

1.  预处理你的数据：对你数据中的特征进行归一化（normalize），让其具有零平均值（zero mean）和单位方差（unit variance）。在后面的小节我们会讨论这些细节。本小节不讨论，是因为图像中的像素都是同质的，不会表现出较大的差异分布，也就不需要标准化处理了。
2.  如果数据是高维数据，考虑使用降维方法，比如 PCA([wiki ref](https://link.zhihu.com/?target=http%3A//en.wikipedia.org/wiki/Principal_component_analysis), [CS229ref](https://link.zhihu.com/?target=http%3A//cs229.stanford.edu/notes/cs229-notes10.pdf), [blog ref](https://link.zhihu.com/?target=http%3A//www.bigdataexaminer.com/understanding-dimensionality-reduction-principal-component-analysis-and-singular-value-decomposition/)) 或[随机投影](https://link.zhihu.com/?target=http%3A//scikit-learn.org/stable/modules/random_projection.html)。
3.  将数据随机分入训练集和验证集。按照一般规律，70%-90% 数据作为训练集。这个比例根据算法中有多少超参数，以及这些超参数对于算法的预期影响来决定。如果需要预测的超参数很多，那么就应该使用更大的验证集来有效地估计它们。如果担心验证集数量不够，那么就尝试交叉验证方法。如果计算资源足够，使用交叉验证总是更加安全的（份数越多，效果越好，也更耗费计算资源）。
4.  在验证集上调优，尝试足够多的 k 值，尝试 L1 和 L2 两种范数计算方式。
5.  如果分类器跑得太慢，尝试使用 Approximate Nearest Neighbor 库（比如 [FLANN](https://link.zhihu.com/?target=http%3A//www.cs.ubc.ca/research/flann/)）来加速这个过程，其代价是降低一些准确率。
6.  对最优的超参数做记录。记录最优参数后，是否应该让使用最优参数的算法在完整的训练集上运行并再次训练呢？因为如果把验证集重新放回到训练集中（自然训练集的数据量就又变大了），有可能最优参数又会有所变化。在实践中，**不要这样做**。千万不要在最终的分类器中使用验证集数据，这样做会破坏对于最优参数的估计。**直接使用测试集来测试用最优参数设置好的最优模型**，得到测试集数据的分类准确率，并以此作为你的 kNN 分类器在该数据上的性能表现。

# 线性分类

## 线性分类

上一篇笔记介绍了图像分类问题。图像分类的任务，就是从已有的固定分类标签集合中选择一个并分配给一张图像。我们还介绍了 k-Nearest Neighbor （k-NN）分类器，该分类器的基本思想是通过将测试图像与训练集带标签的图像进行比较，来给测试图像打上分类标签。k-Nearest Neighbor 分类器存在以下不足：

*   分类器必须_记住_所有训练数据并将其存储起来，以便于未来测试数据用于比较。这在存储空间上是低效的，数据集的大小很容易就以 GB 计。
*   对一个测试图像进行分类需要和所有训练图像作比较，算法计算资源耗费高。

**概述**：我们将要实现一种更强大的方法来解决图像分类问题，该方法可以自然地延伸到神经网络和卷积神经网络上。这种方法主要有两部分组成：一个是**评分函数（score function）**，它是原始图像数据到类别分值的映射。另一个是**损失函数（loss function）**，它是用来量化预测分类标签的得分与真实标签之间一致性的。该方法可转化为一个最优化问题，在最优化过程中，将通过更新评分函数的参数来最小化损失函数值。

## 从图像到标签分值的参数化映射

该方法的第一部分就是定义一个评分函数，这个函数将图像的像素值映射为各个分类类别的得分，得分高低代表图像属于该类别的可能性高低。下面会利用一个具体例子来展示该方法。现在假设有一个包含很多图像的训练集 $x_i\in R^D$，每个图像都有一个对应的分类标签 $y_i$。这里 $i=1,2...N$ 并且 $y_i\in 1...K$。这就是说，我们有 **N** 个图像样例，每个图像的维度是 **D**，共有 **K** 种不同的分类。

举例来说，在 CIFAR-10 中，我们有一个 **N**=50000 的训练集，每个图像有 **D**=32x32x3=3072 个像素，而 **K**=10，这是因为图片被分为 10 个不同的类别（狗，猫，汽车等）。我们现在定义评分函数为：$f:R^D\to R^K$，该函数是原始图像像素到分类分值的映射。

**线性分类器**：在本模型中，我们从最简单的概率函数开始，一个线性映射：  

$\displaystyle f(x_i,W,b)=Wx_i+b$

需要注意的几点：
*   首先，一个单独的矩阵乘法 $Wx_i$ 就高效地并行评估 10 个不同的分类器（每个分类器针对一个分类），其中每个类的分类器就是 W 的一个行向量。
*   注意我们认为输入数据 $(x_i,y_i)$是给定且不可改变的，但参数 **W** 和 **b** 是可控制改变的。我们的目标就是通过设置这些参数，使得计算出来的分类分值情况和训练集中图像数据的真实类别标签相符。在接下来的课程中，我们将详细介绍如何做到这一点，但是目前只需要直观地让正确分类的分值比错误分类的分值高即可。
*   该方法的一个优势是训练数据是用来学习到参数 **W** 和 **b** 的，一旦训练完成，训练数据就可以丢弃，留下学习到的参数即可。这是因为一个测试图像可以简单地输入函数，并基于计算出的分类分值来进行分类。
*   最后，注意只需要做一个矩阵乘法和一个矩阵加法就能对一个测试数据分类，这比 k-NN 中将测试图像和所有训练数据做比较的方法快多了。

_预告：卷积神经网络映射图像像素值到分类分值的方法和上面一样，但是映射 **(f)** 就要复杂多了，其包含的参数也更多。_

## 理解线性分类器

线性分类器计算图像中 3 个颜色通道中所有像素的值与权重的矩阵乘，从而得到分类分值。根据我们对权重设置的值，对于图像中的某些位置的某些颜色，函数表现出喜好或者厌恶（根据每个权重的符号而定）。举个例子，可以想象 “船” 分类就是被大量的蓝色所包围（对应的就是水）。那么 “船” 分类器在蓝色通道上的权重就有很多的正权重（它们的出现提高了 “船” 分类的分值），而在绿色和红色通道上的权重为负的就比较多（它们的出现降低了 “船” 分类的分值）。

![](<assets/1698237300636.png>)
**将图像看做高维度的点**：既然图像被伸展成为了一个高维度的列向量，那么我们可以把图像看做这个高维度空间中的一个点（即每张图像是 3072 维空间中的一个点）。整个数据集就是一个点的集合，每个点都带有 1 个分类标签。

既然定义每个分类类别的分值是权重和图像的矩阵乘，那么每个分类类别的分数就是这个空间中的一个线性函数的函数值。我们没办法可视化 3072 维空间中的线性函数，但假设把这些维度挤压到二维，那么就可以看看这些分类器在做什么了：

——————————————————————————————————————————

![](<assets/1698237301488.png>)
**将线性分类器看做模板匹配**：关于权重 **W** 的另一个解释是**它**的每一行对应着一个分类的模板（有时候也叫作_原型_）。一张图像对应不同分类的得分，是`通过使用内积（也叫_点积_）来比较图像和模板，然后找到和哪个模板最相似`。从这个角度来看，线性分类器就是在利用学习到的模板，针对图像做模板匹配。从另一个角度来看，可以认为还是在高效地使用 k-NN，不同的是我们没有使用所有的训练集的图像来比较，而是每个类别只用了一张图片（这张图片是我们学习到的，而不是训练集中的某一张），而且我们会使用（负）内积来计算向量间的距离，而不是使用 L1 或者 L2 距离。

![](<assets/1698237302346.png>)
可以看到马的模板看起来似乎是两个头的马，这是因为训练集中的马的图像中马头朝向各有左右造成的。线性分类器将这两种情况融合到一起了。类似的，汽车的模板看起来也是将几个不同的模型融合到了一个模板中，并以此来分辨不同方向不同颜色的汽车。这个模板上的车是红色的，这是因为 CIFAR-10 中训练集的车大多是红色的。线性分类器对于不同颜色的车的分类能力是很弱的，但是后面可以看到神经网络是可以完成这一任务的。神经网络可以在它的隐藏层中实现中间神经元来探测不同种类的车（比如绿色车头向左，蓝色车头向前等）。而下一层的神经元通过计算不同的汽车探测器的权重和，将这些合并为一个更精确的汽车分类分值。

**偏差和权重的****合并****技巧：在进一步学习前，要提一下这个经常使用的技巧。它能够将我们常用的参数 $W$ 和 $b$ 合二为一。回忆一下，分类评分函数定义为：

$\displaystyle f(x_i,W,b)=Wx_i+b$

分开处理这两个参数（权重参数 $W$ 和偏差参数 $b$）有点笨拙，一般常用的方法是把两个参数放到同一个矩阵中，同时 $x_i$ 向量就要增加一个维度，这个维度的数值是常量 1，这就是默认的_偏差维度_。这样新的公式就简化成下面这样：

$\displaystyle f(x_i,W)=Wx_i$

![](<assets/1698237303169.png>)

## 损失函数 Loss function

在上一节定义了从图像像素值到所属类别的评分函数（score function），该函数的参数是权重矩阵 $W$。在函数中，数据 $(x_i,y_i)$是给定的，不能修改。但是我们可以调整权重矩阵这个参数，使得评分函数的结果与训练数据集中图像的真实类别一致，即评分函数在正确的分类的位置应当得到最高的评分（score）。

回到之前那张猫的图像分类例子，它有针对 “猫”，“狗”，“船” 三个类别的分数。我们看到例子中权重值非常差，因为猫分类的得分非常低（-96.8），而狗（437.9）和船（61.95）比较高。我们将使用**损失函数（****Loss Function）**（有时也叫**代价函数** **Cost Function** 或**目标函数** **Objective**）来衡量我们对结果的不满意程度。直观地讲，当评分函数输出结果与真实结果之间差异越大，损失函数输出越大，反之越小。

## 多类支持向量机损失 Multiclass Support Vector Machine Loss

损失函数的具体形式多种多样。首先，介绍常用的多类支持向量机（SVM）损失函数。SVM 的损失函数想要 SVM 在正确分类上的得分始终比不正确分类上的得分高出一个边界值$\Delta$。我们可以把损失函数想象成一个人，这位 SVM 先生（或者女士）对于结果有自己的品位，如果某个结果能使得损失值更低，那么 SVM 就更加喜欢它。

让我们更精确一些。回忆一下，第 i 个数据中包含图像 $x_i$的像素和代表正确类别的标签 $y_i$ 。评分函数输入像素数据，然后通过公式 $f(x_i,W)$来计算不同分类类别的分值。这里我们将分值简写为 $s$。比如，针对第 j 个类别的得分就是第 j 个元素：$s_j=f(x_i,W)_j$。针对第 i 个数据的多类 SVM 的损失函数定义如下：
$\displaystyle L_i=\sum_{j\not=y_i}max(0,s_j-s_{y_i}+\Delta)$
**举例**：用一个例子演示公式是如何计算的。假设有 3 个分类，并且得到了分值 $s=[13,-7,11]$。其中第一个类别是正确类别，即 $y_i=0$。同时假设$\Delta$ 是 10（后面会详细介绍该超参数）。上面的公式是将所有不正确分类（$j\not=y_i$) 加起来，所以我们得到两个部分：  

$\displaystyle Li=max(0,-7-13+10)+max(0,11-13+10)$

可以看到第一个部分结果是 0，这是因为 [-7-13+10] 得到的是负数，经过 $max(0,-)$ 函数处理后得到 0。这一对类别分数和标签的损失值是 0，这是因为正确分类的得分 13 与错误分类的得分 - 7 的差为 20，高于边界值 10。而 SVM 只关心差距至少要大于 10，更大的差值还是算作损失值为 0。第二个部分计算 [11-13+10] 得到 8。虽然正确分类的得分比不正确分类的得分要高（13>11），但是比 10 的边界值还是小了，分差只有 2，这就是为什么损失值等于 8。简而言之，SVM 的损失函数想要正确分类类别 $y_i$的分数比不正确类别分数高，而且至少要高$\Delta$。如果不满足这点，就开始计算损失值。

那么在这次的模型中，我们面对的是线性评分函数$f(x_i,W)=Wx_i$，所以我们可以将损失函数的公式稍微改写一下：

$\displaystyle L_i=\sum_{j\not=y_i}max(0,w^T_jx_i-w^T_{y_i}x_i+\Delta)$

其中 $w_j$ 是权重 $W$ 的第 j 行，被变形为列向量。然而，一旦开始考虑更复杂的评分函数 $f$ 公式，这样做就不是必须的了。

在结束这一小节前，还必须提一下的属于是关于 0 的阀值：$max(0,-)$max(0,-) 函数，它常被称为**折叶损失（hinge loss）**。有时候会听到人们使用平方折叶损失 SVM（即 L2-SVM），它使用的是 $max(0,-)^2$，将更强烈（平方地而不是线性地）地惩罚过界的边界值。不使用平方是更标准的版本，但是在某些数据集中，平方折叶损失会工作得更好。可以通过交叉验证来决定到底使用哪个。

我们对于预测训练集数据分类标签的情况总有一些不满意的，而损失函数就能将这些不满意的程度量化。  

—————————————————————————————————————————

![](<assets/1698237314884.png>)

—————————————————————————————————————————

**正则化（Regularization）：上面损失函数有一个问题。假设有一个数据集和一个权重集 W** 能够正确地分类每个数据（即所有的边界都满足，对于所有的 i 都有 $L_i=0$）。问题在于这个 **W** 并不唯一：可能有很多相似的 **W** 都能正确地分类所有的数据。一个简单的例子：如果 **W** 能够正确分类所有数据，即对于每个数据，损失值都是 0。那么当$\lambda>1$时，任何数乘$\lambda W$ 都能使得损失值为 0，因为这个变化将所有分值的大小都均等地扩大了，所以它们之间的绝对差值也扩大了。举个例子，如果一个正确分类的分值和举例它最近的错误分类的分值的差距是 15，对 **W** 乘以 2 将使得差距变成 30。

换句话说，我们希望能向某些特定的权重 **W** 添加一些偏好，对其他权重则不添加，以此来消除模糊性。这一点是能够实现的，方法是向损失函数增加一个**正则化惩罚（regularization penalty）**$R(W)$R(W) 部分。最常用的正则化惩罚是 L2 范式，L2 范式通过对所有参数进行逐元素的平方惩罚来抑制大数值的权重：

$R(W)=\sum_k \sum_l W^2_{k,l}$

上面的表达式中，将 $W$ 中所有元素平方后求和。注意正则化函数不是数据的函数，仅基于权重。包含正则化惩罚后，就能够给出完整的多类 SVM 损失函数了，它由两个部分组成：**数据损失（data loss）**，即所有样例的的平均损失 $L_i$，以及**正则化损失（regularization loss）**。完整公式如下所示：  

$$L=\displaystyle \underbrace{ \frac{1}{N}\sum_i L_i}_{data \ loss}+\underbrace{\lambda R(W)}_{regularization \ loss}$$

将其展开完整公式是：

$$L=\frac{1}{N}\sum_i\sum_{j\not=y_i}[max(0,f(x_i;W)_j-f(x_i;W)_{y_i}+\Delta)]+\lambda \sum_k \sum_l W^2_{k,l}$$

其中，$N$ 是训练集的数据量。现在正则化惩罚添加到了损失函数里面，并用超参数$\lambda$来计算其权重。该超参数无法简单确定，需要通过交叉验证来获取。

除了上述理由外，引入正则化惩罚还带来很多良好的性质，这些性质大多会在后续章节介绍。比如引入了 L2 惩罚后，SVM 们就有了**最大边界（***max margin）这一良好性质。

**代码**：下面是一个无正则化部分的损失函数的 Python 实现，有非向量化和半向量化两个形式：

```python
def L_i(x, y, W):
  """
  unvectorized version. Compute the multiclass svm loss for a single example (x,y)
  - x is a column vector representing an image (e.g. 3073 x 1 in CIFAR-10)
    with an appended bias dimension in the 3073-rd position (i.e. bias trick)
  - y is an integer giving index of correct class (e.g. between 0 and 9 in CIFAR-10)
  - W is the weight matrix (e.g. 10 x 3073 in CIFAR-10)
  """
  delta = 1.0 # see notes about delta later in this section
  scores = W.dot(x) # scores becomes of size 10 x 1, the scores for each class
  correct_class_score = scores[y]
  D = W.shape[0] # number of classes, e.g. 10
  loss_i = 0.0
  for j in xrange(D): # iterate over all wrong classes
    if j == y:
      # skip for the true class to only loop over incorrect classes
      continue
    # accumulate loss for the i-th example
    loss_i += max(0, scores[j] - correct_class_score + delta)
  return loss_i

def L_i_vectorized(x, y, W):
  """
  A faster half-vectorized implementation. half-vectorized
  refers to the fact that for a single example the implementation contains
  no for loops, but there is still one loop over the examples (outside this function)
  """
  delta = 1.0
  scores = W.dot(x)
  # compute the margins for all classes in one vector operation
  margins = np.maximum(0, scores - scores[y] + delta)
  # on y-th position scores[y] - scores[y] canceled and gave delta. We want
  # to ignore the y-th position and only consider margin on max wrong class
  margins[y] = 0
  loss_i = np.sum(margins)
  return loss_i

def L(X, y, W):
  """
  fully-vectorized implementation :
  - X holds all the training examples as columns (e.g. 3073 x 50,000 in CIFAR-10)
  - y is array of integers specifying correct class (e.g. 50,000-D array)
  - W are weights (e.g. 10 x 3073)
  """
  # evaluate loss over all examples in X without using any for loops
  # left as exercise to reader in the assignment
```

在本小节的学习中，一定要记得 SVM 损失采取了一种特殊的方法，使得能够衡量对于训练数据预测分类和实际分类标签的一致性。还有，对训练集中数据做出准确分类预测和让损失值最小化这两件事是等价的。  
接下来要做的，就是找到能够使损失值最小化的权重了。  
## SVM 和 Softmax 的比较

下图有助于区分这 Softmax 和 SVM 这两种分类器：

![](<assets/1698237334342.png>)

针对一个数据点，SVM 和 Softmax 分类器的不同处理方式的例子。两个分类器都计算了同样的分值向量 **f**（本节中是通过矩阵乘来实现）。不同之处在于对 **f** 中分值的解释：SVM 分类器将它们看做是分类评分，它的损失函数鼓励正确的分类（本例中是蓝色的类别 2）的分值比其他分类的分值高出至少一个边界值。Softmax 分类器将这些数值看做是每个分类没有归一化的**对数概率**，鼓励正确分类的归一化的对数概率变高，其余的变低。SVM 的最终的损失值是 1.58，Softmax 的最终的损失值是 0.452，但要注意这两个数值没有可比性。只在给定同样数据，在同样的分类器的损失值计算中，它们才有意义。



