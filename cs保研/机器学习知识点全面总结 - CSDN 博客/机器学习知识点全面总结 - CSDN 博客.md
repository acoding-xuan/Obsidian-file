

**机器学习按照模型类型分为监督学习模型、无监督学习模型和概率模型三大类：**

![](<assets/1713096016909.png>)

![](<assets/1713096016971.png>)

### 学习路线

监督学习：有数据标注情况下学习（回归、分类）

半监督学习：训练数据中带标记的数据不够多

迁移学习：在已学习基础上，做看似和以前学习不相关的事情，但实际效果很好（在猫狗识别基础识别大象老虎等）

非监督学习：没有具体标注数据的情况下学习（机器阅读、机器绘画）

结构化学习：超越简单的回归和分类，产生结构化的结果（如图片、语言、声音）

## 二、[机器学习算法](https://so.csdn.net/so/search?q=%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95&spm=1001.2101.3001.7020)的类型

### 1. 有监督学习

![](<assets/1713096017018.png>)

利用有监督学习解决的问题大致上可以被分为两类：

**分类问题：** 预测某一样本所属的类别（离散的）。比如给定一个人（从数据的角度来说，是给出一个人的数据结构，包括：身高，年龄，体重等信息），然后判断是性别，或者是否健康。

**回归问题：** 预测某一样本的所对应的实数输出（连续的）。比如预测某一地区人的平均身高。

下面所介绍的前五个算法（线性回归，逻辑回归，分类回归树，朴素贝叶斯，K 最近邻算法）均是有监督学习的例子。
除此之外，集成学习也是一种有监督学习。它是将多个不同的相对较弱的机器学习模型的预测组合起来，用来预测新的样本。本文中所介绍的第九个和第十个算法（随机森林装袋法，和 XGBoost 算法）便是集成技术的例子。
### 2. 无监督学习
![](<assets/1713096017112.png>)
 无监督学习问题处理的是，只有输入变量 X 没有相应输出变量的训练数据。它利用没有专家标注训练数据，对数据的结构建模。
可以利用无监督学习解决的问题，大致分为两类：

**关联分析**：发现不同事物之间同时出现的概率。在购物篮分析中被广泛地应用。如果发现买面包的客户有百分之八十的概率买鸡蛋，那么商家就会把鸡蛋和面包放在相邻的货架上。

**聚类问题：** 将相似的样本划分为一个簇（cluster）。与分类问题不同，聚类问题预先并不知道类别，自然训练数据也没有类别的标签。

**降维：** 顾名思义，维度约减是指减少数据的维度同时保证不丢失有意义的信息。利用特征提取方法和特征选择方法，可以达到维度约减的效果。特征选择是指选择原始变量的子集。特征提取是将数据从高纬度转换到低纬度。广为熟知的主成分分析算法就是特征提取的方法。
### 3. 强化学习

  通过学习可以获得最大回报的行为，强化学习可以让 agent（个体）根据自己当前的状态，来决定下一步采取的动作。
强化学习算法通过反复试验来学习最优的动作。这类算法在机器人学中被广泛应用。在与障碍物碰撞后，机器人通过传感收到负面的反馈从而学会去避免冲突。在视频游戏中，我们可以通过反复试验采用一定的动作，获得更高的分数。Agent 能利用回报去理解玩家最优的状态和当前他应该采取的动作。  
 

## 三、常见机器学习概念介绍：

#### 1. 常见机器学习算法概念简介：
1、监督学习（SupervisedLearning）：有类别标签的学习，基于训练样本的输入、输出训练得到最优模型，再使用该模型预测新输入的输出；
代表算法：决策树、朴素贝叶斯、逻辑回归、KNN、SVM、神经网络、随机森林、AdaBoost、遗传算法；

2、半监督学习（Semi-supervisedLearning）：同时使用大量的未标记数据和标记数据，进行模式识别工作；
代表算法：self-training(自训练算法)、generative models 生成模型、SVMs 半监督支持向量机、graph-basedmethods 图论方法、 multiviewlearing 多视角算法等；

3、无监督学习（UnsupervisedLearning）：无类别标签的学习，只给定样本的输入，自动从中寻找潜在的类别规则；
代表算法：主成分分析方法 PCA 等，等距映射方法、局部线性嵌入方法、拉普拉斯特征映射方法、黑塞局部线性嵌入方法、局部切空间排列方法等；

4、HOG 特征：全称 Histogram of Oriented Gradient（方向梯度直方图），由图像的局部区域梯度方向直方图构成特征；

5、LBP 特征：全称 Local Binary Pattern（局部二值模式），通过比较中心与邻域像素灰度值构成图像局部纹理特征；

6、Haar 特征：描述图像的灰度变化，由各模块的像素差值构成特征；

7、核函数（Kernels）：从低维空间到高维空间的映射，把低维空间中线性不可分的两类点变成线性可分的；

8、SVM：全称 Support Vector Machine（支持向量机），在特征空间上找到最佳的超平面使训练集正负样本的间隔最大；是解决二分类问题的有监督学习算法，引入核方法后也可用来解决非线性问题；

9、Adaboost：全称 Adaptive Boosting（自适应增强），对同一个训练集训练不同的弱分类器，把这些弱分类器集合起来，构成一个更强的强分类器；

10、决策树算法（Decision Tree）：处理训练数据，构建决策树模型，再对新数据进行分类；

11、随机森林算法（Random Forest）：使用基本单元（决策树），通过集成学习将多棵树集成；

12、朴素贝叶斯（Naive Bayes）：根据事件的先验知识描述事件的概率，对联合概率建模来获得目标概率值；

13、神经网络（Neural Networks）：模仿动物神经网络行为特征，将许多个单一 “神经元” 联结在一起，通过调整内部大量节点之间相互连接的关系，进行分布式并行信息处理。

#### 2. 其余理论知识

**偏差**
偏差`度量了模型的期望预测与真实结果的偏离程度`， 即刻画了学习算法本身的拟合能力。偏差则表现为在特定分布上的适应能力，偏差越大越偏离真实值。

**方差**
方差度量了同样大小的训练集的变动所导致的学习性能的变化， 即刻画了数据扰动所造成的影响。方差越大，说明数据分布越分散。

**噪声**
噪声表达了在当前任务上任何模型所能达到的期望泛化误差的下界， 即刻画了学习问题本身的难度 。

**泛化误差、偏差、方差和模型复杂度的关系**（图片来源百面机器学习）

![](<assets/1713096017170.png>)

##### **Q2 什么是过拟合和欠拟合，为什么会出现这个现象？**

过拟合指的是在训练数据集上表现良好，而在未知数据上表现差。
欠拟合指的是模型没有很好地学习到数据特征，不能够很好地拟合数据，在训练数据和未知数据上表现都很差。如图所示：

![](<assets/1713096017214.png>)

过拟合的原因在于：
*   参数太多，模型复杂度过高；
*   建模样本选取有误，导致选取的样本数据不足以代表预定的分类规则；
*   样本噪音干扰过大，使得机器将部分噪音认为是特征从而扰乱了预设的分类规则；
*   假设的模型无法合理存在，或者说是假设成立的条件实际并不成立。


欠拟合的原因在于：
*   特征量过少；
*   模型复杂度过低。
    

##### **Q3** 怎么解决欠拟合？
*   增加新特征，可以考虑加入进特征组合、高次特征，来增大假设空间；
*   添加多项式特征，这个在机器学习算法里面用的很普遍，例如将线性模型通过添加二次项或者三次项使模型泛化能力更强；
*   减少正则化参数，正则化的目的是用来防止过拟合的，但是模型出现了欠拟合，则需要减少正则化参数；
*   使用非线性模型，比如核 SVM 、决策树、深度学习等模型；
*   调整模型的容量 (capacity)，通俗地，模型的容量是指其拟合各种函数的能力；
*   容量低的模型可能很难拟合训练集。

##### **Q4** 怎么解决过拟合？**（重点）**
*   获取和使用更多的数据（数据集增强）——解决过拟合的根本性方法
*   特征降维: 人工选择保留特征的方法对特征进行降维
*   加入正则化，控制模型的复杂度
*   Dropout
*   Early stopping
*   交叉验证 增加噪声
* 
## 四、十大机器学习算法介绍

### 有监督学习

#### 1. 线性回归算法

  在机器学习当中，我们有一个变量 X 的集合用来决定输出变量 Y。在输入变量 X 和输出变量 Y 之间存在着某种关系。机器学习的目的就是去量化这种关系。

![](<assets/1713096017266.png>)

  在线性回归里，输入变量 X 和输出变量 Y 之间的关系，用等式 Y = a + bX 来表示。因此，线性回归的目标便是找出系数 a 和 b 的值。在这里，a 是截距，b 是斜率。

  上图绘制了数据中 X 和 Y 的值。我们的目标是去拟合一条最接近所有点的直线。这意味着，直线上每一个 X 对应点的 Y 值与实际数据中点 X 对应的 Y 值，误差最小。

#### 2. 逻辑回归算法

  使用一个转换函数后，线性回归预测的是连续的值（比如降雨量），而逻辑回归预测的是离散的值（比如一个学生是否通过考试，是：0，否：1）。

  逻辑回归最适用于二分类（数据只分为两类，Y = 0 或 1，一般用 1 作为默认的类。比如：预测一个事件是否发生，事件发生分类为 1；预测一个人是否生病，生病分类为 1）。我们称呼其为逻辑回归（logistic regression）是因为我们的转换函数采用了 logistic function (h(x)=1/(1+e 的 - x 次方)) 。

  在逻辑回归中，我们首先得到的输出是连续的默认类的概率 p（0 小于等于 p 小于等于 1）。转换函数 (h(x)=1/(1+e 的 - x 次方)) 的值域便是（0,1）。我们对该函数设置一个域值 t。若概率 p>t，则预测结果为 1。

图 2 使用逻辑回归来判断肿瘤是恶性还是良性。如果概率 p 大于 0.5，则是恶性

![](<assets/1713096017319.png>)

在图 2 中，判断肿瘤是恶性还是良性。默认类 y=1（肿瘤是恶性）。当变量 X 是测量肿瘤的信息，如肿瘤的尺寸。如图所示，logistic 函数由变量 X 得到输出 p。域值 t 在这里设置为 0.5。如果 p 大于 t，那么肿瘤则是恶性。

我们考虑逻辑回归的一种变形：

![](<assets/1713096017351.png>)

 因此，逻辑回归的目标便是训练数据找到适当的参数的值，使得预测的输出和实际的输出最小。我们使用最大似然估计来对参数进行估计。

#### 3. 分类回归树（决策树）

 分类回归树是诸多决策树模型的一种实现，类似还有 ID3、C4.5、CART 等算法。

  非终端节点有根节点（Root Node）和内部节点 (Internal Node)。终端节点是叶子节点 (Leaf Node)。每一个非终端节点代表一个输出变量 X 和一个分岔点，叶叶子节点代表输出变量 Y，见图 3。沿着树的分裂（在分岔点做一次决策）到达叶子节点，输出便是当前叶子节点所代表的值。

![](<assets/1713096017412.png>)

图 3 中的决策树，根据一个人的年龄和婚姻状况进行分类：1. 购买跑车；2. 购买小型货车。如果这个人 30 岁还没有结婚，我们沿着决策树的过程则是：‘超过 30 年？–是 -- 已婚？–否，那么我们的输出便是跑车。

#### 4. 朴素贝叶斯
  
在给定一个事件发生的前提下，计算另外一个事件发生的概率——我们将会使用贝叶斯定理。假设先验知识为 d，为了计算我们的假设 h 为真的概率，我们将要使用如下贝叶斯定理：

![](<assets/1713096017460.png>)

P(h|d)= 后验概率。这是在给定数据 d 的前提下，假设 h 为真的概率。

P(d|h)= 可能性。这是在给定假设 h 为真的前提下，数据 d 的概率。

P(h)= 类先验概率。这是假设 h 为真时的概率（与数据无关）

P(d)= 预测器先验概率。这是数据的概率（与假设无关）

之所以称之为朴素是因为该算法假定所有的变量都是相互独立的（在现实生活大多数情况下都可以做这样的假设）。

![](<assets/1713096017500.png>)

如图，当天气是晴天的时候（第一列第一行），选手的状态是如何的呢？

在给定变量天气是晴天（sunny）的时候，为了判断选手的状态是‘yes’还是‘no’，计算概率，然后选择概率更高的作为输出。

![](<assets/1713096017562.png>)
因此，当天气是晴天的时候，选手的状态是‘yes’

#### 5. KNN（K 近邻算法）

K 最近邻算法是利用整个数据集作为训练集，而不是将数据集分成训练集和测试集。
当要预测一个新的输入实体的输出时，k 最近邻算法寻遍整个数据集去发现 k 个和新的实体距离最近的实体，或者说，k 个与新实体最相似的实体，然后得到这些输出的均值（对于回归问题）或者最多的类（对于分类问题）。而 k 的值一般由用户决定。

不同实体之间的相似度，不同的问题有不同的计算方法，包括但不限于：Euclidean distance 和 Hamming distance。

### 无监督学习算法
#### 7. K-means 算法

k-means 算法是一个迭代算法的聚类算法，它将相似的数据化到一个簇（cluster）中。该算法计算出 k 个簇的中心点，并将数据点分配给距离中心点最近的簇。

![](<assets/1713096017713.png>)

k-means 初始化：

a) 选择一个 k 值。如图 6，k=3。

b) 随机分配每一个数据点到三个簇中的任意一个。

c) 计算每一个簇的中心点。如图 6，红色，蓝色，绿色分别代表三个簇的中心点。

将每一个观察结果与当前簇比较：

a) 重新分配每一个点到距中心点最近的簇中。如图 6，上方 5 个点被分配给蓝色中心点的簇。

重新计算中心点：

a) 为新分配好的簇计算中心点。如图六，中心点改变。

迭代，不再改变则停止：

a) 重复步骤 2-3，直到所有点所属簇不再改变。

#### 8. PCA 主成分分析

主成分分析是通过减少变量的维度，去除数据中冗余的部分或实现可视化。基本的思路将数据中最大方差的部分反映在一个新的坐标系中，这个新的坐标系则被称为 “主要成分”。其中每一个成分，都是原来成分的线性组合，并且每一成分之间相互正交。正交性保证了成分之间是相互独立的。

第一主成分反映了数据最大方差的方向。第二主成分反映了数据中剩余的变量的信息，并且这些变量是与第一主成分无关的。同样地，其他主成分反映了与之前成分无关的变量的信息。

![](<assets/1713096017755.png>)

### 集成学习技术

集成学习是一种将不同学习模型（比如分类器）的结果组合起来，通过投票或平均来进一步提高准确率。一般，对于分类问题用投票；对于回归问题用平均。这样的做法源于 “众人拾材火焰高” 的想法。
集成算法主要有三类：Bagging，Boosting 和 Stacking。本文将不谈及 stacking。

[集成学习](集成学习/集成学习.md)







[机器学习——笔记整理（原理、python 代码）——反反复复，出精活！_珞沫的博客 - CSDN 博客](https://blog.csdn.net/weixin_45666566/article/details/106455782 "机器学习——笔记整理（原理、python代码）——反反复复，出精活！_珞沫的博客-CSDN博客")

[1. 机器学习面试知识点总结 (更新中...)_Meggie 的博客 - CSDN 博客](https://blog.csdn.net/hhchenzheng/article/details/89449678 "1.机器学习面试知识点总结(更新中...)_Meggie的博客-CSDN博客")
#### 机器学习优秀博客参考：
[红色石头的个人博客 - 机器学习、深度学习之路](http://redstonewill.com/ "红色石头的个人博客-机器学习、深度学习之路")

[机器学习初学者 - AI 入门的宝典](http://www.ai-start.com/ "机器学习初学者-AI入门的宝典")