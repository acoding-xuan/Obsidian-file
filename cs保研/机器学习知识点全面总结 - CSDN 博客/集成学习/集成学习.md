---
url: https://blog.csdn.net/weixin_45666566/article/details/107920914
title: 机器学习笔记 17——集成学习 (ensemble learning) 简介_采用集成学习模型有哪些 - CSDN 博客
date: 2024-04-18 22:56:51
tag: 
summary: 
---

![](<assets/1713452211566.png>)

## 1、概述

 集成学习通过构建并**结合多个学习器** (称为基学习器）来完成学习任务，有时也被称为多学习器系统、基于委员会的学习等。集成学习通过将多个学习器进行结合，常可获得比单一学习器显著优越的泛化性能。
## 2、基本步骤

  对于训练集数据，我们通过训练若干个**个体学习器**（`基学习器`），通过一定的**结合策略**，就可以最终形成一个强学习器，以达到博采众长的目的。因此，集成学习主要有以下两个问题

Q1：如何获得若干个个体学习器？  
Q2：采用什么样的结合策略？

  集成学习一般可以分为以下 3 个步骤：

（1）找到误差互相独立的基学习器。  
（2）训练基学习器。  
（3）合并基学习器的结果。

### 2.1 基学习器

这里也涉及到三个问题  
（1）选择什么算法训练个体学习器？  
  通常选择一个现有的学习算法训练产生个体学习器，例如决策树、神经网络

（2）不同的基学习器用同样的算法吗？  
  可以用相同的算法，这样的集成是 “同质” 的，我们可以管 “同质” 的集成里的个体学习器叫基学习器；
  也可以用不同的算法，这样的集成是 “异质” 的，我们可以管 “异质” 的集成里的个体学习器叫组件学习器

#### 同质学习器
*   **1、** 同质学习器（常用）****  
    比如都是决策树个体学习器，或者都是神经网络个体学习器。
    *   强依赖同质学习器  
        个体学习器之间存在强依赖关系，一系列个体学习器基本都需要串行生成，代表算法是 boosting 系列算法，Boosting 主要关注降低偏差，因此 Boosting 能基于泛化性能相当弱的学习器构建出很强的集成；
    *   无依赖同质学习器  
        个体学习器之间不存在强依赖关系，一系列个体学习器可以并行生成，代表算法是 bagging 和随机森林系列算法，Bagging 主要关注降低方差，因此它在不剪枝的决策树、神经网络等学习器上效用更为明显。
#### 异质学习器
*   **2、异质学习器**  
    比如我们有一个分类问题，对训练集采用支持向量机个体学习器，逻辑回归个体学习器和朴素贝叶斯个体学习器来学习，再通过某种结合策略来确定最终的分类强学习器。

（3）什么样的个体学习器更好？  
  个体学习器必须有一定的 “准确性”；一定的 “多样性”，简单的说就是 “好而不同”。
  
### 2.2 合并基学习器策略

  对于基分类器最终的结合策略常见的方法有如下几种：

1.  平均法  
    对于数值型输出，最常见的结合策略即为平均法：
    
    *   ）简单平均法
    *   ）加权平均法
2.  投票法：类别标记输出  
    对于分类任务来说，学习器将从类别集合中预测出一个标记，最常见的结合策略是投票法。
    *   ）绝对多数投票法  
        预测为得票超过半数的标记，如果没有就拒绝预测
    *   ）相对多数投票法  
        预测为得票最多的标记，若同时有多个标记获最高票，则从中随机选取一个。即我们常说的少数服从多数。
    *   ）加权投票法  
        每个弱学习器的票数乘以一个权重，预测为加权后的票数最多的标记
3.  学习法  
      当训练数据很多时，一种更为强大的结合策略是是使用**学习法**，即通过另一个学习器来对所有基学习器产生结果的结合方法进行学习，这时候个体学习器称为初级学习器，用于结合的学习器成为次级学习器或元学习器。
    
  对于学习法，代表方法是 stacking，当使用 stacking 的结合策略时， 我们不是对弱学习器的结果做简单的逻辑处理，而是再加上一层学习器，也就是说，我们将训练集弱学习器的学习结果作为输入，将训练集的输出作为输出，重新训练一个学习器来得到最终结果。

## 3、集成学习算法

  根据个体学习器的生成方式，目前的集成学习方法大致可以分为两大类，即个体学习器间**存在强依赖关系**、**必须串行生成的序列化方法**，以及个体学习器间**不存在强依赖关系**、**可同时生成的并行化方法**；前者的代表是 Boosting，后者的代表是 Bagging 和随机森林。

  集成学习是一个大家族，学习的方式有 Boosting，Bagging 等，每个家族分别有很多不同的实现，他们的不同主要在于生成学习器方式。

### 3.1 Boosting

  Boosting 方法训练基学习器时采用串行的方式，各个基学习器之间有依赖。

**学习机制：**

![](<assets/1713452211676.png>)

*   先从初始训练集训练出一个基学习器；
*   再根据基学习器的表现对训练样本分布进行调整（数据加权），**使得先前基学习器做错的训练样本在后续受到更多关注；**
*   然后基于调整后的样本分布来训练下一个基学习器；
*   重复进行上述步骤，直至基学习器数目达到事先指定的值 T，最终将这 T 个基学习器进行加权结合。

**原理：**
  Boosting，是集成学习算法的一种，核心思想就是：
  1）基学习器之间存在强依赖关系，每一个基分类器是在前一个基分类器的基础之上生成；
  2）将所有基学习器结果进行线性加权求和，作为最终结果输出。所以 boosting 算法，是一个加法模型，再对加法模型进行优化。

  总的来说，**提升方法使用加法模型和前向分步算法。**

#### 3.1.3 系列算法

![](<assets/1713452211721.png>)

##### 3.1.3.1 AdaBoost 算法

[见本文](https://blog.csdn.net/weixin_45666566/article/details/107922928)

##### 3.1.3.2 BDT 算法

[见此文](https://blog.csdn.net/weixin_45666566/article/details/107942444)

##### 3.1.3.3 GBDT 算法

[见本文](https://blog.csdn.net/weixin_45666566/article/details/107943554)

##### 3.1.3.4 XGBoost 算法

[见此文](https://blog.csdn.net/weixin_45666566/article/details/107976411)

**总结:**  
  Boosting 是一族可将弱学习器提升为强学习器的算法。Boosting 算法是对特点的数据分布进行学习，可以通过 “重赋值法” 或者 “重采样法” 来处理。Boosting 算法关注降低偏差，可以基于泛化能力相当弱的学习器构建出很强的集成
### 3.2 Bagging 算法

  Bagging 与 Boosting 的串行训练不同，Bagging 方法在训练过程中，各基学习器之间无强依赖，可以进行并行训练。

**学习机制：**

![](<assets/1713452211788.png>)


**基本流程：**

  对原始数据**自助采样**，产生 T 个不同的子集，分别学习生成 T 个基学习器，再将这些基学习器结合。在对预测输出进行结合时，Bagging 通常对分类任务使用简单投票法，对回归任务使用简单平均法。

*   自助采样法（Bootstrap sampling）, 即对于 m 个样本的原始训练集，我们每次先随机采集一个样本放入采样集，接着把该样本放回，也就是说下次采样时该样本仍有可能被采集到，这样采集 m 次，最终可以得到 m 个样本的采样集，初始训练集中有的样本在采样集里多次出现，有的则从未出现【初始训练集中约有 63.2% 的样本出现在采样集里】。由于是随机采样，这样每次的采样集是和原始训练集不同的，和其他采样集也是不同的，这样得到多个不同的弱学习器。

**算法的伪代码如下：**

![](<assets/1713452211849.png>)


  从偏差 - 方差分解的角度看，降低一个估计的方差的方式是把多个估计平均起来，所以 Bagging 主要关注降低方差，因此它在不剪枝决策树、神经网络等易受样本扰动的学习器上效用更为明显。同时，Bagging 并不能减小模型的偏差，所以应尽量选择偏差较小的基分类器，如未剪枝的决策树。

**特点：**

*   1）样本选择上：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。
*   2）样例权重：使用均匀取样，每个样例的权重相等
*   3）预测函数：所有预测函数的权重相等。
*   4）并行计算：各个预测函数可以并行生成，他们是相互独立的。
*   5）Bagging 对样本重采样，对每一重采样得到的子样本集训练一个模型，最后取平均。
*   6）Bagging 算法主要关注降低方差，从而降低过拟合，但不会降低偏差，因此最好不要用高偏差的模型。
*   7）与标准 AdaBoost 只适用于二分类任务不同，Bagging 能不经修改地用于多分类、回归等任务。

#### 3.2.1 常见算法

  基于决策树基分类器的随机森林（Random Forest, 简称 RF）