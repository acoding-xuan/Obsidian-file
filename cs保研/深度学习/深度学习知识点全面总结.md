## 五、优化方法总结 
### 一、基本的梯度下降方法
深度学习网络训练过程可以分成两大部分：前向计算过程与反向传播过程。前向计算过程，是指通过我们预先设定好的卷积层、池化层等等，按照规定的网络结构一层层前向计算，得到预测的结果。反向传播过程，是为了将设定的网络中的众多参数一步步调整，使得预测结果能更加贴近真实值。

那么，在反向传播过程中，很重要的一点就是：参数如何更新？或者问的更具体点：参数应该朝着什么方向更新？

显然，参数应该是朝着目标损失函数下降最快的方向更新，**更确切的说，要朝着梯度方向更新！** 假设网络参数是 θ，学习率是 η ，网络表示的函数是 J (θ) ，函数此时对θ 的梯度为：▽ θ J ( θ )，于是参数 θ的更新公式可表示为：

![](<assets/1713096349239.png>)
在深度学习中，有三种最基本的梯度下降算法：SGD、BGD、MBGD，他们各有优劣。根据不同的数据量和参数量，可以选择一种具体的实现形式，在训练神经网络是优化算法大体可以分为两类：

1）调整学习率，使得优化更稳定；
2）梯度估计修正，优化训练速度。
![](<assets/1713096349305.png>)
#### （1）随机梯度下降法 SGD

随机梯度下降法 (Stochastic Gradient Descent，SGD)，每次迭代（更新参数）只使用**单个训练样本**（ x (i) , y ( i ) ） ，其中 x 是输入数据，y 是标签。因此，参数更新表达式如下：

![](<assets/1713096349389.png>)
  
优点： SGD 一次迭代只需**对一个样本进行计算**，因此运行速度很快，还可用于在线学习。
缺点：
（1）由于`单个样本的随机性`，实际过程中，`目标损失函数值会剧烈波动`，一方面，SGD 的波动使它能够跳到新的可能更好的局部最小值。另一方面，使得训练永远不会收敛，而是会一直在最小值附近波动。
（2）一次迭代只计算一张图片，没有发挥 GPU 并行运算的优势，使得整体计算的效率不高。

#### （2）批量梯度下降法 BGD
批量梯度下降法 (Batch Gradient Descent，BGD)，每次迭代更新中**使用所有的训练样本**，参数更新表达式如下：
![](<assets/1713096349421.png>)

优缺点分析：BGD 能保证收敛到凸误差表面的全局最小值和非凸表面的局部最小值。
但每迭代一次，需要用到训练集中的所有数据，如果数据量很大，那么迭代速度就会非常慢。

#### （3）小批量梯度下降法 MBGD
小批（3）量梯度下降法 (Mini-Batch Gradient Descent，MBGD)，折中了 BGD 和 SGD 的方法，**每次迭代使用 batch_size 个训练样本**进行计算，参数更新表达式如下：

![](<assets/1713096349453.png>)
优缺点分析：因为每次迭代使用多个样本，所以 MBGD 比 SGD 收敛更稳定，也能避免 BGD 在数据集过大时迭代速度慢的问题。因此，MBGD 是深度学习网络训练中经常使用的梯度下降方法。

**深度学习中，一般的 mini-batch 大小为 64~256，考虑到电脑存储设置和使用的方式，如果 mini-batch 是 2 的次方，代码会运行地更快一些。**  

![](<assets/1713096349491.png>)
上图是 BGD 和 MBGD 训练时，损失代价函数的变化图。可见 BGD 能使代价函数逐渐减小，最终保证收敛到凸误差表面的全局最小值；MBGD 的损失代价函数值比较振荡，但最终也能优化到损失最小值。

### 二、Momentum 动量梯度下降

Momentum 主要引入了**基于梯度的移动指数加权平均**的思想，即`当前的参数更新方向不仅与当前的梯度有关，也受历史的加权平均梯度影响`。对于**梯度指向相同方向的维度，动量会积累并增加**，而对于梯度**改变方向的维度，动量会减少更新**。这也就使得收敛速度加快，同时又不至于摆动幅度太大。

**动量梯度下降法作用是加快学习速度，还有一定摆脱局部最优的能力。如下图红色线所示：**  

![](<assets/1713096349871.png>)

动量梯度下降（Momentum）的参数更新表达式如下所示：
![](<assets/1713096350003.png>)

**其中，λ  表示动量参数 momentum；当λ = 0 时，即是普通的 SGD 梯度下降。0 < λ < 1 ，表示带了动量的 SGD 梯度下降参数更新方式，λ 通常取 0.9。**

普通 SGD 的缺点：SGD 很难在沟壑（即曲面在一个维度上比在另一个维度上弯曲得更陡的区域）中迭代，这在局部最优解中很常见。在这些场景中，SGD 在沟壑的斜坡上振荡，同时沿着底部向局部最优方向缓慢前进。为了缓解这一问题，引入了动量 momentum。

![](<assets/1713096350044.png>)

本质上，当使用动量时，如同我们将球推下山坡。球在滚下坡时积累动量，在途中变得越来越快。同样的事情发生在参数更新上：对于梯度指向相同方向的维度，动量会积累并增加，而对于梯度改变方向的维度，动量会减少更新。**结果，我们获得了更快的收敛和减少的振荡。**

### 三、Adam 优化器 (RMSprop + Momentum)

Adam 是另一种参数**自适应学习率**的方法，**相当于 RMSprop + Momentum**，利用**梯度的一阶矩估计和二阶矩估计动态调整**每个参数的学习率。公式如下：
![](<assets/1713096350095.png>)
由于移动指数平均在迭代开始的初期会导致和开始的值有较大的差异，所以我们需要对上面求得的几个值做偏差修正。通过计算偏差校正的一阶和二阶矩估计来抵消这些偏差：

![](<assets/1713096350138.png>)
然后使用这些来更新参数，就像在 RMSprop 中看到的那样， Adam 的参数更新公式：
![](<assets/1713096350171.png>)
在 Adam 算法中，参数 β1 所对应的就是 Momentum 算法中的 β 值，一般取 0.9，参数 β2 所对应的就是 RMSProp 算法中的 β 值，一般我们取 0.999，而 ϵ 是一个平滑 ，而学习率则需要我们在训练的时候进行微调。
![](<assets/1713096350212.png>)
## 六、损失函数

### **1 回归损失函数**

#### (1) 均/方误差损失函数
均方误差 (Mean Squared Error Loss, MSE) 损失函数定义如下：
![](<assets/1713096350493.png>)
#### **(2) 平均绝对误差损失函数**
平均绝对误差 (Mean Absolute Error Loss, MAE) 损失函数定义如下：
![](<assets/1713096350550.png>)
###   **3 分类损失函数**
####  **(3) 交叉熵损失函数**
Logistic 损失函数和负对数似然损失函数只能处理二分类问题，对于两个分类扩展到 M 个分类，使用交叉熵损失函数 (Cross Entropy Loss)，其定义如下：
![](<assets/1713096350726.png>)
### 5. 激活函数、损失函数、优化函数的区别
**1. 激活函数：将神经网络上一层的输入，经过神经网络层的非线性变换转换后，通过激活函数，得到输出。常见的激活函数包括：sigmoid, tanh, relu 等。
**2. 损失函数：度量神经网络的输出的预测值，与实际值之间的差距的一种方式。常见的损失函数包括：最小二乘损失函数、交叉熵损失函数、回归中使用的 smooth L1 损失函数等。
**3. 优化函数：也就是如何把损失值从神经网络的最外层传递到最前面。如最基础的梯度下降算法，随机梯度下降算法，批量梯度下降算法，带动量的梯度下降算法，Adagrad，Adadelta，Adam 等。

## 七、CNN 卷积神经网络
卷积神经网络（Convolutional Neural Networks, CNN）是一类包含卷积计算且具有深度结构的前馈神经网络（Feedforward Neural Networks），是深度学习（deep learning）的代表算法之一 。目前 CNN 已经得到了广泛的应用，比如：人脸识别、自动驾驶、美图秀秀、安防等很多领域。
**卷积神经网络 – CNN 最擅长的就是图片的处理。它受到人类视觉神经系统的启发。**
### CNN 特点：
1.  能够有效的将大数据量的图片降维成小数据量。
2.  能够有效的保留图片特征，符合图片处理的原则。
### CNN 两大核心：
**卷积层最主要的两个特征就是局部连接和权值共享，有些地方又叫做稀疏连接和参数共享。**
1.  通过卷积操作实现局部连接，这个局部区域的大小就是滤波器 filter，避免了全连接中参数过多造成无法计算的情况，
2.  再通过参数共享来缩减实际参数的数量，为实现多层网络提供了可能。

![](<assets/1713096350827.png>)
#### **1. 局部连接：**
*   一般认为图像的空间联系是局部的像素联系比较密切，而距离较远的像素相关性较弱，因此，每个神经元没必要对全局图像进行感知，只要对局部进行感知，然后在更高层将局部的信息综合起来得到全局信息。利用卷积层实现：（特征映射，每个特征映射是一个神经元阵列）：从上一层通过局部卷积滤波器提取局部特征。卷积层紧跟着一个用来求局部平均与二次提取的计算层，这种二次特征提取结构减少了特征分辨率。
*   即网络部分连通，每个神经元只与上一层的部分神经元相连，只感知局部，而不是整幅图像。（滑窗实现）

####  **2. 参数共享:**
*   在局部连接中，每个神经元的参数都是一样的，即：**同一个卷积核在图像中都是共享的**。（理解：卷积操作实际是在提取一个个局部信息，而局部信息的一些统计特性和其他部分是一样的，也就意味着这部分学到的特征也可以用到另一部分上。所以对图像上的所有位置，都能使用同样的学习特征。）卷积核共享有个问题：提取特征不充分，可以通过增加多个卷积核来弥补，可以学习多种特征。
    
*   对于一个 100 x 100 像素的图像，如果我们用一个神经元来对图像进行操作，这个神经元大小就是 100x100=10000，单如果我们使用 10x10 的卷积核，我们虽然需要计算多次，但我们需要的参数只有 10x10=100 个，加上一个偏向 b，一共只需要 101 个参数。我们取得图像大小还是 100 x 100。
    
*   如果我们取得图像比较大，它的参数将会更加多。我们通过 10 * 10 的卷积核对图像进行特征提取，这样我们就得到一个 Feature Map。
    
*   一个卷积核只能提取一个特征，所以我们需要多几个卷积核，假设我们有 6 个卷积核，我们就会得到 6 个 Feature Map，将这 6 个 Feature Map 组成一起就是一个神经元。这 6 个 Feature Map 我们需要 101 * 6=606 个参数。这个值和 10000 比还是比较小的。
### CNN 网络介绍

以下主要介绍：卷积层、池化层、激活函数、全连接层概念及原理
![](<assets/1713096350886.png>)

###  1. 卷积层

卷积是一种有效提取图片特征的方法 。 一般用一个正方形卷积核，遍历图片上的每一个像素点。图片与卷积核重合区域内相对应的每一个像素值，乘卷积核内相对应点的权重，然后求和， 再加上偏置后，最后得到输出图片中的一个像素值。  
图片分灰度图和彩色图，卷积核可以是单个也可以是多个，因此卷积操作分以下三种情况：

#### 1.1 单通道输入，单卷积核

这里单通道指的是输入为灰度图，单卷积核值卷积核个数是 1 个。

![](<assets/1713096351034.png>)

上面是 5 x 5 x 1的灰度图片，1 表示单通道，5 x 5 表示分辨率，共有 5 行 5 列个灰度值。若用一个 3x3x1 的卷积核对此 5x5x1 的灰度图片进行卷积，偏置项 b = 1，则求卷积的计算是：(-1)x1+0x0+1x2+(-1)x5+0x4+1x2+(-1) x3+0x4+1x5+1=1（注意不要忘记加偏置 1）。

#### 1.2 多通道输入，单卷积核

多数情况下，输入的图片是 R, G, B 三个颜色组成的彩色图，输入的图片包含了红、绿、蓝三层数据，**卷积核的深度（通道数）应该等于输入图片的通道数**，所以使用 3 x 3 x 3 的卷积核，最后一个 3 表示匹配输入图像的 3 个通道，这样这个卷积核有三通道，**每个通道都会随机生成 9 个待优化的参数**，一共有 27 个待优化参数 w 和一个偏置 b。

![](<assets/1713096351080.png>)

注：这里还是单个卷积核的情况，但是一个卷积核可以有多个通道。默认情况下，**卷积核的通道数等于输入图片的通道数。**

#### 1.3 多通道输入，多卷积核

多通道输入、多卷积核是深度[神经网络](https://so.csdn.net/so/search?q=%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C "神经网络")中间最常见的形式。指的是多通道输入，且用多个卷积核的情况。那么卷积过程其实也很简单，**以 3 通道输入，2 个卷积核为例**：

（1）先取出一个卷积核与 3 通道的输入进行卷积，这个过程就和**多通道输入，单卷积核**一样，得到一个 1 通道的输出 output1。同样再取出第二个卷积核进行同样的操作，得到第二个输出 output2  
（2）将相同 size 的 output1 与 output2 进行堆叠，就得到 2 通道的输出 output。

为了更直观地理解，下面给出图示：

![](<assets/1713096351125.png>)

​  
图中输入 X:[1,h,w,3] 指的是：输入 1 张高 h 宽 w 的 3 通道图片。  
卷积核 W:[k,k,3,2] 指的是：卷积核尺寸为 3*3，通道数为 3，个数为 2。

**总结：**  
（1）卷积操作后，**输出的通道数 = 卷积核的个数**  
（2）卷积核的个数和卷积核的通道数是不同的概念。每层卷积核的个数在设计网络时会给出，但是卷积核的通道数不一定会给出。默认情况下，**卷积核的通道数 = 输入的通道数**，因为这是进行卷积操作的**必要条件**。
*   卷积与同道解释参考：[CNN 卷积核与通道讲解](https://blog.csdn.net/yilulvxing/article/details/107452153 "CNN卷积核与通道讲解")

（3）**偏置数 = 卷积核数**
#### 1.4 填充 padding
为了使卷积操作后能得到满意的输出图片尺寸，经常会使用 padding 对输入进行填充操作。默认在图片周围填充 0。

**（1）全零填充 padding='same’**  
使用 same 时，会自动对原图进行全 0 填充，当步长为 1 时，可以保证输出的图片与输入的图片尺寸一致。  
输出尺寸计算公式：**输入长 / 步长 （向上取整）**  
TensorFlow 中实现如下：（这里以卷积核个数：48，卷积核尺寸：3，步长：1，全填充为例）

```
layers.Conv2D(48, kernel_size=3, strides=1, padding='same')
```

**（2）不填充 padding='valid’**  
使用 valid 时，不进行任何填充，直接进行卷积，这是 layers.Conv2D() 默认的方式。  
输出尺寸计算公式：（**输入长 - 核长）/ 步长 + 1 （向下取整）**  
TensorFlow 中实现如下：

```
layers.Conv2D(48, kernel_size=3, strides=1, padding='valid')
```
**（3）自定义填充**  
一般是从上下左右四个方向进行填充，且左、右填充的列数 p w p_wpw​一般相同，上、下填充的行数 p h p_hph​也应该相同。如下图所示：  

![](<assets/1713096351167.png>)
输出尺寸计算公式：
![](<assets/1713096351220.png>)
![](<assets/1713096351252.png>)

​其中，h,w 为原图的高和宽，k 是卷积核的尺寸，s 是步长。

在 TensorFlow2.0 中，自定义填充过程中，padding 参数的设置格式为：  
**padding=[[0，0]，[上，下]，[左，右]，[0，0] ]**

```
# 例如要在上下左右各填充一个单位，实现如下：
layers.Conv2D(48, kernel_size=3, strides=1, padding=[[0,0], [1,1], [1,1], [0,0]])
```

### 2. 池化层

池化作用如下 
1. 使卷积神经网络抽取特征是保证特征局部不变性。
2. 池化操作能降低维度，减少参数数量。
3. 池化操作优化比较简单。
在卷积层中，可以通过调节步长参数 s 实现特征图的高宽成倍缩小，从而降低了网络的参数量。实际上，除了通过设置步长，还有一种专门的网络层可以实现尺寸缩减功能，它就是我们要介绍的池化层 (Pooling layer)。

池化层同样基于局部相关性的思想，通过从局部相关的一组元素中进行采样或信息聚合，从而得到新的元素值。通常我们用到两种池化进行下采样：  
**（1）最大池化 (Max Pooling)**，从局部相关元素集中选取最大的一个元素值。  
**（2）平均池化 (Average Pooling)**，从局部相关元素集中计算平均值并返回。

### 3. 激活函数

激活函数也是神经网络不可缺少的部分，激活函数是用来加入非线性因素，提高网络表达能力，卷积神经网络中最常用的是 ReLU，Sigmoid 使用较少。

![](<assets/1713096351298.png>)

具体如何选择合适的激活函数。可参考这篇博文：[神经网络搭建：激活函数总结](https://blog.csdn.net/wjinjie/article/details/104729911 "神经网络搭建：激活函数总结")  或者上文介绍。

### 4. 全连接层

全连接层 FC，之所以叫全连接，是因为`每个神经元与前后相邻层的每一个神经元都有连接关系`。如下图所示，是一个简单的两层全连接网络，输入时特征，输出是预测的结果。

![](<assets/1713096351349.png>)
全连接层的参数量是可以直接计算的，计算公式如下：

![](<assets/1713096351408.png>)
按照上图搭建的两层全连接网络，要训练分辨率仅仅是 28x28=784 的黑白图像，就有近 40 万个待优化的参数。现实生活中高分辨率的彩色图像，像素点更多，且为红绿蓝三通道信息。待优化的参数过多， 容易导致模型过拟合。为避免这种现象，实际应用中一般不会将原始图片直接喂入全连接网络。  
在实际应用中，会先对原始图像进行卷积特征提取，把提取到的特征喂给全连接网络，再让全连接网络计算出分类评估值。

### 5. 网络参数量与计算量

#### 卷积层参数 / 卷积计算量

卷积参数 = 卷积核长度 x 卷积核宽度 x 输入通道数 x 输出通道数 + 输出通道数（偏置）  
卷积计算量 = 输出数据大小 x 卷积核的尺度 x 输入通道数

例：输入：224x224x3，输出：224x244x64，卷积核：3x3

*   参数量 = 3x3x3x64+64
*   计算量 = 224x224x64x3x3x3

**卷积层：**

比如：输入是 32x32x3 的的彩色图片，经过卷积层：

```
layers.Conv2D(100, kernel_size=3, strides=1, padding='same')
```

**（1）网络参数量**  
主要是**卷积核的参数和偏置**的参数： 3x3x3x100+100=2800

**（2）计算量 FLOPS**  
[深度学习](https://so.csdn.net/so/search?q=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0 "深度学习")框架 FLOPs 的概念：Floating point operations，即浮点运算数量。  
{32x32x[3x3+(3x3-1)]x3+32x32x(3-1)}x100

**全连接层：**

比如第一层节点数是 5，第二层节点数是 10，求网络参数量和计算量 FLOPS

**（1）网络参数量**  
网络参数量主要来源**神经元连接权重和偏置**：5x10+10=60

**（2）计算量 FLOPS**  
5x10+10=60  
2015 年，Google 研究人员 Sergey 等基于参数标准化设计了 **BN 层**。BN 层提出后，广泛地应用在各种深度网络模型上，使得网络的超参数的设定更加自由，同时网络的收敛速度更快，性能也更好。  
详细了解请看：[神经网络搭建：BN 层](https://blog.csdn.net/wjinjie/article/details/105028870 "神经网络搭建：BN层")

### 6. 卷积神经网络训练：

#### **训练基本流程：**

Step 1：用随机数初始化所有的卷积核和参数 / 权重

Step 2：将训练图片作为输入，执行前向步骤 (卷积， ReLU，池化以及全连接层的**前向传播**) 并计算每个类别的对应输出概率。

Step 3：计算输出层的总误差

Step 4：**反向传播算法**计算误差相对于所有权重的梯度，并用梯度下降法**更新所有的卷积核和参数 / 权重的值**，以使输出误差最小化

注：卷积核个数、卷积核尺寸、网络架构这些参数，是在 Step 1 之前就已经固定的，且不会在训练过程中改变——**只有卷积核矩阵和神经元权重会更新**。

![](<assets/1713096351486.png>)

和多层神经网络一样，卷积神经网络中的参数训练也是使用**误差反向传播算法**，关于池化层的训练，需要再提一下，是将池化层改为多层神经网络的形式

![](<assets/1713096351547.png>)

将卷积层也改为多层神经网络的形式

![](<assets/1713096351717.png>)

#### CNN 详细求解：

CNN 在本质上是一种输入到输出的映射，它能够学习大量的输入与输出之间的映射关系，而不需要任何输入和输出之间的精确的数学表达式，只要用已知的模式对卷积网络加以训练，网络就具有输入输出对之间的映射能力。

卷积网络执行的是监督训练，所以其样本集是由形如：（输入向量，理想输出向量）的向量对构成的。所有这些向量对，都应该是来源于网络即将模拟系统的实际 “运行” 结构，它们可以是从实际运行系统中采集来。

**1）参数初始化：**

在开始训练前，所有的权都应该用一些不同的随机数进行初始化。“小随机数” 用来保证网络不会因权值过大而进入饱和状态，从而导致训练失败；“不同” 用来保证网络可以正常地学习。实际上，如果用相同的数去初始化权矩阵，则网络无学习能力。

**2）训练过程包括四步**

**① 第一阶段：前向传播阶段**

*   从样本集中取一个样本，输入网络
    
*   计算相应的实际输出；在此阶段信息从输入层经过逐级的变换，传送到输出层，这个过程也是网络在完成训练之后正常执行时执行的过程
    

**② 第二阶段：后向传播阶段**

*   计算实际输出与相应的理想输出的差
    
*   按照极小化误差的方法**调整权值矩阵**
    
    **网络的训练过程如下：**
    

1.  选定训练组，从样本集中分别随机地寻求 N 个样本作为训练组；
    
2.  将各权值、阈值，置成小的接近于 0 的随机值，并初始化精度控制参数和学习率；
    
3.  从训练组中取一个输入模式加到网络，并给出它的目标输出向量；
    
4.  计算出中间层输出向量，计算出网络的实际输出向量；
    
5.  将输出向量中的元素与目标向量中的元素进行比较，计算出输出误差；对于中间层的隐单元也需要计算出误差；
    
6.  依次计算出各权值的调整量和阈值的调整量；
    
7.  调整权值和调整阈值；
    
8.  当经历 M 后，判断指标是否满足精度要求，如果不满足，则返回 (3)，继续迭代；如果满足就进入下一步；
    
9.  训练结束，将权值和阈值保存在文件中。这时可以认为各个权值已经达到稳定，分类器已经形成。再一次进行训练，直接从文件导出权值和阈值进行训练，不需要进行初始化。
    

## 八、经典网络介绍：

![](<assets/1713096351771.png>)
*   LeNet-5
![](<assets/1713096351827.png>)
![](<assets/1713096351906.png>)

  

![](<assets/1713096351958.png>)

神经元个数 = 卷积核数量 X 输出特征图宽度 X 输出特征图高度  
卷积层可训练参数数量 = 卷积核数量 X（卷积核宽度 X 卷积核高度 + 1）（1 表示偏置）  
汇聚层可训练参数数量 = 卷积核数量 X（1+1）（两个 1 分别表示相加后的系数和偏置，有的汇聚层无参数）  
连接数 = 卷积核数量 X（卷积核宽度 X 卷积核高度 + 1）X 输出特征图宽度 X 输出特征图高度（1 表示偏置）  
全连接层连接数 = 卷积核数量 X（输入特征图数量 X 卷积核宽度 X 卷积核高度 + 1）（输出特征图尺寸为 1X1）

*   **AlexNet**

![](<assets/1713096352021.png>)

  

![](<assets/1713096352074.png>)

  

![](<assets/1713096352136.png>)

*   **Inception 网络**

![](<assets/1713096352205.png>)

  

![](<assets/1713096352275.png>)

  

![](<assets/1713096352330.png>)

*   **残差网络**
*   ![](<assets/1713096352372.png>)
### CNN 总结

本文学习参考资料：
[水很深的深度学习 - Task04 卷积神经网络 CNN_GoAl 的博客 - CSDN 博客](https://blog.csdn.net/qq_36816848/article/details/121576741 "水很深的深度学习-Task04卷积神经网络CNN_GoAl的博客-CSDN博客")
**CNN 的价值：**
1.  能够将大数据量的图片有效的降维成小数据量 (并不影响结果)
2.  能够保留图片的特征，类似人类的视觉原理

**CNN 的基本原理：**
1.  卷积层 – 主要作用是保留图片的特征
2.  池化层 – 主要作用是把数据降维，可以有效的避免过拟合
3.  全连接层 – 根据不同任务输出我们想要的结果

**CNN 的实际应用：**
1.  图片分类、检索
2.  目标定位检测
3.  目标分割
4.  人脸识别
5.  骨骼识别

## 九、RNN 循环神经网络

![](<assets/1713096352466.png>)

![](<assets/1713096352513.png>)

![](<assets/1713096352555.png>)

## 十、LSTM 长短期记忆神经网络

#### 1 LSTM 的产生原因

    RNN 在处理长期依赖（时间序列上距离较远的节点）时会遇到巨大的困难，因为计算距离较远的节点之间的联系时会涉及雅可比矩阵的多次相乘，会造成梯度消失或者梯度膨胀的现象。其中最成功应用最广泛的就是门限 RNN（Gated RNN），而 LSTM 就是门限 RNN 中最著名的一种。有漏单元通过设计连接间的权重系数，从而允许 RNN 累积距离较远节点间的长期联系；而门限 RNN 则泛化了这样的思想，允许在不同时刻改变该系数，且允许网络忘记当前已经累积的信息。

#### 2 RNN 和 LSTM 的区别
所有 RNN 都具有一种重复神经网络模块的链式的形式。在标准的 RNN 中，这个重复的模块只有一个非常简单的结构，例如一个 tanh 层，如下图所示：
![](<assets/1713096352664.png>)
 LSTM 同样是这样的结构，但是重复的模块拥有一个不同的结构。不同于单一神经网络层，这里是有四个，以一种非常特殊的方式进行交互。
![](<assets/1713096352706.png>)
 注：上图图标具体含义如下所示：
![](<assets/1713096352789.png>)
 上图中，每一条黑线传输着一整个向量，从一个节点的输出到其他节点的输入。粉色的圈代表 pointwise 的操作，诸如向量的和，而黄色的矩阵就是学习到的神经网络层。合在一起的线表示向量的连接，分开的线表示内容被复制，然后分发到不同的位置。

#### 3 LSTM 核心

LSTM 有通称作为 “门” 的结构来去除或者增加信息到细胞状态的能力。门是一种让信息选择式通过的方法。他们包含一个 sigmoid 神经网络层和一个 pointwise 乘法操作。示意图如下：
![](<assets/1713096352857.png>)

LSTM 拥有三个门，分别是遗忘门，输入层门和输出层门，来保护和控制细胞状态。
##### **遗忘门**(1步)
作用对象：细胞状态 。
作用：将细胞状态中的信息选择性的遗忘。
操作步骤：该门会读取 $h_{t−1}​​$和  $x_t$​​，输出一个在 0 到 1 之间的数值给每个在细胞状态 $C_{t−1}​​$中的数字。1 表示 “完全保留”，0 表示 “完全舍弃”。示意图如下：

![](<assets/1713096352898.png>)
##### **输入层门**(3步计算)
作用对象：细胞状态
作用：将新的信息选择性的记录到细胞状态中。
操作步骤：
步骤一，sigmoid 层称 “输入门层” 决定`什么值我们将要更新`。
步骤二，tanh 层创建一个新的候选值向量 C~t​​加入到状态中。其示意图如下：

![](<assets/1713096352932.png>)

 步骤三：将 $c_{t−1}$​​更新为 $c_{t​}​$。将旧状态与$f_{t}$​​ 相乘，丢弃掉我们确定需要丢弃的信息。接着加上 it​ ∗ C~t​​得到新的候选值，根据我们决定更新每个状态的程度进行变化。其示意图如下：

![](<assets/1713096352981.png>)

##### **输出层门**(两步计算)
作用对象：隐层  $h_t$​​
作用：确定输出什么值。
操作步骤：
步骤一：将通过 sigmoid 层来确定细胞状态的哪个部分将输出。
步骤二：把细胞状态通过 tanh 进行处理，并它和 sigmoid 门的输出相乘，最终我们仅仅会输出我们确定输出的那部分。
其示意图如下所示：
![](<assets/1713096353035.png>)

![](<assets/1713096353092.png>)
## 十一 、 GRU
![](<assets/1713096353147.png>)

### 1. 什么是 GRU
GRU（Gate Recurrent Unit）是循环神经网络（Recurrent Neural Network, RNN）的一种。和 LSTM（Long-Short Term Memory）一样，也是为了解决长期记忆和反向传播中的梯度等问题而提出来的。

GRU 和 LSTM 在很多情况下实际表现上相差无几，那么为什么我们要使用新人 GRU（2014 年提出）而不是相对经受了更多考验的 LSTM（1997 提出）呢。

下图 1-1 引用论文中的一段话来说明 GRU 的优势所在。
![](<assets/1713251948968.png>)
**简单来说就是贫穷限制了我们的计算能力...**
相比 LSTM，使用 GRU 能够达到相当的效果，并且相比之下更容易进行训练，能够很大程度上提高训练效率，因此很多时候会更倾向于使用 GRU。

OK，那么为什么说 GRU 更容易进行训练呢，下面开始介绍一下 GRU 的内部结构。
### 2. GRU 浅析
#### 2.1 GRU 的输入输出结构
GRU 的输入输出结构与普通的 RNN 是一样的。
![](<assets/1713251949079.png>)
#### 2.2 GRU 的内部结构(三个步骤)
首先，我们先通过上一个传输下来的状态 $h^{t-1}$ 和当前节点的输入 $x^t$ 来获取两个门控状态。如下图 2-2 所示，其中 
$r$  控制重置的门控（reset gate），
$z$ 为控制更新的门控（update gate）。

Tips： $\sigma$ 为_[sigmoid](https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Sigmoid_function)_ 函数，通过这个函数可以将数据变换为 0-1 范围内的数值，从而来充当门控信号。

![](<assets/1713251949143.png>)
**与 LSTM 分明的层次结构不同，下面将对 GRU 进行一气呵成的介绍~~~ 请大家屏住呼吸，不要眨眼。**
得到门控信号之后，首先使用重置门控来得到 **“重置”** 之后的数据 ${h^{t-1}}' = h^{t-1} \odot r$ $${h^{t-1}}' = h^{t-1} \odot r$$ ，再将 ${h^{t-1}}'$ 与输入 $x^t$ 进行拼接，再通过一个 [tanh](https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/tanh) 激活函数来将数据放缩到 **-1~1** 的范围内。即得到如下图 2-3 所示的 $h'$ 。

![](<assets/1713251949209.png>)

这里的 $h'$ 主要是包含了当前输入的 $x^t$  数据。有针对性地对 $h'$ 添加到当前的隐藏状态，相当于” 记忆了当前时刻的状态 “。类似于 LSTM 的选择记忆阶段（参照我的上一篇文章）。

![](<assets/1713251949260.png>)

图 2-4 中的 $\odot$ 是 Hadamard Product，也就是操作矩阵中对应的元素相乘，因此要求两个相乘矩阵是同型的。 $\oplus$ 则代表进行矩阵加法操作。

最后介绍 GRU 最关键的一个步骤，我们可以称之为” 更新记忆 “**阶段。
在这个阶段，我们同时进行了遗忘了记忆两个步骤。我们使用了先前得到的更新门控 $z$ （update gate）。
更新表达式： $$h^t = (1-z) \odot h^{t-1} + z\odot h'$$
首先再次强调一下，门控信号（这里的 $z$）的范围为 0~1。门控信号越接近 1，代表”记忆 “下来的数据越多；而越接近 0 则代表” 遗忘“的越多。

GRU 很聪明的一点就在于，**我们使用了同一个门控 $z$ 就同时可以进行遗忘和选择记忆（LSTM 则要使用多个门控）**。
*   $(1-z) \odot h^{t-1}$ ：表示对原本隐藏状态的选择性 “遗忘”。这里的 $1-z$可以想象成遗忘门（forget gate），忘记 $h^{t-1}$ 维度中一些不重要的信息。
*   $z \odot h'$ ： 表示对包含当前节点信息的 $h'$ 进行选择性” 记忆 “。与上面类似，这里的 $(1-z)$ 同理会忘记 $h '$ 维度中的一些不重要的信息。或者，这里我们更应当看做是对 $h'$  维度中的某些信息进行选择。
*   $h^t =(1- z) \odot h^{t-1} + z\odot h'$：结合上述，这一步的操作就是忘记传递下来的 $h^{t-1}$ 中的某些维度信息，并加入当前节点输入的某些维度信息。

可以看到，这里的遗忘 $z$ 和选择 $(1-z)$ 是联动的。也就是说，对于传递进来的维度信息，我们会进行选择性遗忘，则遗忘了多少权重 （$z$ ），我们就会使用包含当前输入的 $h'$h' 中所对应的权重进行弥补 $(1-z)$ 。以保持一种” 恒定 “状态。

### 3. LSTM 与 GRU 的关系

GRU 是在 2014 年提出来的，而 LSTM 是 1997 年。他们的提出都是为了解决相似的问题，那么 GRU 难免会参考 LSTM 的内部结构。那么他们之间的关系大概是怎么样的呢？这里简单介绍一下。

大家看到 $r$ (reset gate) 实际上与他的名字有点不符。我们仅仅使用它来获得了 $h’$ 。

那么这里的 $h'$ 实际上可以看成对应于 LSTM 中的 hidden state；上一个节点传下来的 $h^{t-1}$则对应于 LSTM 中的 cell state。1-z 对应的则是 LSTM 中的 $z^f$ forget gate，那么 z 我们似乎就可以看成是选择门 $z^i$了。大家可以结合我的两篇文章来进行观察，这是非常有趣的。
### 4. 总结
GRU 输入输出的结构与普通的 RNN 相似，其中的内部思想与 LSTM 相似。
与 LSTM 相比，GRU 内部少了一个” 门控 “，参数比 LSTM 少，但是却也能够达到与 LSTM 相当的功能。考虑到硬件的**计算能力**和**时间成本**，因而很多时候我们也就会选择更加” 实用 “的 GRU 啦。
## 十二 、Transformer

https://zhuanlan.zhihu.com/p/338817680
[Transformer](Transformer.md)

## 十三 BERT

### 模型输入
![](../../img/Pasted%20image%2020240708213325.png)

![](../../img/Pasted%20image%2020240708213213.png)

### 模型结构
![](../../img/Pasted%20image%2020240708213621.png)
## 训练任务
![](../../img/Pasted%20image%2020240708213753.png)
![](../../img/Pasted%20image%2020240708213807.png)