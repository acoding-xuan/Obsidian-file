## 1、[绪论](https://link.zhihu.com/?target=https%3A//www.kesci.com/home/project/5e4f83590e2b66002c1f574b)
## 2、[模型的评估和选择](https://link.zhihu.com/?target=https%3A//www.kesci.com/home/project/5e4f89fb0e2b66002c1f6468)

2.1 误差与过拟合

2.2 评估方法

2.3 训练集与测试集的划分方法 

2.3.1 留出法

2.3.2 交叉验证法

2.3.3 自助法

2.4 调参

2.5 性能度量

2.5.1 最常见的性能度量

2.5.2 查准率 / 查全率 / F1

2.5.3 ROC 与 AUC

2.5.4 代价敏感错误率与代价曲线

2.6 比较检验

2.6.1 假设检验

2.6.2 交叉验证 t 检验

2.6.3 McNemar 检测

2.6.4 Friedman 检测与 Nemenyi 后续检验

2.7 偏差与方差

## 3、[线性模型](https://link.zhihu.com/?target=https%3A//www.kesci.com/home/project/5e4f9a690e2b66002c1f7dc2)

3.1 线性回归

3.2 线性几率回归

3.3 线性判别分析

3.4 多分类学习

3.5 类别不平衡问题

## 4、[决策树](https://link.zhihu.com/?target=https%3A//www.kesci.com/home/project/5e4f9fcc0e2b66002c1f85d0)

4.1 决策树基本概念

4.2 决策树的构造 
> 注意构造过程中生成叶子结点的三种情况及其处理方法。

4.2.1 ID3 算法   
> 使用信息增益, 注意信息熵的定义

4.2.2 C4.5 算法
> 1. 解决了 id3 中的什么问题？
> 2. C4.5算法使用了“增益率”（gain ratio） 如何计算

4.2.3 CART 算法
> CART决策树使用“基尼指数”（Gini index）来选择划分属性
> 基尼指数反映的是从样本集D中随机抽取两个样本，其类别标记不一致的概率

4.3 剪枝处理
> 预剪枝（prepruning）：在构造的过程中先评估，再考虑是否分支。  
> 后剪枝（post-pruning）：在构造好一颗完整的决策树后，自底向上，评估分支的必要性。  

4.4 连续值与缺失值处理

## 5、[神经网络](https://link.zhihu.com/?target=https%3A//www.kesci.com/home/project/5e4fa28a0e2b66002c1f8a37)
感觉没什么东西，跳过即可
5.1 神经元模型

5.2 感知机与多层网络

5.3 BP 神经网络算法

5.4 全局最小与局部最小

5.5 深度学习

## 6、[支持向量机](https://link.zhihu.com/?target=https%3A//www.kesci.com/home/project/5e4fa70a0e2b66002c1f9080)
支持向量机是一种经典的二分类模型，基本模型定义为特征空间中最大间隔的线性分类器，其学习的优化目标便是间隔最大化，因此支持向量机本身可以转化为一个凸二次规划求解的问题。

6.1 间隔(几何间隔)与支持向量
>间隔（Margin）
在SVM中，间隔指的是分类决策边界（即超平面）到最近的训练样本之间的距离。
支持向量：支持向量是离决策边界最近的那些点

6.3 从原始优化问题到对偶问题，SMO 算法的思想。
6.4 核函数的性质，以及一些常见的核函数
> 核矩阵需要是半正定的。
> 包括：线性核，多项式和，高斯核等

6.5 软间隔支持向量机 思想

## 7、[贝叶斯分类器](https://link.zhihu.com/?target=https%3A//www.kesci.com/home/project/5e4fba7c0e2b66002c1fa12b)

7.2 极大似然法(MLE 和 MAP)

7.3 朴素贝叶斯分类器 + laplace 平滑

## 8、[EM 算法](https://link.zhihu.com/?target=https%3A//www.kesci.com/home/project/5e4fbc140e2b66002c1fa261)

8.1 EM 算法思想已经EM算法需要解决什么问题？

8.2 EM 算法数学推导 Jensen 不等式，两步计算

8.3 EM 算法流程

## 9、[集成学习](https://link.zhihu.com/?target=https%3A//www.kesci.com/home/project/5e4fbed50e2b66002c1fa4ac)

9.1 个体与集成
![](../../img/Pasted%20image%2020240526225135.png)

两种分类，根据是否是并行化的方法。
9.2 Boosting  以及Adaboost的主要思想

9.3 Bagging 与 Random Forest

9.3.1 Bagging

9.3.2 随机森林 优化的地方

9.4 结合策略

9.4.1 平均法（回归问题）

9.4.2 投票法（分类问题）

9.4.3 学习法

9.5 多样性（diversity）

## 10、[聚类](https://link.zhihu.com/?target=https%3A//www.kesci.com/home/project/5e4fc1ec0e2b66002c1fa79e)

10.1 距离度量
“闵可夫斯基距离”（Minkowski distance)：
曼哈顿距离（Manhattan distance）：
欧氏距离（Euclidean distance）：
10.2 性能度量 记住外部指标和内部指标分别的作用即可
10.2.1 外部指标

10.2.2 内部指标

10.3 原型聚类
10.3.1 K-Means

10.3.2 学习向量量化（LVQ）

10.3.3 高斯混合聚类(高斯判别分析)

10.4 密度聚类

10.5 层次聚类

## 11、[降维与度量学习](https://link.zhihu.com/?target=https%3A//www.kesci.com/home/project/5e4fc6580e2b66002c1fac1b)

11.1 K 近邻学习

11.2 MDS 算法

11.3 主成分分析（PCA）

11.4 核化线性降维

11.5 流行学习

11.5.1 等度量映射（Isomap）

11.5.2 局部线性嵌入（LLE）

11.6 度量学习

## 12、[特征选择与稀疏学习](https://link.zhihu.com/?target=https%3A//www.kesci.com/home/project/5e4fca6d0e2b66002c1fafb6)

12.1 子集搜索与评价

12.2 过滤式选择（Relief）

12.3 包裹式选择（LVW）

12.4 嵌入式选择与正则化

12.5 稀疏表示与字典学习

12.6 压缩感知

## 13、[计算学习理论](https://link.zhihu.com/?target=https%3A//www.kesci.com/home/project/5e4fcd310e2b66002c1fb293)

13.1 PAC 学习

13.2 有限假设空间

13.2.1 可分情形

13.2.2 不可分情形

13.3 VC 维

13.4 稳定性

## 14、[半监督学习](https://link.zhihu.com/?target=https%3A//www.kesci.com/home/project/5e4fd0930e2b66002c1fb5ff)

14.1 生成式方法

14.2 半监督 SVM

14.3 基于分歧的方法

14.4 半监督聚类

## 15、[概率图模型](https://link.zhihu.com/?target=https%3A//www.kesci.com/home/project/5e4fd53f0e2b66002c1fbacf)

15.1 隐马尔可夫模型（HMM）

15.1.1 HMM 评估问题

15.1.2 HMM 解码问题

15.1.3 HMM 学习问题

15.2 马尔可夫随机场（MRF）

15.3 条件随机场（CRF）

15.4 学习与推断

15.4.1 变量消去

15.4.2 信念传播

15.5 LDA 话题模型

## 16、[强化学习](https://link.zhihu.com/?target=https%3A//www.kesci.com/home/project/5e4fe6330e2b66002c1fd18e)

16.1 基本要素

16.2 K 摇摆赌博机

16.2.1 ε- 贪心

16.2.2 Softmax

16.3 有模型学习

16.3.1 策略评估

16.4 蒙特卡罗强化学习

16.5 AIphaGo 原理浅析