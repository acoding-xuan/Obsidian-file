## 1. 什么是梯度爆炸和梯度消失？如何解决梯度消失、梯度爆炸？

在反向传播过程中需要对激活函数进行求导，如果导数大于 1，那么随着网络层数的增加梯度更新将会朝着指数爆炸的方式增加这就是梯度爆炸。同样如果导数小于 1，那么随着网络层数的增加梯度更新信息会朝着指数衰减的方式减少这就是梯度消失。因此，梯度消失、爆炸，其根本原因在于反向传播训练法则，属于先天不足。
*   解决方法：
1.  对于 RNN，可以通过`梯度截断`，避免梯度爆炸。
2.  可以通过添加正则项，避免梯度爆炸。
3.  使用 LSTM 等自循环和门控制机制，避免梯度消失。
4.  优化激活函数，譬如将 sigmold 改为 relu，避免梯度消失。

## 2. 你对什么方向感兴趣？那个是干什么的？
*   数据挖掘：数据挖掘一般是指从大量的数据中通过算法搜索隐藏于其中信息的过程，包含了机器学习、统计学、数学等多个学科的知识。
*   数据挖掘三大常见的任务：
*   回归任务：回归任务是一种对**连续型随机变量进行预测和建模**的**监督学习算法**，使用案例包括房价预测、股票走势等。
*   分类任务：分类是一种对**离散型变量建模或预测**的**监督学习算法**，使用案例包括邮件过滤、金融欺诈等。
*   聚类任务：聚类是一种**无监督学习**，它是基于数据的内部结构寻找观察样本的自然族群（集群），使用案例包括新闻聚类、文章推荐等。
*   监督学习：数据集中每个样本都有相应的标签。
*   无监督学习：数据集中的样本没有相应的标签。
*   无监督学习算法（Unsupervised Learning）使用无标记数据（输入变量没有对应输出结果），试图识别数据本身的内部结构。**无监督学习算法主要有两类：降维算法**（降低数据维度）如**主成分分析**等，**聚类算法**如 **K 均值聚类**、层次聚类等。

## 3. 简述 PCA 的计算过程 / 介绍下 PCA 算法过程
PCA (Principal Component Analysis) 是最常用的线性降维方法，它的目标是通过某种线性投影，将高维的数据映射到低维的空间中表示，并期望在所投影的维度上数据的方差最大，以此使用较少的数据维度，同时保留住较多的原数据点的特性。
![](<assets/1713021843542.png>)
## 4. 线性回归模型和随机森林模型

### （1）关于回归
回归算法是一种有监督学习算法，用来建立自变量 X 和观测变量 Y 之间的映射关系，如果观测变量是离散的，则称其为分类 Classification；如果观测变量是连续的，则称其为回归 Regression。

回归算法的目的是寻找假设函数 hypothesis 来最好的拟合给定的数据集。常用的回归算法有：线性回归（Linear Regression）、逻辑回归（Logistic Regression）、多项式回归（Polynomial Regression）、岭回归（Ridge Regression）、LASSO 回归（Least Absolute Shrinkage and Selection Operator）、弹性网络（Elastic Net estimators）、逐步回归（Stepwise Regression）等。

### （2）线性回归模型
**线性回归模型试图学得一个线性模型以尽可能准确地预测实值 X 的输出标记 Y。** 在这个模型中，因变量 Y 是连续的，自变量 X 可以是连续或离散的。
（线性回归的定义是：目标值预期是输入变量的线性组合。）
在回归分析中，如果只包括一个自变量和一个因变量，且二者关系可用一条直线近似表示，称为**一元线性回归分析**；如果回归分析中包括两个或两个以上的自变量，且因变量和自变量是线性关系，则称为**多元线性回归分析**。对于二维空间线性是一条直线，对于三维空间线性是一个平面，对于多维空间线性是一个超平面。

### （3）随机森林（Random Forest，简称 RF）算法
#### **a. 集成学习**

集成学习通过建立几个模型组合的来解决单一预测问题。它的工作原理是生成多个分类器 / 模型，各自独立地学习和做出预测。这些预测最后结合成单预测，因此优于任何一个单分类的做出预测。随机森林是集成学习的一个子类，它依靠于决策树的投票选择来决定最后的分类结果。
*   集成学习分两种：
*   模型之间彼此存在依赖关系，按一定的次序搭建多个分类模型，一般后一个模型的加入都需要对现有的集成模型有一定贡献，进而不断提高更新过后的集成模型性能，并借助多个弱分类器搭建出强分类器。代表有 Boosting（AdaBoost）算法。该算法与第一种的随机森林主要区别在于每一颗决策树在生成的过程中都会尽可能降低模型在训练集上的拟合或训练误差
*   模型之间彼此不存在依赖关系，彼此独立。利用相同的训练数据同时搭建多个独立的分类模型，然后通过投票的方式，以少数服从多数的原则做出最终的分类决策。例如：Bagging 和随机森林（Random Forest）.
#### **b. 概述**

严格来说，随机森林其实算是一种集成算法。它首先随机选取不同的特征 (feature) 和训练样本(training sample)，生成大量的决策树，然后综合这些决策树的结果来进行最终的分类。

随机森林算法是最常用也是最强大的监督学习算法之一，它兼顾了解决回归问题和分类问题的能力。随机森林是通过**集成学习**的思想，将多棵决策树进行集成的算法。对于分类问题，其输出的类别是由个别树输出的众数所决定的。在回归问题中，把每一棵决策树的输出进行平均得到最终的回归结果。

tips：决策树的数量越大，随机森林算法的鲁棒性越强，精确度越高。

随机森林在现实分析中被大量使用，它相对于决策树，在准确性上有了很大的提升，同时一定程度上改善了决策树容易被攻击的特点。

#### **c. 随机森林算法的步骤**

1.  **首先，对样本数据进行有放回的抽样，得到多个样本集。** 具体来讲就是每次从原来的 N 个训练样本中有放回地随机抽取 N 个样本 (包括可能重复样本)。
2.  **然后，从候选的特征中随机抽取 m 个特征，作为当前节点下决策的备选特征，从这些特征中选择最好的划分训练样本的特征。用每个样本集作为训练样本构造决策树。** 单个决策树在产生样本集和确定特征后，使用 CART 算法计算，不剪枝。
3.  **最后，得到所需数目的决策树后，随机森林方法对这些树的输出进行投票，以得票最多的类作为随机森林的决策。**

**或：**

随机森林是基于 bagging 框架下的决策树模型，随机森林包含了很多树，每棵树给出分类结果，每棵树的生成规则如下：

1.  如果训练集大小为 N，对于每棵树而言，随机且有放回地从训练中抽取 N 个训练样本，作为该树的训练集，重复 K 次，生成 K 组训练样本集。
2.  如果每个特征的样本维度为 M，指定一个常数 m<<M，随机地从 M 个特征中选取 m 个特征。
3.  利用 m 个特征对每棵树尽最大程度的生长，并且没有剪枝过程。

![](<assets/1713021843645.png>)

*   随机森林中有两个可控制参数：
*   森林中树的数量（一般选取值较大）。
*   抽取的属性值 m 的大小。

#### **d. 随机森林的特点**

*   在当前所有算法中，具有极好的准确率
*   能够有效地运行在大数据集上
*   能够处理具有高维特征的输入样本，而且不需要降维
*   能够评估各个特征在分类问题上的重要性
*   在生成过程中，能够获取到内部生成误差的一种无偏估计
*   对于缺省值问题也能够获得很好的结果

#### **e. 随机森林算法的缺点**

*   随机森林在解决回归问题时，并没有像它在分类中表现的那么好，这是因为它并不能给出一个连续的输出。当进行回归时，随机森林不能够做出超越训练集数据范围的预测，这可能导致在某些特定噪声的数据进行建模时出现过度拟合。（PS: 随机森林已经被证明在某些噪音较大的分类或者回归问题上回过拟合）。
*   对于小数据或者低维数据（特征较少的数据），可能不能产生很好的分类。（处理高维数据，处理特征遗失数据，处理不平衡数据是随机森林的长处）。

#### **f. 适用情景**

（随机森林既可以用于分类，也可以用于回归。一般适用于数据维度较低，同时对准确性要求较高的场景中。）

*   数据维度相对低（几十维），同时对准确性有较高要求时。
*   因为不需要很多参数调整就可以达到不错的效果，基本上不知道用什么方法时都可以先试一下随机森林。

## 5. 常见的数据挖掘算法

### （1）k-means 算法（k 均值算法）

**聚类算法**，事先确定常数 k，k 代表着聚类类别数。首先随机选取 k 个初始点为质心，并通过计算每一个样本与质心之间的相似度（可以采用欧式距离），将样本点归到最相似的类中，接着重新计算每个类的质心（该类中所有点的平均值），重复这样的过程直到质心不再改变，最终就确定了每个样本所属的类别以及每个类的质心。

*   优点：原理简单、容易实现。
*   缺点：收敛太慢、算法复杂度高、需先确定 K 的个数、结果不一定是全局最优，只能保证局部最优。
*   由于每次都要计算所有样本与每一个质心之间的相似度，故在大规模的数据集上，K-Means 算法的收敛速度比较慢。
*   改进收敛速度：第一次迭代正常进行，选取 K 个初始点为质心，然后计算所有节点到这些质心的距离，后续的迭代中，不再计算每个点到所有 K 个质心的距离，仅仅计算上一次迭代中离这个节点最近的某几个质心的距离，对于其他的质心，因为距离太远，归属到那些组的可能性非常小，所以不用再重复计算距离了。

### （2）kNN （k 近邻）学习

**思路：对于待判断的点，找到离它最近的几个数据点，根据它们的类型决定待判断点的类型。**

k 近邻学习是一种常用的**监督学习**方法，其工作机制非常简单：给定测试样本，基于某种距离度量找出训练集中与其最靠近的 k 个训练样本，然后基于这 k 个 “邻居” 的信息来进行预测。通常，在**分类**任务中可使用 “投票法”，即选择这 k 个样本中出现最多的类别标记作为预测结果；在**回归**任务中可使用 “平均法”，即将这 k 个样本的实值输出标记的平均值作为预测结果；还可基于距离远近进行加权平均或加权投票，距离越近的样本权重越大。

（k 最邻近分类算法，每个样本都可以用它最接近的 k 个邻居中大多数样本所属的类别来代表，其中近邻距离的度量方法有余弦值，在实际中 k 值一般取一个比较小的数值，通常采用交叉验证法（就是利用一部分样本做训练集，一部分样本做测试集），通过观察 k 值不同时模型的分类效果来选取最优的 k 值。）

### （3）[决策树](https://link.zhihu.com/?target=https%3A//blog.csdn.net/qq_24519677/article/details/82084718)（ID3 算法和 C4.5 算法）

#### **a. 概述**
决策树是一种简单高效并且具有强解释性的模型，广泛应用于数据分析领域。**其本质是一颗由多个判断节点组成的树。在使用模型进行预测时，根据输入参数依次在各个判断节点进行判断游走，最后到叶子节点即为预测结果。**
决策树学习通常包括 3 个步骤：特征选择、决策树的生成和决策树的修剪。
决策树算法的**核心**是通过对数据的学习，选定判断节点，构造一颗合适的决策树。

#### **b. 构造决策树的主要步骤**
1.  **遍历每个决策条件，对结果集进行拆分。**
2.  **计算在该决策条件下，所有可能的拆分情况的信息增益，信息增益最大的拆分为本次最优拆分。**
3.  **递归执行 1、2 两步，直至信息增益 <=0。**

![](<assets/1713021843717.png>)

决策树既可以作为**分类算法**，也可以作为**回归算法**，同时也特别适合**集成学习**比如**随机森林**。
#### **c. 决策树的剪枝方式**

剪枝 (pruning) 是决策树学习算法对付 “过拟合” 的主要手段。

决策树剪枝的基本策略有 “预剪枝” 和“后剪枝”。预剪枝是指在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶结点；后剪枝则是先从训练集生成一棵完整的决策树，然后自底向上地对非叶节点进行考察，`若将该结点对应的子树替换为叶节点能带来决策树泛化性能提升，则将该子树替换为叶结点。`
#### **d. 优缺点**
**优点：**
1、计算量较小
2、清晰表达属性的重要程度
3、可增量学习对模型进行部分重构
4、不需要任何领域知识和参数假设
5、适合高维数据
6、随机森林是基于决策树的集成学习策略，随机森林鲜有短板
**缺点：**
1、没有考虑属性间依赖
2、容易过拟合，通过剪枝缓解
3、不可用于推测属性缺失的样本
#### **e. ID3 算法**

熵是信息论中的概念，熵度量了事物的不确定性，越不确定的事物，它的熵就越大。当每件事物发生的概率相同时，它们发生的随机性最大，所以它们的熵也就越大。ID3 算法就是用信息增益来判别当前节点应该用什么特征来构建决策树。某个特征的信息增益越大表示该特征对数据集的分类的不确定性减少的程度越高，越适合用来分类。

**ID3 算法的核心是在决策树各个节点上应用信息增益准则选择特征，递归地构建决策树。** 具体方法是：从根节点开始，对节点计算所有可能的特征的信息增益，选择信息增益最大的特征作为节点的特征，由该特征的不同取值建立子节点；再对子节点递归的调用以上方法，构建决策树；直到所有特征的信息增益均很小或没有特征可以选择为止。最后得到一个决策树。ID3 相当于用极大似然估计法进行概率模型的选择。
*   ID3 算法的不足：
![](<assets/1713021843801.png>)

**f. C4.5 算法**

以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题。使用信息增益比可以对这一问题进行校正。C4.5 算法与 ID3 算法相似，C4.5 算法对 ID3 算法进行了改进，C4.5 在生成的过程中，用信息增益比来选择特征。

*   C4.5 算法的不足：

![](<assets/1713021843864.png>)

### （4）朴素贝叶斯 (naive Bayes)

#### **a. 概述**
逻辑回归通过拟合曲线（或者学习超平面）实现分类，决策树通过寻找最佳划分特征进而学习样本路径实现分类，支持向量机通过寻找分类超平面进而最大化类别间隔实现分类。相比之下，朴素贝叶斯独辟蹊径，通过特征概率来预测分类。

**朴素贝叶斯（naive Bayes）法是一种基于贝叶斯定理与特征条件独立假设的分类方法。对于给定的训练数据集，首先基于特征条件独立假设学习输入输出的联合概率分布；然后基于此模型，对给定的输入 x，利用贝叶斯定理求出后验概率最大的输出 y。**

朴素贝叶斯是经典的机器学习算法之一，也是为数不多的基于概率论的分类算法。朴素贝叶斯原理简单，也很容易实现，学习与预测的效率都很高，多用于文本分类，比如垃圾邮件过滤。
#### **b. QA：朴素贝叶斯朴素在哪里呢？**
—— **两个假设**：
*   一个特征出现的概率与其他特征（条件）独立。
*   每个特征同等重要。
或：假设各特征之间相互独立。
#### **c. 优缺点**
**优点：**
1、计算量较小 2、支持懒惰学习、增量学习 3、对缺失数据不太敏感 4、推断即查表，速度极快。
**缺点：**
1、没有考虑属性间依赖 2、通过类先验概率产生模型

### （5）逻辑回归 (logistic 回归)
logistic 回归虽然带回归两字却和线性回归有很大的区别，线性回归主要用于预测问题，其输出值为连续变量，而 logistic 回归主要用于**分类**问题，其输出值为**离散值**。logistic 回归可以用于多元分类问题，也可以用于二元分类问题，但二元分类更为常用。
逻辑回归是应用非常广泛的一个分类机器学习算法，它将数据拟合到一个 logit 函数 (或者叫做 logistic 函数) 中，从而能够完成对事件发生的概率进行预测。
逻辑回归是一个使用逻辑函数将线性回归的结果归一化的分类模型，这里的归一化指将值约束在 0 和 1 之间。
缺点：容易欠拟合，分类精度可能不高。
### （6）SVM（支持向量机）

SVM 的全称是 Support Vector Machine，即支持向量机，主要用于解决模式识别领域中的数据分类问题，属于**有监督学习**算法的一种。

通俗来讲，**SVM 是一种二类分类模型，其基本模型定义为特征空间上的间隔最大的线性分类器，其学习策略便是间隔最大化，最终可转化为一个凸二次规划问题的求解。**

**SVM 分类，就是找到一个超平面，让两个分类集合的支持向量或者所有的数据（LSSVM）离分类平面最远**；SVR 回归，就是找到一个回归平面，让一个集合的所有数据到该平面的距离最近。SVR 是支持向量回归 (support vector regression) 的英文缩写，是支持向量机 (SVM) 的重要的应用分支。
#### **a. 目标**
**SVM 是一个二类分类器，它的目标是找到一个超平面，使用两类数据离超平面越远越好，从而对新的数据分类更准确，即使分类器更加健壮。**
支持向量（Support Vetor）：就是离分隔超平面最近的那些点。
寻找最大间隔：就是寻找最大化支持向量到分隔超平面的距离，在此条件下求出分隔超平面。
#### **b. 支持向量机的基本原理**
*   【硬间隔】支持向量机的基本原理是在（不同的）类间找到合适的最宽的‘街道’（street）。换句话说，目标是在划分两类训练样本的决策边界之间找到最大的间隔。
*   【软间隔】当用软间隔（soft-margin）进行分类时，SVM 在‘完美划分两类’和‘找到最宽街道’之间做一个折中（亦即少数样本会落到‘街道’上）。
*   【核】另外一个关键思想是当在非线性数据集上用核（kernel）。核函数（kernel function）将特征从低维到高维进行转换，但是它是先在低维上进行计算，实际的分类效果表现在高维上。这样就避免了在高维上复杂的计算，仍得到相同的结果。
一些常用的核函数：多项式核、高斯核、线性核。
#### **c. SVM 特点**
1.  非线性映射是 SVM 方法的理论基础，SVM 利用内积核函数代替向高维空间的非线性映射；
2.  对特征空间划分的最优超平面是 SVM 的目标，最大化分类边际的思想是 SVM 方法的核心；
3.  支持向量是 SVM 的训练结果, 在 SVM 分类决策中起决定作用的是支持向量。因此，模型需要存储空间小，算法鲁棒性强；
4.  无任何前提假设，不涉及概率测度；
5.  SVM 算法对大规模训练样本难以实施。由于 SVM 是借助二次规划来求解支持向量，而求解二次规划将涉及 N 阶矩阵的计算（N 为样本的个数），当 N 数目很大时该矩阵的存储和计算将耗费大量的机器内存和运算时间。针对以上问题的主要改进有有 J.Platt 的 SMO 算法、T.Joachims 的 SVM、C.J.C.Burges 等的 PCGC、张学工的 CSVM 以及 O.L.Mangasarian 等的 SOR 算法。
6.  用 SVM 解决多分类问题存在困难。经典的支持向量机算法只给出了二类分类的算法，而在数据挖掘的实际应用中，一般要解决多类的分类问题。可以通过多个二类支持向量机的组合来解决。主要有一对多组合模式、一对一组合模式和 SVM 决策树；再就是通过构造多个分类器的组合来解决。主要原理是克服 SVM 固有的缺点，结合其他算法的优势，解决多类问题的分类精度。如：与粗集理论结合，形成一种优势互补的多类问题的组合分类器。

#### **d. 解释 SVM 为什么要化对偶形？百万样本量可以用 SVM 吗？**

1.  对偶问题将原始问题中的约束转为了对偶问题中的等式约束
2.  方便核函数的引入
3.  改变了问题的复杂度。由求特征向量 w 转化为求比例系数 a，在原始问题下，求解的复杂度与样本的维度有关，即 w 的维度。在对偶问题下，只与样本数量有关。

*   **在一个有百万量级的样本和数以百计的特征的训练集上，该用原始形式还是对偶形式的 SVM 来训练模型？**
*   这个问题只能是对线性 SVM 来说有意义，因为核 - SVM 只能用对偶形式。SVM 的原始形式的计算复杂度与样本数 m 成比例（O(m)），对偶形式的计算复杂度在 m^2 和 m^3 之间（O(m^2)~O(m^3)）。因此如果有百万量级的样本，肯定需要用原始形式，因为对偶形式慢得多。

#### **e. 优缺点**
**优点：**
1、可解决小样本的机器学习任务 2、可解决高维问题 3、可通过核方法解决非线性问题

**缺点：**

1、对缺失数据敏感 2、对于非线性问题，核函数方法选择一直是个未决问题

### （7）神经网络算法
*   使用神经网络有 4 个步骤：
1.  提取问题中实体的特征向量作为神经网络的输入，不同实体可以提取不同的特征向量。
2.  定义神经网络的结构，并定义如何从神经网络中的输入得到输出，这个过程就是神经网络的前向传播算法。
3.  通过训练数据来调整神经网络参数的取值，这就是训练神经网络的过程。
4.  使用训练的神经网络来预测未知的数据。

#### **a. 优缺点**
**优点：**
1、分类的准确度极高
2、可解决复杂的非线性问题
3、对噪声神经有较强的鲁棒性和容错能力
4、并行分布处理能力强, 分布存储及学习能力强
5、常用于图像识别
6、数据量越大，表现越好
**缺点：**
1、黑箱模型，难以解释
2、需要初始化以及训练大量参数，如网络结构、权值、阈值，计算复杂
3、误差逆传播的损失
4、容易陷入局部最小

#### **b. 深度学习原理**
`使用足够多的参数就可以以任意精度逼近任何函数`，而深度学习很容易就把参数加多（层数加深 + 加宽），这是传统的机器学习方法无法轻易做到的。
1.  对神经网络的权重随机赋值，由于是对输入数据进行随机的变换，因此跟预期值可能差距很大，相应地，损失值也很高；
2.  根据损失值，利用反向传播算法来微调神经网络每层的参数，从而较低损失值；
3.  根据调整的参数继续计算预测值，并计算预测值和预期值的差距，即损失值；
4.  重复步骤 2,3，直到整个网络的损失值达到最小，即算法收敛。

#### **c. 深度学习的优点**
机器学习技术（浅层学习）仅将输入数据变换到一两个连续的表示空间，通常使用简单的变换，这通常无法得到复杂问题所需要的精确表示。因此，人们必须竭尽全力让输入数据更适合这些方法来处理，也必须手动为数据设计好表示层，这个过程叫做特征工程。

深度学习的优点在于它在很多问题上都变现出更好的性能，并且简化了问题的解决步骤，因为`它将特征工程完全自动化`。利用深度学习，你可以一次性学习所有特征，而无须自己手动设计。这极大地简化了机器学习工作流程，通常将复杂的多阶段流程替换为一个简单的、端到端的深度学习模型。

深度学习的变革性在于，模型可以在同一时间共同学习所有表示层，而不是依次连续学习（这被称为贪婪学习）。通过共同的特征学习，一旦模型修改某个内部特征，所有依赖于该特征的其他特征都会相应地自动调节适应，无须人为干预。

深度学习从数据中进行学习时有两个基本特征：第一，通过渐进的、逐层的方式形成越来越复杂的表示；第二，对中间这些渐进的表示共同进行学习，每一层的变化都需要同时考虑上下两层的需要。

## 6. SVD 奇异值分解

[奇异值](https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E5%25A5%2587%25E5%25BC%2582%25E5%2580%25BC)分解（Singular Value Decomposition）是[线性代数](https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E7%25BA%25BF%25E6%2580%25A7%25E4%25BB%25A3%25E6%2595%25B0/800%2522%2520%255Ct%2520%2522_blank)中一种重要的[矩阵分解](https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E7%259F%25A9%25E9%2598%25B5%25E5%2588%2586%25E8%25A7%25A3/4035386)，**是[特征分解](https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E7%2589%25B9%25E5%25BE%2581%25E5%2588%2586%25E8%25A7%25A3/12522621)（矩阵必须为方阵）在任意矩阵上的推广**。奇异值分解是一个适用于任意矩阵的一种分解的方法。

奇异值分解在统计中的主要应用为主成分分析（PCA），一种数据分析方法，用来找出大量数据中所隐含的 “模式”，它可以用在模式识别，数据压缩等方面。`PCA 算法的作用是把数据集映射到低维空间中去`。数据集的特征值（在 SVD 中用奇异值表征）按照重要性排列，降维的过程就是舍弃不重要的特征向量的过程，而剩下的特征向量组成的空间即为降维后的空间。
### （1）特征值分解 (EVD)
![](<assets/1713021843939.png>)

**矩阵分解的物理意义**

特征值分解可以得到特征值与特征向量，特征值表示的是这个特征到底有多重要，而特征向量表示这个特征是什么，`可以将每一个特征向量理解为一个线性的子空间`，我们可以利用这些线性的子空间干很多的事情。不过，特征值分解也有很多的局限，比如说变换的矩阵必须是方阵。
### （2）奇异值分解 (SVD)
![](<assets/1713021843994.png>)
![](<assets/1713021844048.png>)

![](<assets/1713021844098.png>)

![](<assets/1713021844156.png>)
![](<assets/1713021844213.png>)
## **7. 牛顿法和拟牛顿法**
### **（1）牛顿法**

梯度下降法只用到了目标函数的一阶导数，牛顿法是一种**二阶优化算法**，其核心思想是**对函数进行泰勒展开**。
**a. 用于方程求解**

求解方程 f(x) = 0 的解：
1.  选择一个接近函数 f(x)=0 处的 x0，计算相应的 f (x0) 和切线斜率 f′(x0)
2.  计算过点 (x0,f(x0)) 并且斜率为 f′(x0) 的直线和 X 轴的交点的 x 坐标，也就是求如下方程的解：f(x0)+f′(x0)∗(x−x0)=0
3.  将新求得的点的 x 坐标命名为 x1，通常 x1 会比 x0 更接近方程 f(x) = 0 的解。因此我们现在可以利用 x1 开始下一轮迭代。迭代公式可化简为如下所示：

![](<assets/1713021844539.png>)

由于牛顿法是基于当前位置的切线来确定下一次的位置，所以牛顿法又被很形象地称为是” 切线法”。

![](<assets/1713021844590.png>)

或者这张图，更好理解：

![](<assets/1713021844650.png>)

已经证明，如果 f'是连续的，并且待求的零点 x 是孤立的，那么在零点 x 周围存在一个区域，只要初始值 x0 位于这个邻近区域内，那么牛顿法必定收敛。 并且，如果 f’(x) 不为 0, 那么牛顿法将具有平方收敛的性能，这意味着每迭代一次，牛顿法结果的有效数字将增加一倍。

**b. 用于最优化**

对于求模板书极大极小值的问题，可以转化为求函数 f 的导数为 0 的问题，这样问题就可以看成和方程求解一样的问题 (f'=0)，与用牛顿法求解很相似了。

1.  先对 f(x) 进行二阶泰勒公式展开

![](<assets/1713021844702.png>)

2. 然后对 f(x) 求导，得到：

![](<assets/1713021844770.png>)

注意，所有的 xk 和其导数都是已知的，视为常数项。

3. 令 f'(x)=0 得到

![](<assets/1713021844831.png>)

**c. 关于牛顿法和梯度下降法的效率对比**

从本质上去看，牛顿法是二阶收敛，梯度下降是一阶收敛，所以牛顿法就更快。更通俗地说，比如你想找一条最短的路径走到一个盆地的最底部，梯度下降法每次只从你当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大（二阶导数信息）。所以，可以说牛顿法比梯度下降法看得更远一点，能更快地走到最底部。（牛顿法目光更加长远，所以少走弯路；相对而言，梯度下降法只考虑了局部的最优，没有全局思想。）

从几何上说，牛顿法就是用一个二次曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。

**d. 牛顿法的优缺点总结**
优点：二阶收敛，收敛速度快； 缺点：牛顿法是一种迭代算法，每一步都需要求解目标函数的 Hessian 矩阵的逆矩阵，计算比较复杂。

因此，如果在目标函数的梯度和 Hessian 矩阵比较好求的时候应使用 Newton 法。当模型的参数很多时 Hessian 矩阵的计算成本将会很大，导致收敛速度变慢，所以在深度学习中也很少使用牛顿法。

### **（2）拟牛顿法**

拟牛顿法是求解非线性优化问题最有效的方法之一。

拟牛顿法的本质思想是改善牛顿法每次需要求解复杂的 Hessian 矩阵的逆矩阵的缺陷，它使用正定矩阵来近似 Hessian 矩阵的逆，从而简化了运算的复杂度。
拟牛顿法和最速下降法一样只要求每一步迭代时知道目标函数的梯度。通过测量梯度的变化，构造一个目标函数的模型使之足以产生超线性收敛性。这类方法大大优于最速下降法，尤其对于困难的问题。另外，因为拟牛顿法不需要二阶导数的信息，所以有时比牛顿法更为有效。如今，优化软件中包含了大量的拟牛顿算法用来解决无约束，约束，和大规模的优化问题。